{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import torch \n",
    "#import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "#from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pd.nn.model import Conv\n",
    "from pd.metric import amex_metric\n",
    "from pd.data.loader import CustomerData, DataLoader\n",
    "from pd.params import *\n",
    "from pd.pred import pred_test_npy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = OUTDIR + \"pd_lgbm_feat_trans__ght1pi1/\"\n",
    "import glob\n",
    "files = glob.glob(data_dir+\"oof_lgbm_baseline_2fold_seed42_*.csv\")\n",
    "\n",
    "data = pd.read_csv(data_dir+\"oof_lgbm_baseline_2fold_seed42.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n",
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_40419/4062239063.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data[df.columns[0]] = df[df.columns[0]]\n"
     ]
    }
   ],
   "source": [
    "#for fdir in files:\n",
    "#    data[]\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "    data[df.columns[0]] = df[df.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>target</th>\n",
       "      <th>D_124</th>\n",
       "      <th>D_81</th>\n",
       "      <th>D_50</th>\n",
       "      <th>B_37</th>\n",
       "      <th>D_45</th>\n",
       "      <th>D_109</th>\n",
       "      <th>B_20</th>\n",
       "      <th>D_140</th>\n",
       "      <th>...</th>\n",
       "      <th>S_8</th>\n",
       "      <th>D_46</th>\n",
       "      <th>D_58</th>\n",
       "      <th>D_122</th>\n",
       "      <th>D_126</th>\n",
       "      <th>D_83</th>\n",
       "      <th>D_63</th>\n",
       "      <th>B_29</th>\n",
       "      <th>R_3</th>\n",
       "      <th>D_51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.300378</td>\n",
       "      <td>0.257368</td>\n",
       "      <td>0.163501</td>\n",
       "      <td>0.122973</td>\n",
       "      <td>0.197915</td>\n",
       "      <td>0.293117</td>\n",
       "      <td>0.124314</td>\n",
       "      <td>0.272010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124521</td>\n",
       "      <td>0.187688</td>\n",
       "      <td>0.173656</td>\n",
       "      <td>0.228618</td>\n",
       "      <td>0.272977</td>\n",
       "      <td>0.253315</td>\n",
       "      <td>0.359062</td>\n",
       "      <td>0.295753</td>\n",
       "      <td>0.172025</td>\n",
       "      <td>0.061715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000fd6641609c6ece5454664794f0340ad84dddce9a2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.220659</td>\n",
       "      <td>0.257528</td>\n",
       "      <td>0.341581</td>\n",
       "      <td>0.165804</td>\n",
       "      <td>0.196310</td>\n",
       "      <td>0.292490</td>\n",
       "      <td>0.123678</td>\n",
       "      <td>0.271069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320611</td>\n",
       "      <td>0.212026</td>\n",
       "      <td>0.082227</td>\n",
       "      <td>0.271827</td>\n",
       "      <td>0.271874</td>\n",
       "      <td>0.252600</td>\n",
       "      <td>0.419863</td>\n",
       "      <td>0.294780</td>\n",
       "      <td>0.212669</td>\n",
       "      <td>0.100263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00001b22f846c82c51f6e3958ccd81970162bae8b007e8...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.234569</td>\n",
       "      <td>0.257528</td>\n",
       "      <td>0.341581</td>\n",
       "      <td>0.079703</td>\n",
       "      <td>0.174259</td>\n",
       "      <td>0.292490</td>\n",
       "      <td>0.123678</td>\n",
       "      <td>0.271069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152276</td>\n",
       "      <td>0.202265</td>\n",
       "      <td>0.122288</td>\n",
       "      <td>0.342314</td>\n",
       "      <td>0.271874</td>\n",
       "      <td>0.252600</td>\n",
       "      <td>0.419863</td>\n",
       "      <td>0.294780</td>\n",
       "      <td>0.170585</td>\n",
       "      <td>0.160865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000041bdba6ecadd89a52d11886e8eaaec9325906c9723...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.325749</td>\n",
       "      <td>0.257528</td>\n",
       "      <td>0.095682</td>\n",
       "      <td>0.256496</td>\n",
       "      <td>0.386565</td>\n",
       "      <td>0.292490</td>\n",
       "      <td>0.123678</td>\n",
       "      <td>0.271069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401980</td>\n",
       "      <td>0.168278</td>\n",
       "      <td>0.113478</td>\n",
       "      <td>0.266747</td>\n",
       "      <td>0.271874</td>\n",
       "      <td>0.252600</td>\n",
       "      <td>0.419863</td>\n",
       "      <td>0.294780</td>\n",
       "      <td>0.248176</td>\n",
       "      <td>0.257508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.220679</td>\n",
       "      <td>0.257368</td>\n",
       "      <td>0.231843</td>\n",
       "      <td>0.074861</td>\n",
       "      <td>0.173456</td>\n",
       "      <td>0.293117</td>\n",
       "      <td>0.124314</td>\n",
       "      <td>0.272010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267331</td>\n",
       "      <td>0.257610</td>\n",
       "      <td>0.356312</td>\n",
       "      <td>0.228618</td>\n",
       "      <td>0.272977</td>\n",
       "      <td>0.253315</td>\n",
       "      <td>0.407932</td>\n",
       "      <td>0.295753</td>\n",
       "      <td>0.201479</td>\n",
       "      <td>0.279499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458908</th>\n",
       "      <td>ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.237945</td>\n",
       "      <td>0.257528</td>\n",
       "      <td>0.341581</td>\n",
       "      <td>0.127871</td>\n",
       "      <td>0.412144</td>\n",
       "      <td>0.292490</td>\n",
       "      <td>0.123678</td>\n",
       "      <td>0.271069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205438</td>\n",
       "      <td>0.226265</td>\n",
       "      <td>0.106783</td>\n",
       "      <td>0.184708</td>\n",
       "      <td>0.271874</td>\n",
       "      <td>0.252600</td>\n",
       "      <td>0.419863</td>\n",
       "      <td>0.294780</td>\n",
       "      <td>0.486636</td>\n",
       "      <td>0.112394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458909</th>\n",
       "      <td>ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.229717</td>\n",
       "      <td>0.257368</td>\n",
       "      <td>0.180313</td>\n",
       "      <td>0.515397</td>\n",
       "      <td>0.168948</td>\n",
       "      <td>0.293117</td>\n",
       "      <td>0.478467</td>\n",
       "      <td>0.272010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348009</td>\n",
       "      <td>0.239159</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>0.268949</td>\n",
       "      <td>0.272977</td>\n",
       "      <td>0.253315</td>\n",
       "      <td>0.407932</td>\n",
       "      <td>0.295753</td>\n",
       "      <td>0.212552</td>\n",
       "      <td>0.122594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458910</th>\n",
       "      <td>ffff9984b999fccb2b6127635ed0736dda94e544e67e02...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.255652</td>\n",
       "      <td>0.257368</td>\n",
       "      <td>0.143870</td>\n",
       "      <td>0.200746</td>\n",
       "      <td>0.266988</td>\n",
       "      <td>0.293117</td>\n",
       "      <td>0.172772</td>\n",
       "      <td>0.272010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124494</td>\n",
       "      <td>0.314794</td>\n",
       "      <td>0.088757</td>\n",
       "      <td>0.344559</td>\n",
       "      <td>0.272977</td>\n",
       "      <td>0.253315</td>\n",
       "      <td>0.407932</td>\n",
       "      <td>0.295753</td>\n",
       "      <td>0.240991</td>\n",
       "      <td>0.130741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458911</th>\n",
       "      <td>ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.332922</td>\n",
       "      <td>0.257528</td>\n",
       "      <td>0.341581</td>\n",
       "      <td>0.119195</td>\n",
       "      <td>0.257795</td>\n",
       "      <td>0.292490</td>\n",
       "      <td>0.329601</td>\n",
       "      <td>0.271069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414683</td>\n",
       "      <td>0.340837</td>\n",
       "      <td>0.317578</td>\n",
       "      <td>0.360752</td>\n",
       "      <td>0.271874</td>\n",
       "      <td>0.252600</td>\n",
       "      <td>0.419863</td>\n",
       "      <td>0.294780</td>\n",
       "      <td>0.255430</td>\n",
       "      <td>0.381021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458912</th>\n",
       "      <td>fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252889</td>\n",
       "      <td>0.257368</td>\n",
       "      <td>0.291778</td>\n",
       "      <td>0.224240</td>\n",
       "      <td>0.171385</td>\n",
       "      <td>0.293117</td>\n",
       "      <td>0.383201</td>\n",
       "      <td>0.272010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297357</td>\n",
       "      <td>0.255990</td>\n",
       "      <td>0.148749</td>\n",
       "      <td>0.266869</td>\n",
       "      <td>0.272977</td>\n",
       "      <td>0.253315</td>\n",
       "      <td>0.434523</td>\n",
       "      <td>0.295753</td>\n",
       "      <td>0.220068</td>\n",
       "      <td>0.382600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458913 rows  181 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              customer_ID  target     D_124  \\\n",
       "0       0000099d6bd597052cdcda90ffabf56573fe9d7c79be5f...       0  0.300378   \n",
       "1       00000fd6641609c6ece5454664794f0340ad84dddce9a2...       0  0.220659   \n",
       "2       00001b22f846c82c51f6e3958ccd81970162bae8b007e8...       0  0.234569   \n",
       "3       000041bdba6ecadd89a52d11886e8eaaec9325906c9723...       0  0.325749   \n",
       "4       00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8a...       0  0.220679   \n",
       "...                                                   ...     ...       ...   \n",
       "458908  ffff41c8a52833b56430603969b9ca48d208e7c192c6a4...       0  0.237945   \n",
       "458909  ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fd...       0  0.229717   \n",
       "458910  ffff9984b999fccb2b6127635ed0736dda94e544e67e02...       0  0.255652   \n",
       "458911  ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf38814...       1  0.332922   \n",
       "458912  fffff1d38b785cef84adeace64f8f83db3a0c31e8d92ea...       0  0.252889   \n",
       "\n",
       "            D_81      D_50      B_37      D_45     D_109      B_20     D_140  \\\n",
       "0       0.257368  0.163501  0.122973  0.197915  0.293117  0.124314  0.272010   \n",
       "1       0.257528  0.341581  0.165804  0.196310  0.292490  0.123678  0.271069   \n",
       "2       0.257528  0.341581  0.079703  0.174259  0.292490  0.123678  0.271069   \n",
       "3       0.257528  0.095682  0.256496  0.386565  0.292490  0.123678  0.271069   \n",
       "4       0.257368  0.231843  0.074861  0.173456  0.293117  0.124314  0.272010   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "458908  0.257528  0.341581  0.127871  0.412144  0.292490  0.123678  0.271069   \n",
       "458909  0.257368  0.180313  0.515397  0.168948  0.293117  0.478467  0.272010   \n",
       "458910  0.257368  0.143870  0.200746  0.266988  0.293117  0.172772  0.272010   \n",
       "458911  0.257528  0.341581  0.119195  0.257795  0.292490  0.329601  0.271069   \n",
       "458912  0.257368  0.291778  0.224240  0.171385  0.293117  0.383201  0.272010   \n",
       "\n",
       "        ...       S_8      D_46      D_58     D_122     D_126      D_83  \\\n",
       "0       ...  0.124521  0.187688  0.173656  0.228618  0.272977  0.253315   \n",
       "1       ...  0.320611  0.212026  0.082227  0.271827  0.271874  0.252600   \n",
       "2       ...  0.152276  0.202265  0.122288  0.342314  0.271874  0.252600   \n",
       "3       ...  0.401980  0.168278  0.113478  0.266747  0.271874  0.252600   \n",
       "4       ...  0.267331  0.257610  0.356312  0.228618  0.272977  0.253315   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "458908  ...  0.205438  0.226265  0.106783  0.184708  0.271874  0.252600   \n",
       "458909  ...  0.348009  0.239159  0.418882  0.268949  0.272977  0.253315   \n",
       "458910  ...  0.124494  0.314794  0.088757  0.344559  0.272977  0.253315   \n",
       "458911  ...  0.414683  0.340837  0.317578  0.360752  0.271874  0.252600   \n",
       "458912  ...  0.297357  0.255990  0.148749  0.266869  0.272977  0.253315   \n",
       "\n",
       "            D_63      B_29       R_3      D_51  \n",
       "0       0.359062  0.295753  0.172025  0.061715  \n",
       "1       0.419863  0.294780  0.212669  0.100263  \n",
       "2       0.419863  0.294780  0.170585  0.160865  \n",
       "3       0.419863  0.294780  0.248176  0.257508  \n",
       "4       0.407932  0.295753  0.201479  0.279499  \n",
       "...          ...       ...       ...       ...  \n",
       "458908  0.419863  0.294780  0.486636  0.112394  \n",
       "458909  0.407932  0.295753  0.212552  0.122594  \n",
       "458910  0.407932  0.295753  0.240991  0.130741  \n",
       "458911  0.419863  0.294780  0.255430  0.381021  \n",
       "458912  0.434523  0.295753  0.220068  0.382600  \n",
       "\n",
       "[458913 rows x 181 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int) -> None:\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        self.query_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.key_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.bias = nn.Parameter(torch.rand(hidden_dim).uniform_(-0.1, 0.1))\n",
    "        self.score_proj = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        score = self.score_proj(torch.tanh(self.key_proj(key) + self.query_proj(query) + self.bias)).squeeze(-1)\n",
    "        attn = F.softmax(score, dim=-1)\n",
    "        context = attn.unsqueeze(1) * value\n",
    "        return context, attn\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute the dot products of the query with all values and apply a softmax function to obtain the weights on the values\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, query: torch.Tensor, value: torch.Tensor):\n",
    "        batch_size, hidden_dim, input_size = query.size(0), query.size(2), value.size(1)\n",
    "\n",
    "        score = query * value.transpose(0, 1)\n",
    "        attn = F.softmax(score.view(-1, input_size), dim=1).view(batch_size, -1, input_size)\n",
    "        context = torch.bmm(attn, value)\n",
    "\n",
    "        return context, attn\n",
    "\n",
    "class MLP(ESModule):\n",
    "    def __init__(self, input_dim, hidden_dim=128,):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.nf1 = nn.LayerNorm([hidden_dim])\n",
    "        self.att = AdditiveAttention(hidden_dim=128, )        \n",
    "        self.nf2 = nn.LayerNorm([hidden_dim])\n",
    "        self.fcout = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "    \n",
    "    def forward(self, h, return_featues=False):\n",
    "        h = F.selu(self.fc1(h))\n",
    "        h = self.nf1(h)\n",
    "        r, att = self.att(h, h, h)\n",
    "        h = self.nf2(h+r) \n",
    "        if return_featues:\n",
    "            return torch.sigmoid(self.fcout(h)), h\n",
    "        \n",
    "        return torch.sigmoid(self.fcout(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(ESModule):\n",
    "    def __init__(self, input_dim, hidden_dim=128,):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.nf1 = nn.LayerNorm([hidden_dim])\n",
    "        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.nf2 = nn.LayerNorm([hidden_dim])\n",
    "        \n",
    "        self.fc3 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.nf3 = nn.LayerNorm([hidden_dim])\n",
    "                \n",
    "        self.fcout = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "    \n",
    "    def forward(self, h, return_featues=False):\n",
    "        h = F.selu(self.fc1(h))\n",
    "        h = self.nf1(h)\n",
    "        r = F.selu(self.fc2(h))\n",
    "        r = self.nf2(r)\n",
    "        h = F.selu(self.fc3(h+r))\n",
    "        h = self.nf3(h)\n",
    "        h = h+r\n",
    "        if return_featues:\n",
    "            return torch.sigmoid(self.fcout(h)), h\n",
    "        \n",
    "        return torch.sigmoid(self.fcout(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[data.columns[2:]].values\n",
    "train_labels = data[\"target\"].values.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, train_labels, test_size=1/9, random_state=0, shuffle=True)\n",
    "validation_data = (X_test, y_test)\n",
    "\n",
    "train_dataset = CustomerData(X_train, train_labels=y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, BCE loss: 0.693, amex train: -0.178, val 0.000\n",
      "0, BCE loss: 0.995, amex train: 0.299, val 0.000\n",
      "0, BCE loss: 0.815, amex train: 0.585, val 0.000\n",
      "0, BCE loss: 0.545, amex train: 0.650, val 0.000\n",
      "0, BCE loss: 0.477, amex train: 0.644, val 0.000\n",
      "0, BCE loss: 0.599, amex train: 0.652, val 0.000\n",
      "0, BCE loss: 0.570, amex train: 0.635, val 0.000\n",
      "0, BCE loss: 0.466, amex train: 0.652, val 0.000\n",
      "0, BCE loss: 0.423, amex train: 0.633, val 0.000\n",
      "0, BCE loss: 0.445, amex train: 0.638, val 0.000\n",
      "0, BCE loss: 0.466, amex train: 0.653, val 0.000\n",
      "0, BCE loss: 0.464, amex train: 0.647, val 0.000\n",
      "0, BCE loss: 0.421, amex train: 0.649, val 0.000\n",
      "0, BCE loss: 0.387, amex train: 0.646, val 0.000\n",
      "0, BCE loss: 0.356, amex train: 0.634, val 0.000\n",
      "0, BCE loss: 0.370, amex train: 0.650, val 0.000\n",
      "0, BCE loss: 0.375, amex train: 0.649, val 0.000\n",
      "0, BCE loss: 0.363, amex train: 0.650, val 0.000\n",
      "0, BCE loss: 0.334, amex train: 0.656, val 0.000\n",
      "0, BCE loss: 0.320, amex train: 0.645, val 0.000\n",
      "0, BCE loss: 0.337, amex train: 0.646, val 0.000\n",
      "1, BCE loss: 0.337, amex train: 0.645, val 0.000\n",
      "1, BCE loss: 0.325, amex train: 0.657, val 0.000\n",
      "1, BCE loss: 0.306, amex train: 0.653, val 0.000\n",
      "1, BCE loss: 0.300, amex train: 0.674, val 0.000\n",
      "1, BCE loss: 0.313, amex train: 0.660, val 0.000\n",
      "1, BCE loss: 0.306, amex train: 0.685, val 0.000\n",
      "1, BCE loss: 0.302, amex train: 0.663, val 0.000\n",
      "1, BCE loss: 0.289, amex train: 0.681, val 0.000\n",
      "1, BCE loss: 0.306, amex train: 0.669, val 0.000\n",
      "1, BCE loss: 0.301, amex train: 0.681, val 0.000\n",
      "1, BCE loss: 0.279, amex train: 0.695, val 0.000\n",
      "1, BCE loss: 0.277, amex train: 0.700, val 0.000\n",
      "1, BCE loss: 0.282, amex train: 0.701, val 0.000\n",
      "1, BCE loss: 0.281, amex train: 0.707, val 0.000\n",
      "1, BCE loss: 0.269, amex train: 0.700, val 0.000\n",
      "1, BCE loss: 0.271, amex train: 0.717, val 0.000\n",
      "1, BCE loss: 0.270, amex train: 0.715, val 0.000\n",
      "1, BCE loss: 0.266, amex train: 0.713, val 0.000\n",
      "1, BCE loss: 0.269, amex train: 0.718, val 0.000\n",
      "1, BCE loss: 0.274, amex train: 0.712, val 0.000\n",
      "1, BCE loss: 0.271, amex train: 0.718, val 0.000\n",
      "2, BCE loss: 0.266, amex train: 0.711, val 0.000\n",
      "2, BCE loss: 0.272, amex train: 0.710, val 0.000\n",
      "2, BCE loss: 0.266, amex train: 0.714, val 0.000\n",
      "2, BCE loss: 0.260, amex train: 0.725, val 0.000\n",
      "2, BCE loss: 0.264, amex train: 0.721, val 0.000\n",
      "2, BCE loss: 0.255, amex train: 0.731, val 0.000\n",
      "2, BCE loss: 0.270, amex train: 0.708, val 0.000\n",
      "2, BCE loss: 0.259, amex train: 0.723, val 0.000\n",
      "2, BCE loss: 0.264, amex train: 0.718, val 0.000\n",
      "2, BCE loss: 0.263, amex train: 0.715, val 0.000\n",
      "2, BCE loss: 0.256, amex train: 0.722, val 0.000\n",
      "2, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "2, BCE loss: 0.250, amex train: 0.729, val 0.000\n",
      "2, BCE loss: 0.259, amex train: 0.734, val 0.000\n",
      "2, BCE loss: 0.253, amex train: 0.727, val 0.000\n",
      "2, BCE loss: 0.250, amex train: 0.741, val 0.000\n",
      "2, BCE loss: 0.247, amex train: 0.738, val 0.000\n",
      "2, BCE loss: 0.252, amex train: 0.739, val 0.000\n",
      "2, BCE loss: 0.252, amex train: 0.740, val 0.000\n",
      "2, BCE loss: 0.258, amex train: 0.734, val 0.000\n",
      "2, BCE loss: 0.258, amex train: 0.737, val 0.000\n",
      "3, BCE loss: 0.254, amex train: 0.735, val 0.000\n",
      "3, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "3, BCE loss: 0.255, amex train: 0.734, val 0.000\n",
      "3, BCE loss: 0.247, amex train: 0.751, val 0.000\n",
      "3, BCE loss: 0.250, amex train: 0.747, val 0.000\n",
      "3, BCE loss: 0.243, amex train: 0.754, val 0.000\n",
      "3, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "3, BCE loss: 0.246, amex train: 0.749, val 0.000\n",
      "3, BCE loss: 0.251, amex train: 0.742, val 0.000\n",
      "3, BCE loss: 0.251, amex train: 0.735, val 0.000\n",
      "3, BCE loss: 0.247, amex train: 0.738, val 0.000\n",
      "3, BCE loss: 0.251, amex train: 0.754, val 0.000\n",
      "3, BCE loss: 0.290, amex train: 0.751, val 0.000\n",
      "3, BCE loss: 0.406, amex train: 0.746, val 0.000\n",
      "3, BCE loss: 0.348, amex train: 0.741, val 0.000\n",
      "3, BCE loss: 0.296, amex train: 0.751, val 0.000\n",
      "3, BCE loss: 0.282, amex train: 0.748, val 0.000\n",
      "3, BCE loss: 0.320, amex train: 0.742, val 0.000\n",
      "3, BCE loss: 0.259, amex train: 0.745, val 0.000\n",
      "3, BCE loss: 0.312, amex train: 0.737, val 0.000\n",
      "3, BCE loss: 0.294, amex train: 0.734, val 0.000\n",
      "4, BCE loss: 0.262, amex train: 0.729, val 0.000\n",
      "4, BCE loss: 0.305, amex train: 0.728, val 0.000\n",
      "4, BCE loss: 0.271, amex train: 0.727, val 0.000\n",
      "4, BCE loss: 0.267, amex train: 0.747, val 0.000\n",
      "4, BCE loss: 0.293, amex train: 0.735, val 0.000\n",
      "4, BCE loss: 0.250, amex train: 0.746, val 0.000\n",
      "4, BCE loss: 0.280, amex train: 0.721, val 0.000\n",
      "4, BCE loss: 0.269, amex train: 0.739, val 0.000\n",
      "4, BCE loss: 0.259, amex train: 0.726, val 0.000\n",
      "4, BCE loss: 0.276, amex train: 0.728, val 0.000\n",
      "4, BCE loss: 0.261, amex train: 0.729, val 0.000\n",
      "4, BCE loss: 0.250, amex train: 0.739, val 0.000\n",
      "4, BCE loss: 0.264, amex train: 0.739, val 0.000\n",
      "4, BCE loss: 0.259, amex train: 0.740, val 0.000\n",
      "4, BCE loss: 0.252, amex train: 0.732, val 0.000\n",
      "4, BCE loss: 0.258, amex train: 0.744, val 0.000\n",
      "4, BCE loss: 0.246, amex train: 0.744, val 0.000\n",
      "4, BCE loss: 0.251, amex train: 0.740, val 0.000\n",
      "4, BCE loss: 0.259, amex train: 0.744, val 0.000\n",
      "4, BCE loss: 0.252, amex train: 0.741, val 0.000\n",
      "4, BCE loss: 0.263, amex train: 0.738, val 0.000\n",
      "5, BCE loss: 0.252, amex train: 0.741, val 0.000\n",
      "5, BCE loss: 0.255, amex train: 0.735, val 0.000\n",
      "5, BCE loss: 0.259, amex train: 0.736, val 0.000\n",
      "5, BCE loss: 0.244, amex train: 0.749, val 0.000\n",
      "5, BCE loss: 0.253, amex train: 0.747, val 0.000\n",
      "5, BCE loss: 0.243, amex train: 0.756, val 0.000\n",
      "5, BCE loss: 0.255, amex train: 0.728, val 0.000\n",
      "5, BCE loss: 0.248, amex train: 0.747, val 0.000\n",
      "5, BCE loss: 0.250, amex train: 0.738, val 0.000\n",
      "5, BCE loss: 0.253, amex train: 0.737, val 0.000\n",
      "5, BCE loss: 0.246, amex train: 0.738, val 0.000\n",
      "5, BCE loss: 0.244, amex train: 0.751, val 0.000\n",
      "5, BCE loss: 0.242, amex train: 0.751, val 0.000\n",
      "5, BCE loss: 0.250, amex train: 0.749, val 0.000\n",
      "5, BCE loss: 0.246, amex train: 0.742, val 0.000\n",
      "5, BCE loss: 0.241, amex train: 0.754, val 0.000\n",
      "5, BCE loss: 0.240, amex train: 0.752, val 0.000\n",
      "5, BCE loss: 0.242, amex train: 0.747, val 0.000\n",
      "5, BCE loss: 0.243, amex train: 0.754, val 0.000\n",
      "5, BCE loss: 0.248, amex train: 0.749, val 0.000\n",
      "5, BCE loss: 0.250, amex train: 0.750, val 0.000\n",
      "6, BCE loss: 0.244, amex train: 0.748, val 0.000\n",
      "6, BCE loss: 0.248, amex train: 0.740, val 0.000\n",
      "6, BCE loss: 0.247, amex train: 0.746, val 0.000\n",
      "6, BCE loss: 0.239, amex train: 0.759, val 0.000\n",
      "6, BCE loss: 0.243, amex train: 0.755, val 0.000\n",
      "6, BCE loss: 0.237, amex train: 0.762, val 0.000\n",
      "6, BCE loss: 0.250, amex train: 0.741, val 0.000\n",
      "6, BCE loss: 0.241, amex train: 0.756, val 0.000\n",
      "6, BCE loss: 0.245, amex train: 0.747, val 0.000\n",
      "6, BCE loss: 0.247, amex train: 0.744, val 0.000\n",
      "6, BCE loss: 0.243, amex train: 0.747, val 0.000\n",
      "6, BCE loss: 0.239, amex train: 0.761, val 0.000\n",
      "6, BCE loss: 0.235, amex train: 0.760, val 0.000\n",
      "6, BCE loss: 0.245, amex train: 0.752, val 0.000\n",
      "6, BCE loss: 0.240, amex train: 0.752, val 0.000\n",
      "6, BCE loss: 0.237, amex train: 0.758, val 0.000\n",
      "6, BCE loss: 0.235, amex train: 0.760, val 0.000\n",
      "6, BCE loss: 0.241, amex train: 0.757, val 0.000\n",
      "6, BCE loss: 0.242, amex train: 0.761, val 0.000\n",
      "6, BCE loss: 0.249, amex train: 0.756, val 0.000\n",
      "6, BCE loss: 0.252, amex train: 0.759, val 0.000\n",
      "7, BCE loss: 0.251, amex train: 0.756, val 0.000\n",
      "7, BCE loss: 0.262, amex train: 0.744, val 0.000\n",
      "7, BCE loss: 0.265, amex train: 0.750, val 0.000\n",
      "7, BCE loss: 0.252, amex train: 0.765, val 0.000\n",
      "7, BCE loss: 0.241, amex train: 0.757, val 0.000\n",
      "7, BCE loss: 0.245, amex train: 0.766, val 0.000\n",
      "7, BCE loss: 0.269, amex train: 0.740, val 0.000\n",
      "7, BCE loss: 0.244, amex train: 0.759, val 0.000\n",
      "7, BCE loss: 0.249, amex train: 0.748, val 0.000\n",
      "7, BCE loss: 0.261, amex train: 0.747, val 0.000\n",
      "7, BCE loss: 0.243, amex train: 0.749, val 0.000\n",
      "7, BCE loss: 0.249, amex train: 0.757, val 0.000\n",
      "7, BCE loss: 0.239, amex train: 0.758, val 0.000\n",
      "7, BCE loss: 0.249, amex train: 0.754, val 0.000\n",
      "7, BCE loss: 0.252, amex train: 0.752, val 0.000\n",
      "7, BCE loss: 0.238, amex train: 0.758, val 0.000\n",
      "7, BCE loss: 0.246, amex train: 0.758, val 0.000\n",
      "7, BCE loss: 0.245, amex train: 0.755, val 0.000\n",
      "7, BCE loss: 0.244, amex train: 0.759, val 0.000\n",
      "7, BCE loss: 0.259, amex train: 0.754, val 0.000\n",
      "7, BCE loss: 0.246, amex train: 0.753, val 0.000\n",
      "8, BCE loss: 0.252, amex train: 0.755, val 0.000\n",
      "8, BCE loss: 0.252, amex train: 0.746, val 0.000\n",
      "8, BCE loss: 0.246, amex train: 0.747, val 0.000\n",
      "8, BCE loss: 0.247, amex train: 0.763, val 0.000\n",
      "8, BCE loss: 0.241, amex train: 0.757, val 0.000\n",
      "8, BCE loss: 0.238, amex train: 0.766, val 0.000\n",
      "8, BCE loss: 0.252, amex train: 0.742, val 0.000\n",
      "8, BCE loss: 0.238, amex train: 0.761, val 0.000\n",
      "8, BCE loss: 0.244, amex train: 0.752, val 0.000\n",
      "8, BCE loss: 0.245, amex train: 0.749, val 0.000\n",
      "8, BCE loss: 0.241, amex train: 0.749, val 0.000\n",
      "8, BCE loss: 0.238, amex train: 0.760, val 0.000\n",
      "8, BCE loss: 0.234, amex train: 0.764, val 0.000\n",
      "8, BCE loss: 0.246, amex train: 0.757, val 0.000\n",
      "8, BCE loss: 0.238, amex train: 0.756, val 0.000\n",
      "8, BCE loss: 0.236, amex train: 0.762, val 0.000\n",
      "8, BCE loss: 0.237, amex train: 0.763, val 0.000\n",
      "8, BCE loss: 0.244, amex train: 0.762, val 0.000\n",
      "8, BCE loss: 0.244, amex train: 0.765, val 0.000\n",
      "8, BCE loss: 0.247, amex train: 0.759, val 0.000\n",
      "8, BCE loss: 0.245, amex train: 0.759, val 0.000\n",
      "9, BCE loss: 0.238, amex train: 0.757, val 0.000\n",
      "9, BCE loss: 0.243, amex train: 0.749, val 0.000\n",
      "9, BCE loss: 0.245, amex train: 0.751, val 0.000\n",
      "9, BCE loss: 0.238, amex train: 0.767, val 0.000\n",
      "9, BCE loss: 0.243, amex train: 0.760, val 0.000\n",
      "9, BCE loss: 0.241, amex train: 0.771, val 0.000\n",
      "9, BCE loss: 0.260, amex train: 0.744, val 0.000\n",
      "9, BCE loss: 0.256, amex train: 0.762, val 0.000\n",
      "9, BCE loss: 0.258, amex train: 0.753, val 0.000\n",
      "9, BCE loss: 0.245, amex train: 0.752, val 0.000\n",
      "9, BCE loss: 0.246, amex train: 0.753, val 0.000\n",
      "9, BCE loss: 0.253, amex train: 0.758, val 0.000\n",
      "9, BCE loss: 0.240, amex train: 0.764, val 0.000\n",
      "9, BCE loss: 0.243, amex train: 0.757, val 0.000\n",
      "9, BCE loss: 0.244, amex train: 0.756, val 0.000\n",
      "9, BCE loss: 0.241, amex train: 0.760, val 0.000\n",
      "9, BCE loss: 0.233, amex train: 0.763, val 0.000\n",
      "9, BCE loss: 0.238, amex train: 0.761, val 0.000\n",
      "9, BCE loss: 0.242, amex train: 0.763, val 0.000\n",
      "9, BCE loss: 0.247, amex train: 0.759, val 0.000\n",
      "9, BCE loss: 0.244, amex train: 0.759, val 0.000\n",
      "10, BCE loss: 0.241, amex train: 0.761, val 0.000\n",
      "10, BCE loss: 0.248, amex train: 0.748, val 0.000\n",
      "10, BCE loss: 0.244, amex train: 0.753, val 0.000\n",
      "10, BCE loss: 0.234, amex train: 0.767, val 0.000\n",
      "10, BCE loss: 0.239, amex train: 0.759, val 0.000\n",
      "10, BCE loss: 0.240, amex train: 0.771, val 0.000\n",
      "10, BCE loss: 0.270, amex train: 0.745, val 0.000\n",
      "10, BCE loss: 0.259, amex train: 0.763, val 0.000\n",
      "10, BCE loss: 0.242, amex train: 0.753, val 0.000\n",
      "10, BCE loss: 0.258, amex train: 0.751, val 0.000\n",
      "10, BCE loss: 0.272, amex train: 0.753, val 0.000\n",
      "10, BCE loss: 0.266, amex train: 0.759, val 0.000\n",
      "10, BCE loss: 0.253, amex train: 0.767, val 0.000\n",
      "10, BCE loss: 0.260, amex train: 0.756, val 0.000\n",
      "10, BCE loss: 0.257, amex train: 0.755, val 0.000\n",
      "10, BCE loss: 0.243, amex train: 0.760, val 0.000\n",
      "10, BCE loss: 0.235, amex train: 0.762, val 0.000\n",
      "10, BCE loss: 0.245, amex train: 0.758, val 0.000\n",
      "10, BCE loss: 0.244, amex train: 0.765, val 0.000\n",
      "10, BCE loss: 0.244, amex train: 0.757, val 0.000\n",
      "10, BCE loss: 0.247, amex train: 0.755, val 0.000\n",
      "11, BCE loss: 0.244, amex train: 0.756, val 0.000\n",
      "11, BCE loss: 0.250, amex train: 0.745, val 0.000\n",
      "11, BCE loss: 0.253, amex train: 0.753, val 0.000\n",
      "11, BCE loss: 0.245, amex train: 0.762, val 0.000\n",
      "11, BCE loss: 0.242, amex train: 0.758, val 0.000\n",
      "11, BCE loss: 0.240, amex train: 0.768, val 0.000\n",
      "11, BCE loss: 0.266, amex train: 0.742, val 0.000\n",
      "11, BCE loss: 0.264, amex train: 0.762, val 0.000\n",
      "11, BCE loss: 0.253, amex train: 0.754, val 0.000\n",
      "11, BCE loss: 0.245, amex train: 0.751, val 0.000\n",
      "11, BCE loss: 0.254, amex train: 0.752, val 0.000\n",
      "11, BCE loss: 0.240, amex train: 0.761, val 0.000\n",
      "11, BCE loss: 0.237, amex train: 0.765, val 0.000\n",
      "11, BCE loss: 0.251, amex train: 0.757, val 0.000\n",
      "11, BCE loss: 0.239, amex train: 0.757, val 0.000\n",
      "11, BCE loss: 0.237, amex train: 0.760, val 0.000\n",
      "11, BCE loss: 0.234, amex train: 0.762, val 0.000\n",
      "11, BCE loss: 0.237, amex train: 0.763, val 0.000\n",
      "11, BCE loss: 0.237, amex train: 0.763, val 0.000\n",
      "11, BCE loss: 0.242, amex train: 0.759, val 0.000\n",
      "11, BCE loss: 0.243, amex train: 0.755, val 0.000\n",
      "12, BCE loss: 0.238, amex train: 0.760, val 0.000\n",
      "12, BCE loss: 0.243, amex train: 0.752, val 0.000\n",
      "12, BCE loss: 0.242, amex train: 0.753, val 0.000\n",
      "12, BCE loss: 0.235, amex train: 0.766, val 0.000\n",
      "12, BCE loss: 0.239, amex train: 0.762, val 0.000\n",
      "12, BCE loss: 0.233, amex train: 0.771, val 0.000\n",
      "12, BCE loss: 0.249, amex train: 0.746, val 0.000\n",
      "12, BCE loss: 0.245, amex train: 0.765, val 0.000\n",
      "12, BCE loss: 0.262, amex train: 0.755, val 0.000\n",
      "12, BCE loss: 0.268, amex train: 0.751, val 0.000\n",
      "12, BCE loss: 0.250, amex train: 0.757, val 0.000\n",
      "12, BCE loss: 0.241, amex train: 0.760, val 0.000\n",
      "12, BCE loss: 0.253, amex train: 0.768, val 0.000\n",
      "12, BCE loss: 0.265, amex train: 0.760, val 0.000\n",
      "12, BCE loss: 0.239, amex train: 0.757, val 0.000\n",
      "12, BCE loss: 0.242, amex train: 0.762, val 0.000\n",
      "12, BCE loss: 0.250, amex train: 0.765, val 0.000\n",
      "12, BCE loss: 0.239, amex train: 0.763, val 0.000\n",
      "12, BCE loss: 0.251, amex train: 0.764, val 0.000\n",
      "12, BCE loss: 0.256, amex train: 0.760, val 0.000\n",
      "12, BCE loss: 0.245, amex train: 0.754, val 0.000\n",
      "13, BCE loss: 0.252, amex train: 0.755, val 0.000\n",
      "13, BCE loss: 0.248, amex train: 0.747, val 0.000\n",
      "13, BCE loss: 0.247, amex train: 0.751, val 0.000\n",
      "13, BCE loss: 0.244, amex train: 0.766, val 0.000\n",
      "13, BCE loss: 0.239, amex train: 0.760, val 0.000\n",
      "13, BCE loss: 0.238, amex train: 0.769, val 0.000\n",
      "13, BCE loss: 0.250, amex train: 0.743, val 0.000\n",
      "13, BCE loss: 0.237, amex train: 0.763, val 0.000\n",
      "13, BCE loss: 0.244, amex train: 0.750, val 0.000\n",
      "13, BCE loss: 0.244, amex train: 0.751, val 0.000\n",
      "13, BCE loss: 0.241, amex train: 0.753, val 0.000\n",
      "13, BCE loss: 0.236, amex train: 0.761, val 0.000\n",
      "13, BCE loss: 0.232, amex train: 0.768, val 0.000\n",
      "13, BCE loss: 0.243, amex train: 0.759, val 0.000\n",
      "13, BCE loss: 0.236, amex train: 0.758, val 0.000\n",
      "13, BCE loss: 0.236, amex train: 0.765, val 0.000\n",
      "13, BCE loss: 0.233, amex train: 0.766, val 0.000\n",
      "13, BCE loss: 0.235, amex train: 0.765, val 0.000\n",
      "13, BCE loss: 0.236, amex train: 0.768, val 0.000\n",
      "13, BCE loss: 0.246, amex train: 0.761, val 0.000\n",
      "13, BCE loss: 0.253, amex train: 0.763, val 0.000\n",
      "14, BCE loss: 0.250, amex train: 0.760, val 0.000\n",
      "14, BCE loss: 0.244, amex train: 0.756, val 0.000\n",
      "14, BCE loss: 0.244, amex train: 0.754, val 0.000\n",
      "14, BCE loss: 0.246, amex train: 0.769, val 0.000\n",
      "14, BCE loss: 0.252, amex train: 0.763, val 0.000\n",
      "14, BCE loss: 0.236, amex train: 0.772, val 0.000\n",
      "14, BCE loss: 0.249, amex train: 0.746, val 0.000\n",
      "14, BCE loss: 0.247, amex train: 0.765, val 0.000\n",
      "14, BCE loss: 0.244, amex train: 0.754, val 0.000\n",
      "14, BCE loss: 0.245, amex train: 0.753, val 0.000\n",
      "14, BCE loss: 0.249, amex train: 0.755, val 0.000\n",
      "14, BCE loss: 0.240, amex train: 0.760, val 0.000\n",
      "14, BCE loss: 0.234, amex train: 0.768, val 0.000\n",
      "14, BCE loss: 0.245, amex train: 0.760, val 0.000\n",
      "14, BCE loss: 0.238, amex train: 0.759, val 0.000\n",
      "14, BCE loss: 0.235, amex train: 0.763, val 0.000\n",
      "14, BCE loss: 0.232, amex train: 0.767, val 0.000\n",
      "14, BCE loss: 0.236, amex train: 0.766, val 0.000\n",
      "14, BCE loss: 0.235, amex train: 0.769, val 0.000\n",
      "14, BCE loss: 0.241, amex train: 0.761, val 0.000\n",
      "14, BCE loss: 0.243, amex train: 0.761, val 0.000\n",
      "15, BCE loss: 0.237, amex train: 0.760, val 0.000\n",
      "15, BCE loss: 0.241, amex train: 0.755, val 0.000\n",
      "15, BCE loss: 0.241, amex train: 0.754, val 0.000\n",
      "15, BCE loss: 0.233, amex train: 0.770, val 0.000\n",
      "15, BCE loss: 0.238, amex train: 0.765, val 0.000\n",
      "15, BCE loss: 0.231, amex train: 0.774, val 0.000\n",
      "15, BCE loss: 0.245, amex train: 0.750, val 0.000\n",
      "15, BCE loss: 0.237, amex train: 0.767, val 0.000\n",
      "15, BCE loss: 0.242, amex train: 0.757, val 0.000\n",
      "15, BCE loss: 0.250, amex train: 0.753, val 0.000\n",
      "15, BCE loss: 0.278, amex train: 0.756, val 0.000\n",
      "15, BCE loss: 0.331, amex train: 0.764, val 0.000\n",
      "15, BCE loss: 0.292, amex train: 0.768, val 0.000\n",
      "15, BCE loss: 0.278, amex train: 0.755, val 0.000\n",
      "15, BCE loss: 0.321, amex train: 0.756, val 0.000\n",
      "15, BCE loss: 0.279, amex train: 0.756, val 0.000\n",
      "15, BCE loss: 0.250, amex train: 0.758, val 0.000\n",
      "15, BCE loss: 0.278, amex train: 0.755, val 0.000\n",
      "15, BCE loss: 0.258, amex train: 0.754, val 0.000\n",
      "15, BCE loss: 0.254, amex train: 0.751, val 0.000\n",
      "15, BCE loss: 0.266, amex train: 0.747, val 0.000\n",
      "16, BCE loss: 0.247, amex train: 0.750, val 0.000\n",
      "16, BCE loss: 0.261, amex train: 0.743, val 0.000\n",
      "16, BCE loss: 0.249, amex train: 0.747, val 0.000\n",
      "16, BCE loss: 0.249, amex train: 0.764, val 0.000\n",
      "16, BCE loss: 0.244, amex train: 0.754, val 0.000\n",
      "16, BCE loss: 0.246, amex train: 0.768, val 0.000\n",
      "16, BCE loss: 0.264, amex train: 0.740, val 0.000\n",
      "16, BCE loss: 0.242, amex train: 0.756, val 0.000\n",
      "16, BCE loss: 0.260, amex train: 0.745, val 0.000\n",
      "16, BCE loss: 0.247, amex train: 0.746, val 0.000\n",
      "16, BCE loss: 0.251, amex train: 0.748, val 0.000\n",
      "16, BCE loss: 0.240, amex train: 0.754, val 0.000\n",
      "16, BCE loss: 0.244, amex train: 0.759, val 0.000\n",
      "16, BCE loss: 0.251, amex train: 0.757, val 0.000\n",
      "16, BCE loss: 0.243, amex train: 0.754, val 0.000\n",
      "16, BCE loss: 0.245, amex train: 0.759, val 0.000\n",
      "16, BCE loss: 0.236, amex train: 0.762, val 0.000\n",
      "16, BCE loss: 0.243, amex train: 0.757, val 0.000\n",
      "16, BCE loss: 0.239, amex train: 0.764, val 0.000\n",
      "16, BCE loss: 0.245, amex train: 0.757, val 0.000\n",
      "16, BCE loss: 0.246, amex train: 0.753, val 0.000\n",
      "17, BCE loss: 0.242, amex train: 0.756, val 0.000\n",
      "17, BCE loss: 0.244, amex train: 0.747, val 0.000\n",
      "17, BCE loss: 0.245, amex train: 0.754, val 0.000\n",
      "17, BCE loss: 0.236, amex train: 0.768, val 0.000\n",
      "17, BCE loss: 0.241, amex train: 0.762, val 0.000\n",
      "17, BCE loss: 0.235, amex train: 0.770, val 0.000\n",
      "17, BCE loss: 0.248, amex train: 0.743, val 0.000\n",
      "17, BCE loss: 0.239, amex train: 0.763, val 0.000\n",
      "17, BCE loss: 0.243, amex train: 0.755, val 0.000\n",
      "17, BCE loss: 0.243, amex train: 0.751, val 0.000\n",
      "17, BCE loss: 0.240, amex train: 0.754, val 0.000\n",
      "17, BCE loss: 0.237, amex train: 0.764, val 0.000\n",
      "17, BCE loss: 0.233, amex train: 0.769, val 0.000\n",
      "17, BCE loss: 0.244, amex train: 0.760, val 0.000\n",
      "17, BCE loss: 0.237, amex train: 0.757, val 0.000\n",
      "17, BCE loss: 0.237, amex train: 0.764, val 0.000\n",
      "17, BCE loss: 0.237, amex train: 0.767, val 0.000\n",
      "17, BCE loss: 0.238, amex train: 0.765, val 0.000\n",
      "17, BCE loss: 0.239, amex train: 0.767, val 0.000\n",
      "17, BCE loss: 0.249, amex train: 0.760, val 0.000\n",
      "17, BCE loss: 0.244, amex train: 0.763, val 0.000\n",
      "18, BCE loss: 0.239, amex train: 0.755, val 0.000\n",
      "18, BCE loss: 0.247, amex train: 0.753, val 0.000\n",
      "18, BCE loss: 0.243, amex train: 0.754, val 0.000\n",
      "18, BCE loss: 0.237, amex train: 0.770, val 0.000\n",
      "18, BCE loss: 0.241, amex train: 0.764, val 0.000\n",
      "18, BCE loss: 0.233, amex train: 0.774, val 0.000\n",
      "18, BCE loss: 0.248, amex train: 0.748, val 0.000\n",
      "18, BCE loss: 0.238, amex train: 0.767, val 0.000\n",
      "18, BCE loss: 0.243, amex train: 0.757, val 0.000\n",
      "18, BCE loss: 0.243, amex train: 0.755, val 0.000\n",
      "18, BCE loss: 0.239, amex train: 0.757, val 0.000\n",
      "18, BCE loss: 0.237, amex train: 0.763, val 0.000\n",
      "18, BCE loss: 0.231, amex train: 0.772, val 0.000\n",
      "18, BCE loss: 0.243, amex train: 0.761, val 0.000\n",
      "18, BCE loss: 0.237, amex train: 0.761, val 0.000\n",
      "18, BCE loss: 0.236, amex train: 0.765, val 0.000\n",
      "18, BCE loss: 0.234, amex train: 0.768, val 0.000\n",
      "18, BCE loss: 0.240, amex train: 0.766, val 0.000\n",
      "18, BCE loss: 0.237, amex train: 0.768, val 0.000\n",
      "18, BCE loss: 0.241, amex train: 0.762, val 0.000\n",
      "18, BCE loss: 0.247, amex train: 0.764, val 0.000\n",
      "19, BCE loss: 0.243, amex train: 0.761, val 0.000\n",
      "19, BCE loss: 0.242, amex train: 0.755, val 0.000\n",
      "19, BCE loss: 0.243, amex train: 0.753, val 0.000\n",
      "19, BCE loss: 0.239, amex train: 0.769, val 0.000\n",
      "19, BCE loss: 0.242, amex train: 0.766, val 0.000\n",
      "19, BCE loss: 0.232, amex train: 0.776, val 0.000\n",
      "19, BCE loss: 0.249, amex train: 0.749, val 0.000\n",
      "19, BCE loss: 0.243, amex train: 0.767, val 0.000\n",
      "19, BCE loss: 0.244, amex train: 0.756, val 0.000\n",
      "19, BCE loss: 0.242, amex train: 0.755, val 0.000\n",
      "19, BCE loss: 0.245, amex train: 0.758, val 0.000\n",
      "19, BCE loss: 0.241, amex train: 0.764, val 0.000\n",
      "19, BCE loss: 0.232, amex train: 0.772, val 0.000\n",
      "19, BCE loss: 0.243, amex train: 0.760, val 0.000\n",
      "19, BCE loss: 0.240, amex train: 0.762, val 0.000\n",
      "19, BCE loss: 0.238, amex train: 0.765, val 0.000\n",
      "19, BCE loss: 0.233, amex train: 0.766, val 0.000\n",
      "19, BCE loss: 0.236, amex train: 0.765, val 0.000\n",
      "19, BCE loss: 0.239, amex train: 0.769, val 0.000\n",
      "19, BCE loss: 0.246, amex train: 0.761, val 0.000\n",
      "19, BCE loss: 0.244, amex train: 0.764, val 0.000\n",
      "20, BCE loss: 0.238, amex train: 0.762, val 0.000\n",
      "20, BCE loss: 0.247, amex train: 0.756, val 0.000\n",
      "20, BCE loss: 0.247, amex train: 0.755, val 0.000\n",
      "20, BCE loss: 0.237, amex train: 0.770, val 0.000\n",
      "20, BCE loss: 0.238, amex train: 0.765, val 0.000\n",
      "20, BCE loss: 0.241, amex train: 0.777, val 0.000\n",
      "20, BCE loss: 0.277, amex train: 0.747, val 0.000\n",
      "20, BCE loss: 0.267, amex train: 0.768, val 0.000\n",
      "20, BCE loss: 0.245, amex train: 0.757, val 0.000\n",
      "20, BCE loss: 0.280, amex train: 0.753, val 0.000\n",
      "20, BCE loss: 0.292, amex train: 0.753, val 0.000\n",
      "20, BCE loss: 0.280, amex train: 0.760, val 0.000\n",
      "20, BCE loss: 0.261, amex train: 0.768, val 0.000\n",
      "20, BCE loss: 0.273, amex train: 0.753, val 0.000\n",
      "20, BCE loss: 0.561, amex train: 0.750, val 0.000\n",
      "20, BCE loss: 0.327, amex train: 0.760, val 0.000\n",
      "20, BCE loss: 0.281, amex train: 0.760, val 0.000\n",
      "20, BCE loss: 0.288, amex train: 0.754, val 0.000\n",
      "20, BCE loss: 0.399, amex train: 0.754, val 0.000\n",
      "20, BCE loss: 0.771, amex train: 0.750, val 0.000\n",
      "20, BCE loss: 0.661, amex train: 0.745, val 0.000\n",
      "21, BCE loss: 0.480, amex train: 0.742, val 0.000\n",
      "21, BCE loss: 0.429, amex train: 0.738, val 0.000\n",
      "21, BCE loss: 0.511, amex train: 0.734, val 0.000\n",
      "21, BCE loss: 0.518, amex train: 0.746, val 0.000\n",
      "21, BCE loss: 0.447, amex train: 0.737, val 0.000\n",
      "21, BCE loss: 0.403, amex train: 0.740, val 0.000\n",
      "21, BCE loss: 0.411, amex train: 0.716, val 0.000\n",
      "21, BCE loss: 0.405, amex train: 0.730, val 0.000\n",
      "21, BCE loss: 0.363, amex train: 0.711, val 0.000\n",
      "21, BCE loss: 0.286, amex train: 0.711, val 0.000\n",
      "21, BCE loss: 0.422, amex train: 0.709, val 0.000\n",
      "21, BCE loss: 0.273, amex train: 0.724, val 0.000\n",
      "21, BCE loss: 0.301, amex train: 0.723, val 0.000\n",
      "21, BCE loss: 0.320, amex train: 0.723, val 0.000\n",
      "21, BCE loss: 0.293, amex train: 0.712, val 0.000\n",
      "21, BCE loss: 0.285, amex train: 0.728, val 0.000\n",
      "21, BCE loss: 0.291, amex train: 0.727, val 0.000\n",
      "21, BCE loss: 0.298, amex train: 0.719, val 0.000\n",
      "21, BCE loss: 0.285, amex train: 0.730, val 0.000\n",
      "21, BCE loss: 0.283, amex train: 0.723, val 0.000\n",
      "21, BCE loss: 0.301, amex train: 0.717, val 0.000\n",
      "22, BCE loss: 0.292, amex train: 0.718, val 0.000\n",
      "22, BCE loss: 0.281, amex train: 0.719, val 0.000\n",
      "22, BCE loss: 0.283, amex train: 0.720, val 0.000\n",
      "22, BCE loss: 0.281, amex train: 0.740, val 0.000\n",
      "22, BCE loss: 0.277, amex train: 0.730, val 0.000\n",
      "22, BCE loss: 0.266, amex train: 0.737, val 0.000\n",
      "22, BCE loss: 0.280, amex train: 0.716, val 0.000\n",
      "22, BCE loss: 0.272, amex train: 0.730, val 0.000\n",
      "22, BCE loss: 0.273, amex train: 0.712, val 0.000\n",
      "22, BCE loss: 0.269, amex train: 0.720, val 0.000\n",
      "22, BCE loss: 0.264, amex train: 0.729, val 0.000\n",
      "22, BCE loss: 0.262, amex train: 0.736, val 0.000\n",
      "22, BCE loss: 0.257, amex train: 0.733, val 0.000\n",
      "22, BCE loss: 0.264, amex train: 0.734, val 0.000\n",
      "22, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "22, BCE loss: 0.258, amex train: 0.741, val 0.000\n",
      "22, BCE loss: 0.253, amex train: 0.738, val 0.000\n",
      "22, BCE loss: 0.258, amex train: 0.735, val 0.000\n",
      "22, BCE loss: 0.261, amex train: 0.739, val 0.000\n",
      "22, BCE loss: 0.261, amex train: 0.732, val 0.000\n",
      "22, BCE loss: 0.264, amex train: 0.732, val 0.000\n",
      "23, BCE loss: 0.258, amex train: 0.736, val 0.000\n",
      "23, BCE loss: 0.261, amex train: 0.728, val 0.000\n",
      "23, BCE loss: 0.259, amex train: 0.735, val 0.000\n",
      "23, BCE loss: 0.251, amex train: 0.746, val 0.000\n",
      "23, BCE loss: 0.255, amex train: 0.738, val 0.000\n",
      "23, BCE loss: 0.249, amex train: 0.752, val 0.000\n",
      "23, BCE loss: 0.261, amex train: 0.726, val 0.000\n",
      "23, BCE loss: 0.253, amex train: 0.741, val 0.000\n",
      "23, BCE loss: 0.256, amex train: 0.732, val 0.000\n",
      "23, BCE loss: 0.256, amex train: 0.731, val 0.000\n",
      "23, BCE loss: 0.251, amex train: 0.735, val 0.000\n",
      "23, BCE loss: 0.248, amex train: 0.743, val 0.000\n",
      "23, BCE loss: 0.245, amex train: 0.746, val 0.000\n",
      "23, BCE loss: 0.255, amex train: 0.748, val 0.000\n",
      "23, BCE loss: 0.248, amex train: 0.740, val 0.000\n",
      "23, BCE loss: 0.248, amex train: 0.749, val 0.000\n",
      "23, BCE loss: 0.244, amex train: 0.746, val 0.000\n",
      "23, BCE loss: 0.248, amex train: 0.744, val 0.000\n",
      "23, BCE loss: 0.249, amex train: 0.749, val 0.000\n",
      "23, BCE loss: 0.252, amex train: 0.743, val 0.000\n",
      "23, BCE loss: 0.255, amex train: 0.739, val 0.000\n",
      "24, BCE loss: 0.249, amex train: 0.746, val 0.000\n",
      "24, BCE loss: 0.253, amex train: 0.739, val 0.000\n",
      "24, BCE loss: 0.253, amex train: 0.741, val 0.000\n",
      "24, BCE loss: 0.244, amex train: 0.756, val 0.000\n",
      "24, BCE loss: 0.248, amex train: 0.750, val 0.000\n",
      "24, BCE loss: 0.243, amex train: 0.759, val 0.000\n",
      "24, BCE loss: 0.255, amex train: 0.737, val 0.000\n",
      "24, BCE loss: 0.246, amex train: 0.751, val 0.000\n",
      "24, BCE loss: 0.251, amex train: 0.744, val 0.000\n",
      "24, BCE loss: 0.250, amex train: 0.739, val 0.000\n",
      "24, BCE loss: 0.246, amex train: 0.741, val 0.000\n",
      "24, BCE loss: 0.243, amex train: 0.752, val 0.000\n",
      "24, BCE loss: 0.240, amex train: 0.757, val 0.000\n",
      "24, BCE loss: 0.250, amex train: 0.753, val 0.000\n",
      "24, BCE loss: 0.243, amex train: 0.749, val 0.000\n",
      "24, BCE loss: 0.242, amex train: 0.757, val 0.000\n",
      "24, BCE loss: 0.239, amex train: 0.754, val 0.000\n",
      "24, BCE loss: 0.243, amex train: 0.752, val 0.000\n",
      "24, BCE loss: 0.244, amex train: 0.759, val 0.000\n",
      "24, BCE loss: 0.247, amex train: 0.753, val 0.000\n",
      "24, BCE loss: 0.250, amex train: 0.753, val 0.000\n",
      "25, BCE loss: 0.244, amex train: 0.752, val 0.000\n",
      "25, BCE loss: 0.248, amex train: 0.742, val 0.000\n",
      "25, BCE loss: 0.248, amex train: 0.747, val 0.000\n",
      "25, BCE loss: 0.239, amex train: 0.765, val 0.000\n",
      "25, BCE loss: 0.243, amex train: 0.758, val 0.000\n",
      "25, BCE loss: 0.238, amex train: 0.765, val 0.000\n",
      "25, BCE loss: 0.252, amex train: 0.742, val 0.000\n",
      "25, BCE loss: 0.243, amex train: 0.760, val 0.000\n",
      "25, BCE loss: 0.246, amex train: 0.753, val 0.000\n",
      "25, BCE loss: 0.247, amex train: 0.749, val 0.000\n",
      "25, BCE loss: 0.246, amex train: 0.752, val 0.000\n",
      "25, BCE loss: 0.241, amex train: 0.759, val 0.000\n",
      "25, BCE loss: 0.236, amex train: 0.763, val 0.000\n",
      "25, BCE loss: 0.247, amex train: 0.756, val 0.000\n",
      "25, BCE loss: 0.240, amex train: 0.753, val 0.000\n",
      "25, BCE loss: 0.240, amex train: 0.762, val 0.000\n",
      "25, BCE loss: 0.240, amex train: 0.761, val 0.000\n",
      "25, BCE loss: 0.243, amex train: 0.759, val 0.000\n",
      "25, BCE loss: 0.240, amex train: 0.761, val 0.000\n",
      "25, BCE loss: 0.248, amex train: 0.756, val 0.000\n",
      "25, BCE loss: 0.258, amex train: 0.758, val 0.000\n",
      "26, BCE loss: 0.253, amex train: 0.757, val 0.000\n",
      "26, BCE loss: 0.248, amex train: 0.747, val 0.000\n",
      "26, BCE loss: 0.250, amex train: 0.751, val 0.000\n",
      "26, BCE loss: 0.244, amex train: 0.766, val 0.000\n",
      "26, BCE loss: 0.242, amex train: 0.760, val 0.000\n",
      "26, BCE loss: 0.245, amex train: 0.769, val 0.000\n",
      "26, BCE loss: 0.258, amex train: 0.745, val 0.000\n",
      "26, BCE loss: 0.241, amex train: 0.762, val 0.000\n",
      "26, BCE loss: 0.254, amex train: 0.753, val 0.000\n",
      "26, BCE loss: 0.247, amex train: 0.749, val 0.000\n",
      "26, BCE loss: 0.248, amex train: 0.753, val 0.000\n",
      "26, BCE loss: 0.239, amex train: 0.759, val 0.000\n",
      "26, BCE loss: 0.241, amex train: 0.761, val 0.000\n",
      "26, BCE loss: 0.251, amex train: 0.757, val 0.000\n",
      "26, BCE loss: 0.241, amex train: 0.756, val 0.000\n",
      "26, BCE loss: 0.244, amex train: 0.760, val 0.000\n",
      "26, BCE loss: 0.237, amex train: 0.763, val 0.000\n",
      "26, BCE loss: 0.243, amex train: 0.760, val 0.000\n",
      "26, BCE loss: 0.239, amex train: 0.763, val 0.000\n",
      "26, BCE loss: 0.246, amex train: 0.758, val 0.000\n",
      "26, BCE loss: 0.247, amex train: 0.757, val 0.000\n",
      "27, BCE loss: 0.242, amex train: 0.755, val 0.000\n",
      "27, BCE loss: 0.246, amex train: 0.747, val 0.000\n",
      "27, BCE loss: 0.246, amex train: 0.752, val 0.000\n",
      "27, BCE loss: 0.237, amex train: 0.765, val 0.000\n",
      "27, BCE loss: 0.242, amex train: 0.762, val 0.000\n",
      "27, BCE loss: 0.237, amex train: 0.768, val 0.000\n",
      "27, BCE loss: 0.249, amex train: 0.745, val 0.000\n",
      "27, BCE loss: 0.240, amex train: 0.762, val 0.000\n",
      "27, BCE loss: 0.246, amex train: 0.756, val 0.000\n",
      "27, BCE loss: 0.245, amex train: 0.751, val 0.000\n",
      "27, BCE loss: 0.242, amex train: 0.753, val 0.000\n",
      "27, BCE loss: 0.239, amex train: 0.761, val 0.000\n",
      "27, BCE loss: 0.234, amex train: 0.768, val 0.000\n",
      "27, BCE loss: 0.247, amex train: 0.758, val 0.000\n",
      "27, BCE loss: 0.239, amex train: 0.758, val 0.000\n",
      "27, BCE loss: 0.237, amex train: 0.765, val 0.000\n",
      "27, BCE loss: 0.240, amex train: 0.765, val 0.000\n",
      "27, BCE loss: 0.241, amex train: 0.764, val 0.000\n",
      "27, BCE loss: 0.238, amex train: 0.766, val 0.000\n",
      "27, BCE loss: 0.251, amex train: 0.759, val 0.000\n",
      "27, BCE loss: 0.250, amex train: 0.761, val 0.000\n",
      "28, BCE loss: 0.240, amex train: 0.759, val 0.000\n",
      "28, BCE loss: 0.247, amex train: 0.749, val 0.000\n",
      "28, BCE loss: 0.247, amex train: 0.753, val 0.000\n",
      "28, BCE loss: 0.236, amex train: 0.768, val 0.000\n",
      "28, BCE loss: 0.242, amex train: 0.764, val 0.000\n",
      "28, BCE loss: 0.238, amex train: 0.771, val 0.000\n",
      "28, BCE loss: 0.249, amex train: 0.746, val 0.000\n",
      "28, BCE loss: 0.240, amex train: 0.765, val 0.000\n",
      "28, BCE loss: 0.247, amex train: 0.756, val 0.000\n",
      "28, BCE loss: 0.246, amex train: 0.755, val 0.000\n",
      "28, BCE loss: 0.241, amex train: 0.754, val 0.000\n",
      "28, BCE loss: 0.240, amex train: 0.761, val 0.000\n",
      "28, BCE loss: 0.234, amex train: 0.768, val 0.000\n",
      "28, BCE loss: 0.245, amex train: 0.760, val 0.000\n",
      "28, BCE loss: 0.241, amex train: 0.759, val 0.000\n",
      "28, BCE loss: 0.237, amex train: 0.766, val 0.000\n",
      "28, BCE loss: 0.238, amex train: 0.766, val 0.000\n",
      "28, BCE loss: 0.243, amex train: 0.765, val 0.000\n",
      "28, BCE loss: 0.239, amex train: 0.767, val 0.000\n",
      "28, BCE loss: 0.249, amex train: 0.759, val 0.000\n",
      "28, BCE loss: 0.250, amex train: 0.760, val 0.000\n",
      "29, BCE loss: 0.240, amex train: 0.761, val 0.000\n",
      "29, BCE loss: 0.247, amex train: 0.753, val 0.000\n",
      "29, BCE loss: 0.247, amex train: 0.753, val 0.000\n",
      "29, BCE loss: 0.236, amex train: 0.770, val 0.000\n",
      "29, BCE loss: 0.242, amex train: 0.764, val 0.000\n",
      "29, BCE loss: 0.239, amex train: 0.771, val 0.000\n",
      "29, BCE loss: 0.250, amex train: 0.747, val 0.000\n",
      "29, BCE loss: 0.240, amex train: 0.765, val 0.000\n",
      "29, BCE loss: 0.247, amex train: 0.757, val 0.000\n",
      "29, BCE loss: 0.247, amex train: 0.755, val 0.000\n",
      "29, BCE loss: 0.241, amex train: 0.755, val 0.000\n",
      "29, BCE loss: 0.240, amex train: 0.762, val 0.000\n",
      "29, BCE loss: 0.235, amex train: 0.768, val 0.000\n",
      "29, BCE loss: 0.244, amex train: 0.760, val 0.000\n",
      "29, BCE loss: 0.240, amex train: 0.761, val 0.000\n",
      "29, BCE loss: 0.239, amex train: 0.767, val 0.000\n",
      "29, BCE loss: 0.236, amex train: 0.767, val 0.000\n",
      "29, BCE loss: 0.241, amex train: 0.766, val 0.000\n",
      "29, BCE loss: 0.239, amex train: 0.767, val 0.000\n",
      "29, BCE loss: 0.245, amex train: 0.761, val 0.000\n",
      "29, BCE loss: 0.250, amex train: 0.764, val 0.000\n",
      "30, BCE loss: 0.240, amex train: 0.760, val 0.000\n",
      "30, BCE loss: 0.245, amex train: 0.753, val 0.000\n",
      "30, BCE loss: 0.248, amex train: 0.753, val 0.000\n",
      "30, BCE loss: 0.236, amex train: 0.771, val 0.000\n",
      "30, BCE loss: 0.243, amex train: 0.764, val 0.000\n",
      "30, BCE loss: 0.236, amex train: 0.773, val 0.000\n",
      "30, BCE loss: 0.251, amex train: 0.749, val 0.000\n",
      "30, BCE loss: 0.240, amex train: 0.766, val 0.000\n",
      "30, BCE loss: 0.244, amex train: 0.757, val 0.000\n",
      "30, BCE loss: 0.249, amex train: 0.758, val 0.000\n",
      "30, BCE loss: 0.254, amex train: 0.756, val 0.000\n",
      "30, BCE loss: 0.256, amex train: 0.761, val 0.000\n",
      "30, BCE loss: 0.236, amex train: 0.771, val 0.000\n",
      "30, BCE loss: 0.254, amex train: 0.758, val 0.000\n",
      "30, BCE loss: 0.259, amex train: 0.758, val 0.000\n",
      "30, BCE loss: 0.260, amex train: 0.769, val 0.000\n",
      "30, BCE loss: 0.238, amex train: 0.767, val 0.000\n",
      "30, BCE loss: 0.251, amex train: 0.765, val 0.000\n",
      "30, BCE loss: 0.267, amex train: 0.767, val 0.000\n",
      "30, BCE loss: 0.268, amex train: 0.760, val 0.000\n",
      "30, BCE loss: 0.265, amex train: 0.761, val 0.000\n",
      "31, BCE loss: 0.261, amex train: 0.761, val 0.000\n",
      "31, BCE loss: 0.265, amex train: 0.749, val 0.000\n",
      "31, BCE loss: 0.268, amex train: 0.753, val 0.000\n",
      "31, BCE loss: 0.254, amex train: 0.767, val 0.000\n",
      "31, BCE loss: 0.254, amex train: 0.761, val 0.000\n",
      "31, BCE loss: 0.254, amex train: 0.771, val 0.000\n",
      "31, BCE loss: 0.266, amex train: 0.746, val 0.000\n",
      "31, BCE loss: 0.255, amex train: 0.762, val 0.000\n",
      "31, BCE loss: 0.248, amex train: 0.755, val 0.000\n",
      "31, BCE loss: 0.254, amex train: 0.752, val 0.000\n",
      "31, BCE loss: 0.250, amex train: 0.753, val 0.000\n",
      "31, BCE loss: 0.245, amex train: 0.758, val 0.000\n",
      "31, BCE loss: 0.239, amex train: 0.765, val 0.000\n",
      "31, BCE loss: 0.250, amex train: 0.758, val 0.000\n",
      "31, BCE loss: 0.247, amex train: 0.758, val 0.000\n",
      "31, BCE loss: 0.241, amex train: 0.763, val 0.000\n",
      "31, BCE loss: 0.241, amex train: 0.764, val 0.000\n",
      "31, BCE loss: 0.247, amex train: 0.764, val 0.000\n",
      "31, BCE loss: 0.243, amex train: 0.766, val 0.000\n",
      "31, BCE loss: 0.253, amex train: 0.758, val 0.000\n",
      "31, BCE loss: 0.249, amex train: 0.754, val 0.000\n",
      "32, BCE loss: 0.246, amex train: 0.757, val 0.000\n",
      "32, BCE loss: 0.251, amex train: 0.750, val 0.000\n",
      "32, BCE loss: 0.247, amex train: 0.752, val 0.000\n",
      "32, BCE loss: 0.244, amex train: 0.767, val 0.000\n",
      "32, BCE loss: 0.243, amex train: 0.762, val 0.000\n",
      "32, BCE loss: 0.237, amex train: 0.769, val 0.000\n",
      "32, BCE loss: 0.252, amex train: 0.747, val 0.000\n",
      "32, BCE loss: 0.241, amex train: 0.765, val 0.000\n",
      "32, BCE loss: 0.246, amex train: 0.755, val 0.000\n",
      "32, BCE loss: 0.245, amex train: 0.755, val 0.000\n",
      "32, BCE loss: 0.244, amex train: 0.753, val 0.000\n",
      "32, BCE loss: 0.239, amex train: 0.761, val 0.000\n",
      "32, BCE loss: 0.234, amex train: 0.769, val 0.000\n",
      "32, BCE loss: 0.246, amex train: 0.760, val 0.000\n",
      "32, BCE loss: 0.240, amex train: 0.758, val 0.000\n",
      "32, BCE loss: 0.238, amex train: 0.767, val 0.000\n",
      "32, BCE loss: 0.236, amex train: 0.767, val 0.000\n",
      "32, BCE loss: 0.241, amex train: 0.763, val 0.000\n",
      "32, BCE loss: 0.240, amex train: 0.767, val 0.000\n",
      "32, BCE loss: 0.246, amex train: 0.761, val 0.000\n",
      "32, BCE loss: 0.248, amex train: 0.760, val 0.000\n",
      "33, BCE loss: 0.241, amex train: 0.761, val 0.000\n",
      "33, BCE loss: 0.245, amex train: 0.754, val 0.000\n",
      "33, BCE loss: 0.247, amex train: 0.754, val 0.000\n",
      "33, BCE loss: 0.238, amex train: 0.771, val 0.000\n",
      "33, BCE loss: 0.241, amex train: 0.765, val 0.000\n",
      "33, BCE loss: 0.238, amex train: 0.773, val 0.000\n",
      "33, BCE loss: 0.256, amex train: 0.748, val 0.000\n",
      "33, BCE loss: 0.242, amex train: 0.766, val 0.000\n",
      "33, BCE loss: 0.244, amex train: 0.759, val 0.000\n",
      "33, BCE loss: 0.248, amex train: 0.758, val 0.000\n",
      "33, BCE loss: 0.245, amex train: 0.756, val 0.000\n",
      "33, BCE loss: 0.238, amex train: 0.764, val 0.000\n",
      "33, BCE loss: 0.235, amex train: 0.769, val 0.000\n",
      "33, BCE loss: 0.248, amex train: 0.761, val 0.000\n",
      "33, BCE loss: 0.239, amex train: 0.761, val 0.000\n",
      "33, BCE loss: 0.239, amex train: 0.769, val 0.000\n",
      "33, BCE loss: 0.236, amex train: 0.769, val 0.000\n",
      "33, BCE loss: 0.238, amex train: 0.766, val 0.000\n",
      "33, BCE loss: 0.239, amex train: 0.771, val 0.000\n",
      "33, BCE loss: 0.246, amex train: 0.760, val 0.000\n",
      "33, BCE loss: 0.246, amex train: 0.765, val 0.000\n",
      "34, BCE loss: 0.241, amex train: 0.762, val 0.000\n",
      "34, BCE loss: 0.245, amex train: 0.755, val 0.000\n",
      "34, BCE loss: 0.246, amex train: 0.754, val 0.000\n",
      "34, BCE loss: 0.237, amex train: 0.770, val 0.000\n",
      "34, BCE loss: 0.242, amex train: 0.766, val 0.000\n",
      "34, BCE loss: 0.235, amex train: 0.774, val 0.000\n",
      "34, BCE loss: 0.251, amex train: 0.750, val 0.000\n",
      "34, BCE loss: 0.245, amex train: 0.768, val 0.000\n",
      "34, BCE loss: 0.246, amex train: 0.758, val 0.000\n",
      "34, BCE loss: 0.244, amex train: 0.759, val 0.000\n",
      "34, BCE loss: 0.244, amex train: 0.756, val 0.000\n",
      "34, BCE loss: 0.243, amex train: 0.763, val 0.000\n",
      "34, BCE loss: 0.240, amex train: 0.772, val 0.000\n",
      "34, BCE loss: 0.249, amex train: 0.761, val 0.000\n",
      "34, BCE loss: 0.239, amex train: 0.764, val 0.000\n",
      "34, BCE loss: 0.238, amex train: 0.768, val 0.000\n",
      "34, BCE loss: 0.240, amex train: 0.768, val 0.000\n",
      "34, BCE loss: 0.246, amex train: 0.767, val 0.000\n",
      "34, BCE loss: 0.239, amex train: 0.771, val 0.000\n",
      "34, BCE loss: 0.246, amex train: 0.760, val 0.000\n",
      "34, BCE loss: 0.256, amex train: 0.762, val 0.000\n",
      "35, BCE loss: 0.253, amex train: 0.763, val 0.000\n",
      "35, BCE loss: 0.247, amex train: 0.755, val 0.000\n",
      "35, BCE loss: 0.249, amex train: 0.754, val 0.000\n",
      "35, BCE loss: 0.252, amex train: 0.769, val 0.000\n",
      "35, BCE loss: 0.257, amex train: 0.766, val 0.000\n",
      "35, BCE loss: 0.240, amex train: 0.773, val 0.000\n",
      "35, BCE loss: 0.263, amex train: 0.750, val 0.000\n",
      "35, BCE loss: 0.265, amex train: 0.766, val 0.000\n",
      "35, BCE loss: 0.251, amex train: 0.758, val 0.000\n",
      "35, BCE loss: 0.254, amex train: 0.757, val 0.000\n",
      "35, BCE loss: 0.261, amex train: 0.757, val 0.000\n",
      "35, BCE loss: 0.251, amex train: 0.762, val 0.000\n",
      "35, BCE loss: 0.236, amex train: 0.768, val 0.000\n",
      "35, BCE loss: 0.248, amex train: 0.760, val 0.000\n",
      "35, BCE loss: 0.246, amex train: 0.760, val 0.000\n",
      "35, BCE loss: 0.241, amex train: 0.768, val 0.000\n",
      "35, BCE loss: 0.237, amex train: 0.769, val 0.000\n",
      "35, BCE loss: 0.242, amex train: 0.765, val 0.000\n",
      "35, BCE loss: 0.243, amex train: 0.769, val 0.000\n",
      "35, BCE loss: 0.247, amex train: 0.760, val 0.000\n",
      "35, BCE loss: 0.246, amex train: 0.758, val 0.000\n",
      "36, BCE loss: 0.242, amex train: 0.759, val 0.000\n",
      "36, BCE loss: 0.247, amex train: 0.753, val 0.000\n",
      "36, BCE loss: 0.247, amex train: 0.755, val 0.000\n",
      "36, BCE loss: 0.237, amex train: 0.771, val 0.000\n",
      "36, BCE loss: 0.244, amex train: 0.765, val 0.000\n",
      "36, BCE loss: 0.241, amex train: 0.773, val 0.000\n",
      "36, BCE loss: 0.255, amex train: 0.749, val 0.000\n",
      "36, BCE loss: 0.241, amex train: 0.766, val 0.000\n",
      "36, BCE loss: 0.247, amex train: 0.757, val 0.000\n",
      "36, BCE loss: 0.253, amex train: 0.757, val 0.000\n",
      "36, BCE loss: 0.245, amex train: 0.757, val 0.000\n",
      "36, BCE loss: 0.240, amex train: 0.765, val 0.000\n",
      "36, BCE loss: 0.237, amex train: 0.770, val 0.000\n",
      "36, BCE loss: 0.247, amex train: 0.763, val 0.000\n",
      "36, BCE loss: 0.239, amex train: 0.760, val 0.000\n",
      "36, BCE loss: 0.240, amex train: 0.768, val 0.000\n",
      "36, BCE loss: 0.236, amex train: 0.769, val 0.000\n",
      "36, BCE loss: 0.240, amex train: 0.766, val 0.000\n",
      "36, BCE loss: 0.240, amex train: 0.770, val 0.000\n",
      "36, BCE loss: 0.244, amex train: 0.762, val 0.000\n",
      "36, BCE loss: 0.246, amex train: 0.763, val 0.000\n",
      "37, BCE loss: 0.242, amex train: 0.762, val 0.000\n",
      "37, BCE loss: 0.245, amex train: 0.755, val 0.000\n",
      "37, BCE loss: 0.245, amex train: 0.754, val 0.000\n",
      "37, BCE loss: 0.238, amex train: 0.772, val 0.000\n",
      "37, BCE loss: 0.242, amex train: 0.768, val 0.000\n",
      "37, BCE loss: 0.236, amex train: 0.775, val 0.000\n",
      "37, BCE loss: 0.251, amex train: 0.751, val 0.000\n",
      "37, BCE loss: 0.245, amex train: 0.767, val 0.000\n",
      "37, BCE loss: 0.245, amex train: 0.758, val 0.000\n",
      "37, BCE loss: 0.245, amex train: 0.759, val 0.000\n",
      "37, BCE loss: 0.245, amex train: 0.759, val 0.000\n",
      "37, BCE loss: 0.245, amex train: 0.765, val 0.000\n",
      "37, BCE loss: 0.237, amex train: 0.773, val 0.000\n",
      "37, BCE loss: 0.247, amex train: 0.760, val 0.000\n",
      "37, BCE loss: 0.239, amex train: 0.762, val 0.000\n",
      "37, BCE loss: 0.241, amex train: 0.768, val 0.000\n",
      "37, BCE loss: 0.240, amex train: 0.770, val 0.000\n",
      "37, BCE loss: 0.240, amex train: 0.767, val 0.000\n",
      "37, BCE loss: 0.239, amex train: 0.773, val 0.000\n",
      "37, BCE loss: 0.247, amex train: 0.761, val 0.000\n",
      "37, BCE loss: 0.253, amex train: 0.763, val 0.000\n",
      "38, BCE loss: 0.246, amex train: 0.763, val 0.000\n",
      "38, BCE loss: 0.245, amex train: 0.757, val 0.000\n",
      "38, BCE loss: 0.246, amex train: 0.754, val 0.000\n",
      "38, BCE loss: 0.241, amex train: 0.771, val 0.000\n",
      "38, BCE loss: 0.245, amex train: 0.768, val 0.000\n",
      "38, BCE loss: 0.237, amex train: 0.776, val 0.000\n",
      "38, BCE loss: 0.251, amex train: 0.752, val 0.000\n",
      "38, BCE loss: 0.246, amex train: 0.767, val 0.000\n",
      "38, BCE loss: 0.249, amex train: 0.758, val 0.000\n",
      "38, BCE loss: 0.245, amex train: 0.759, val 0.000\n",
      "38, BCE loss: 0.242, amex train: 0.757, val 0.000\n",
      "38, BCE loss: 0.242, amex train: 0.764, val 0.000\n",
      "38, BCE loss: 0.240, amex train: 0.773, val 0.000\n",
      "38, BCE loss: 0.251, amex train: 0.761, val 0.000\n",
      "38, BCE loss: 0.242, amex train: 0.764, val 0.000\n",
      "38, BCE loss: 0.239, amex train: 0.767, val 0.000\n",
      "38, BCE loss: 0.242, amex train: 0.769, val 0.000\n",
      "38, BCE loss: 0.251, amex train: 0.767, val 0.000\n",
      "38, BCE loss: 0.246, amex train: 0.773, val 0.000\n",
      "38, BCE loss: 0.245, amex train: 0.761, val 0.000\n",
      "38, BCE loss: 0.265, amex train: 0.763, val 0.000\n",
      "39, BCE loss: 0.281, amex train: 0.765, val 0.000\n",
      "39, BCE loss: 0.285, amex train: 0.757, val 0.000\n",
      "39, BCE loss: 0.277, amex train: 0.750, val 0.000\n",
      "39, BCE loss: 0.294, amex train: 0.773, val 0.000\n",
      "39, BCE loss: 0.371, amex train: 0.765, val 0.000\n",
      "39, BCE loss: 0.864, amex train: 0.775, val 0.000\n",
      "39, BCE loss: 0.581, amex train: 0.746, val 0.000\n",
      "39, BCE loss: 0.530, amex train: 0.759, val 0.000\n",
      "39, BCE loss: 0.528, amex train: 0.744, val 0.000\n",
      "39, BCE loss: 0.533, amex train: 0.738, val 0.000\n",
      "39, BCE loss: 0.519, amex train: 0.730, val 0.000\n",
      "39, BCE loss: 0.483, amex train: 0.703, val 0.000\n",
      "39, BCE loss: 0.556, amex train: 0.763, val 0.000\n",
      "39, BCE loss: 0.577, amex train: 0.755, val 0.000\n",
      "39, BCE loss: 0.561, amex train: 0.747, val 0.000\n",
      "39, BCE loss: 0.546, amex train: 0.751, val 0.000\n",
      "39, BCE loss: 0.537, amex train: 0.748, val 0.000\n",
      "39, BCE loss: 0.536, amex train: 0.734, val 0.000\n",
      "39, BCE loss: 0.527, amex train: 0.739, val 0.000\n",
      "39, BCE loss: 0.526, amex train: 0.727, val 0.000\n",
      "39, BCE loss: 0.530, amex train: 0.723, val 0.000\n",
      "40, BCE loss: 0.521, amex train: 0.716, val 0.000\n",
      "40, BCE loss: 0.511, amex train: 0.717, val 0.000\n",
      "40, BCE loss: 0.500, amex train: 0.712, val 0.000\n",
      "40, BCE loss: 0.487, amex train: 0.730, val 0.000\n",
      "40, BCE loss: 0.473, amex train: 0.712, val 0.000\n",
      "40, BCE loss: 0.443, amex train: 0.726, val 0.000\n",
      "40, BCE loss: 0.414, amex train: 0.704, val 0.000\n",
      "40, BCE loss: 0.370, amex train: 0.719, val 0.000\n",
      "40, BCE loss: 0.356, amex train: 0.697, val 0.000\n",
      "40, BCE loss: 0.401, amex train: 0.693, val 0.000\n",
      "40, BCE loss: 0.444, amex train: 0.698, val 0.000\n",
      "40, BCE loss: 0.362, amex train: 0.703, val 0.000\n",
      "40, BCE loss: 0.298, amex train: 0.713, val 0.000\n",
      "40, BCE loss: 0.368, amex train: 0.720, val 0.000\n",
      "40, BCE loss: 0.344, amex train: 0.711, val 0.000\n",
      "40, BCE loss: 0.299, amex train: 0.726, val 0.000\n",
      "40, BCE loss: 0.294, amex train: 0.718, val 0.000\n",
      "40, BCE loss: 0.320, amex train: 0.708, val 0.000\n",
      "40, BCE loss: 0.318, amex train: 0.714, val 0.000\n",
      "40, BCE loss: 0.306, amex train: 0.708, val 0.000\n",
      "40, BCE loss: 0.298, amex train: 0.704, val 0.000\n",
      "41, BCE loss: 0.298, amex train: 0.712, val 0.000\n",
      "41, BCE loss: 0.310, amex train: 0.712, val 0.000\n",
      "41, BCE loss: 0.298, amex train: 0.712, val 0.000\n",
      "41, BCE loss: 0.277, amex train: 0.728, val 0.000\n",
      "41, BCE loss: 0.284, amex train: 0.718, val 0.000\n",
      "41, BCE loss: 0.281, amex train: 0.726, val 0.000\n",
      "41, BCE loss: 0.288, amex train: 0.704, val 0.000\n",
      "41, BCE loss: 0.270, amex train: 0.723, val 0.000\n",
      "41, BCE loss: 0.280, amex train: 0.708, val 0.000\n",
      "41, BCE loss: 0.284, amex train: 0.714, val 0.000\n",
      "41, BCE loss: 0.272, amex train: 0.723, val 0.000\n",
      "41, BCE loss: 0.265, amex train: 0.727, val 0.000\n",
      "41, BCE loss: 0.264, amex train: 0.725, val 0.000\n",
      "41, BCE loss: 0.273, amex train: 0.730, val 0.000\n",
      "41, BCE loss: 0.266, amex train: 0.719, val 0.000\n",
      "41, BCE loss: 0.261, amex train: 0.738, val 0.000\n",
      "41, BCE loss: 0.258, amex train: 0.734, val 0.000\n",
      "41, BCE loss: 0.263, amex train: 0.727, val 0.000\n",
      "41, BCE loss: 0.261, amex train: 0.738, val 0.000\n",
      "41, BCE loss: 0.262, amex train: 0.732, val 0.000\n",
      "41, BCE loss: 0.266, amex train: 0.734, val 0.000\n",
      "42, BCE loss: 0.260, amex train: 0.735, val 0.000\n",
      "42, BCE loss: 0.262, amex train: 0.731, val 0.000\n",
      "42, BCE loss: 0.261, amex train: 0.735, val 0.000\n",
      "42, BCE loss: 0.253, amex train: 0.750, val 0.000\n",
      "42, BCE loss: 0.256, amex train: 0.744, val 0.000\n",
      "42, BCE loss: 0.250, amex train: 0.755, val 0.000\n",
      "42, BCE loss: 0.262, amex train: 0.726, val 0.000\n",
      "42, BCE loss: 0.253, amex train: 0.745, val 0.000\n",
      "42, BCE loss: 0.257, amex train: 0.734, val 0.000\n",
      "42, BCE loss: 0.256, amex train: 0.737, val 0.000\n",
      "42, BCE loss: 0.252, amex train: 0.738, val 0.000\n",
      "42, BCE loss: 0.249, amex train: 0.748, val 0.000\n",
      "42, BCE loss: 0.245, amex train: 0.750, val 0.000\n",
      "42, BCE loss: 0.255, amex train: 0.750, val 0.000\n",
      "42, BCE loss: 0.249, amex train: 0.746, val 0.000\n",
      "42, BCE loss: 0.249, amex train: 0.753, val 0.000\n",
      "42, BCE loss: 0.246, amex train: 0.749, val 0.000\n",
      "42, BCE loss: 0.250, amex train: 0.748, val 0.000\n",
      "42, BCE loss: 0.249, amex train: 0.755, val 0.000\n",
      "42, BCE loss: 0.253, amex train: 0.746, val 0.000\n",
      "42, BCE loss: 0.255, amex train: 0.742, val 0.000\n",
      "43, BCE loss: 0.250, amex train: 0.748, val 0.000\n",
      "43, BCE loss: 0.254, amex train: 0.741, val 0.000\n",
      "43, BCE loss: 0.254, amex train: 0.744, val 0.000\n",
      "43, BCE loss: 0.245, amex train: 0.759, val 0.000\n",
      "43, BCE loss: 0.250, amex train: 0.754, val 0.000\n",
      "43, BCE loss: 0.244, amex train: 0.761, val 0.000\n",
      "43, BCE loss: 0.256, amex train: 0.739, val 0.000\n",
      "43, BCE loss: 0.247, amex train: 0.755, val 0.000\n",
      "43, BCE loss: 0.252, amex train: 0.748, val 0.000\n",
      "43, BCE loss: 0.251, amex train: 0.745, val 0.000\n",
      "43, BCE loss: 0.248, amex train: 0.748, val 0.000\n",
      "43, BCE loss: 0.244, amex train: 0.757, val 0.000\n",
      "43, BCE loss: 0.241, amex train: 0.761, val 0.000\n",
      "43, BCE loss: 0.252, amex train: 0.755, val 0.000\n",
      "43, BCE loss: 0.245, amex train: 0.753, val 0.000\n",
      "43, BCE loss: 0.245, amex train: 0.761, val 0.000\n",
      "43, BCE loss: 0.242, amex train: 0.760, val 0.000\n",
      "43, BCE loss: 0.246, amex train: 0.757, val 0.000\n",
      "43, BCE loss: 0.245, amex train: 0.763, val 0.000\n",
      "43, BCE loss: 0.250, amex train: 0.757, val 0.000\n",
      "43, BCE loss: 0.252, amex train: 0.754, val 0.000\n",
      "44, BCE loss: 0.247, amex train: 0.752, val 0.000\n",
      "44, BCE loss: 0.250, amex train: 0.746, val 0.000\n",
      "44, BCE loss: 0.251, amex train: 0.750, val 0.000\n",
      "44, BCE loss: 0.241, amex train: 0.767, val 0.000\n",
      "44, BCE loss: 0.247, amex train: 0.762, val 0.000\n",
      "44, BCE loss: 0.241, amex train: 0.768, val 0.000\n",
      "44, BCE loss: 0.254, amex train: 0.745, val 0.000\n",
      "44, BCE loss: 0.246, amex train: 0.762, val 0.000\n",
      "44, BCE loss: 0.249, amex train: 0.755, val 0.000\n",
      "44, BCE loss: 0.251, amex train: 0.751, val 0.000\n",
      "44, BCE loss: 0.248, amex train: 0.754, val 0.000\n",
      "44, BCE loss: 0.242, amex train: 0.763, val 0.000\n",
      "44, BCE loss: 0.239, amex train: 0.768, val 0.000\n",
      "44, BCE loss: 0.249, amex train: 0.760, val 0.000\n",
      "44, BCE loss: 0.243, amex train: 0.757, val 0.000\n",
      "44, BCE loss: 0.243, amex train: 0.765, val 0.000\n",
      "44, BCE loss: 0.241, amex train: 0.764, val 0.000\n",
      "44, BCE loss: 0.245, amex train: 0.762, val 0.000\n",
      "44, BCE loss: 0.243, amex train: 0.764, val 0.000\n",
      "44, BCE loss: 0.251, amex train: 0.758, val 0.000\n",
      "44, BCE loss: 0.252, amex train: 0.761, val 0.000\n",
      "45, BCE loss: 0.245, amex train: 0.760, val 0.000\n",
      "45, BCE loss: 0.251, amex train: 0.750, val 0.000\n",
      "45, BCE loss: 0.250, amex train: 0.754, val 0.000\n",
      "45, BCE loss: 0.241, amex train: 0.767, val 0.000\n",
      "45, BCE loss: 0.247, amex train: 0.762, val 0.000\n",
      "45, BCE loss: 0.242, amex train: 0.771, val 0.000\n",
      "45, BCE loss: 0.253, amex train: 0.746, val 0.000\n",
      "45, BCE loss: 0.244, amex train: 0.765, val 0.000\n",
      "45, BCE loss: 0.250, amex train: 0.756, val 0.000\n",
      "45, BCE loss: 0.249, amex train: 0.753, val 0.000\n",
      "45, BCE loss: 0.245, amex train: 0.754, val 0.000\n",
      "45, BCE loss: 0.243, amex train: 0.762, val 0.000\n",
      "45, BCE loss: 0.238, amex train: 0.767, val 0.000\n",
      "45, BCE loss: 0.250, amex train: 0.761, val 0.000\n",
      "45, BCE loss: 0.244, amex train: 0.759, val 0.000\n",
      "45, BCE loss: 0.242, amex train: 0.768, val 0.000\n",
      "45, BCE loss: 0.243, amex train: 0.767, val 0.000\n",
      "45, BCE loss: 0.247, amex train: 0.764, val 0.000\n",
      "45, BCE loss: 0.243, amex train: 0.768, val 0.000\n",
      "45, BCE loss: 0.253, amex train: 0.759, val 0.000\n",
      "45, BCE loss: 0.254, amex train: 0.762, val 0.000\n",
      "46, BCE loss: 0.245, amex train: 0.760, val 0.000\n",
      "46, BCE loss: 0.253, amex train: 0.751, val 0.000\n",
      "46, BCE loss: 0.251, amex train: 0.754, val 0.000\n",
      "46, BCE loss: 0.241, amex train: 0.770, val 0.000\n",
      "46, BCE loss: 0.248, amex train: 0.762, val 0.000\n",
      "46, BCE loss: 0.240, amex train: 0.770, val 0.000\n",
      "46, BCE loss: 0.254, amex train: 0.748, val 0.000\n",
      "46, BCE loss: 0.245, amex train: 0.767, val 0.000\n",
      "46, BCE loss: 0.248, amex train: 0.757, val 0.000\n",
      "46, BCE loss: 0.249, amex train: 0.756, val 0.000\n",
      "46, BCE loss: 0.246, amex train: 0.754, val 0.000\n",
      "46, BCE loss: 0.242, amex train: 0.762, val 0.000\n",
      "46, BCE loss: 0.239, amex train: 0.767, val 0.000\n",
      "46, BCE loss: 0.250, amex train: 0.761, val 0.000\n",
      "46, BCE loss: 0.242, amex train: 0.760, val 0.000\n",
      "46, BCE loss: 0.244, amex train: 0.769, val 0.000\n",
      "46, BCE loss: 0.242, amex train: 0.767, val 0.000\n",
      "46, BCE loss: 0.243, amex train: 0.765, val 0.000\n",
      "46, BCE loss: 0.245, amex train: 0.769, val 0.000\n",
      "46, BCE loss: 0.252, amex train: 0.760, val 0.000\n",
      "46, BCE loss: 0.250, amex train: 0.762, val 0.000\n",
      "47, BCE loss: 0.246, amex train: 0.762, val 0.000\n",
      "47, BCE loss: 0.253, amex train: 0.755, val 0.000\n",
      "47, BCE loss: 0.250, amex train: 0.753, val 0.000\n",
      "47, BCE loss: 0.242, amex train: 0.772, val 0.000\n",
      "47, BCE loss: 0.249, amex train: 0.763, val 0.000\n",
      "47, BCE loss: 0.242, amex train: 0.773, val 0.000\n",
      "47, BCE loss: 0.253, amex train: 0.750, val 0.000\n",
      "47, BCE loss: 0.245, amex train: 0.767, val 0.000\n",
      "47, BCE loss: 0.251, amex train: 0.758, val 0.000\n",
      "47, BCE loss: 0.250, amex train: 0.758, val 0.000\n",
      "47, BCE loss: 0.246, amex train: 0.756, val 0.000\n",
      "47, BCE loss: 0.244, amex train: 0.762, val 0.000\n",
      "47, BCE loss: 0.239, amex train: 0.770, val 0.000\n",
      "47, BCE loss: 0.250, amex train: 0.761, val 0.000\n",
      "47, BCE loss: 0.244, amex train: 0.762, val 0.000\n",
      "47, BCE loss: 0.243, amex train: 0.769, val 0.000\n",
      "47, BCE loss: 0.243, amex train: 0.768, val 0.000\n",
      "47, BCE loss: 0.247, amex train: 0.766, val 0.000\n",
      "47, BCE loss: 0.244, amex train: 0.769, val 0.000\n",
      "47, BCE loss: 0.252, amex train: 0.761, val 0.000\n",
      "47, BCE loss: 0.253, amex train: 0.764, val 0.000\n",
      "48, BCE loss: 0.245, amex train: 0.763, val 0.000\n",
      "48, BCE loss: 0.252, amex train: 0.755, val 0.000\n",
      "48, BCE loss: 0.252, amex train: 0.755, val 0.000\n",
      "48, BCE loss: 0.241, amex train: 0.772, val 0.000\n",
      "48, BCE loss: 0.249, amex train: 0.764, val 0.000\n",
      "48, BCE loss: 0.244, amex train: 0.774, val 0.000\n",
      "48, BCE loss: 0.255, amex train: 0.750, val 0.000\n",
      "48, BCE loss: 0.246, amex train: 0.767, val 0.000\n",
      "48, BCE loss: 0.253, amex train: 0.757, val 0.000\n",
      "48, BCE loss: 0.251, amex train: 0.757, val 0.000\n",
      "48, BCE loss: 0.246, amex train: 0.756, val 0.000\n",
      "48, BCE loss: 0.245, amex train: 0.763, val 0.000\n",
      "48, BCE loss: 0.239, amex train: 0.769, val 0.000\n",
      "48, BCE loss: 0.251, amex train: 0.762, val 0.000\n",
      "48, BCE loss: 0.246, amex train: 0.762, val 0.000\n",
      "48, BCE loss: 0.244, amex train: 0.769, val 0.000\n",
      "48, BCE loss: 0.245, amex train: 0.768, val 0.000\n",
      "48, BCE loss: 0.248, amex train: 0.766, val 0.000\n",
      "48, BCE loss: 0.244, amex train: 0.770, val 0.000\n",
      "48, BCE loss: 0.254, amex train: 0.762, val 0.000\n",
      "48, BCE loss: 0.254, amex train: 0.763, val 0.000\n",
      "49, BCE loss: 0.245, amex train: 0.763, val 0.000\n",
      "49, BCE loss: 0.254, amex train: 0.755, val 0.000\n",
      "49, BCE loss: 0.253, amex train: 0.755, val 0.000\n",
      "49, BCE loss: 0.242, amex train: 0.772, val 0.000\n",
      "49, BCE loss: 0.251, amex train: 0.765, val 0.000\n",
      "49, BCE loss: 0.245, amex train: 0.774, val 0.000\n",
      "49, BCE loss: 0.255, amex train: 0.751, val 0.000\n",
      "49, BCE loss: 0.247, amex train: 0.767, val 0.000\n",
      "49, BCE loss: 0.253, amex train: 0.758, val 0.000\n",
      "49, BCE loss: 0.251, amex train: 0.759, val 0.000\n",
      "49, BCE loss: 0.247, amex train: 0.756, val 0.000\n",
      "49, BCE loss: 0.246, amex train: 0.764, val 0.000\n",
      "49, BCE loss: 0.239, amex train: 0.771, val 0.000\n",
      "49, BCE loss: 0.251, amex train: 0.762, val 0.000\n",
      "49, BCE loss: 0.247, amex train: 0.764, val 0.000\n",
      "49, BCE loss: 0.245, amex train: 0.769, val 0.000\n",
      "49, BCE loss: 0.245, amex train: 0.769, val 0.000\n",
      "49, BCE loss: 0.250, amex train: 0.767, val 0.000\n",
      "49, BCE loss: 0.244, amex train: 0.771, val 0.000\n",
      "49, BCE loss: 0.254, amex train: 0.762, val 0.000\n",
      "49, BCE loss: 0.256, amex train: 0.762, val 0.000\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_dim=X.shape[-1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.001)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "for epoch in range(50): \n",
    "    for idx, (feat, clabel) in enumerate(train_loader):\n",
    "        if len(feat.shape) == 4:  ## Reduce shape if its coming from a ratio version of the loader\n",
    "            feat = feat.squeeze(dim=0)\n",
    "            clabel = clabel.squeeze(dim=0)\n",
    "\n",
    "        pred = model(feat)\n",
    "        #weight = clabel.clone()\n",
    "        #weight[weight==0] = 4\n",
    "        #criterion.weight = weight\n",
    "        loss = criterion(pred, clabel)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model_metric = amex_metric(clabel.detach().numpy(), pred.detach().numpy())\n",
    "        val_metrix = 0\n",
    "        if model_metric > 0.78:\n",
    "            X_test, y_test = validation_data\n",
    "            val_features = torch.as_tensor(X_test, dtype=torch.float32)\n",
    "            val_pred = model(val_features)\n",
    "            val_metrix = amex_metric(y_test, val_pred.detach().numpy())\n",
    "\n",
    "        log_message = f\"{epoch}, BCE loss: {loss.item():.3f}, amex train: {model_metric:.3f}, val {val_metrix:.3f}\"\n",
    "        print(log_message)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7553599270057281"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred = model(torch.as_tensor(X_test, dtype=torch.float32))\n",
    "amex_metric(y_test, val_pred.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50991, 188)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6728076658891633"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amex_metric(train_labels, data[scores[scores.score>0.5].feature.values].mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D_44</td>\n",
       "      <td>0.520132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>P_2</td>\n",
       "      <td>0.617505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>B_9</td>\n",
       "      <td>0.519722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>R_1</td>\n",
       "      <td>0.553033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>D_48</td>\n",
       "      <td>0.529506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>B_2</td>\n",
       "      <td>0.513858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature     score\n",
       "4     D_44  0.520132\n",
       "13     P_2  0.617505\n",
       "15     B_9  0.519722\n",
       "16     R_1  0.553033\n",
       "17    D_48  0.529506\n",
       "23     B_2  0.513858"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[scores.score>0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv(data_dir+\"scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores.plot(kind=\"hist\", bins=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>feature</th>\n",
       "      <th>B_30</th>\n",
       "      <th>B_38</th>\n",
       "      <th>D_114</th>\n",
       "      <th>D_116</th>\n",
       "      <th>D_117</th>\n",
       "      <th>D_120</th>\n",
       "      <th>D_126</th>\n",
       "      <th>D_63</th>\n",
       "      <th>D_64</th>\n",
       "      <th>D_66</th>\n",
       "      <th>D_68</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0.408168</td>\n",
       "      <td>0.449818</td>\n",
       "      <td>0.190889</td>\n",
       "      <td>0.14299</td>\n",
       "      <td>0.202967</td>\n",
       "      <td>0.201567</td>\n",
       "      <td>0.120872</td>\n",
       "      <td>0.142744</td>\n",
       "      <td>0.213286</td>\n",
       "      <td>0.124187</td>\n",
       "      <td>0.207338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "feature      B_30      B_38     D_114    D_116     D_117     D_120     D_126  \\\n",
       "score    0.408168  0.449818  0.190889  0.14299  0.202967  0.201567  0.120872   \n",
       "\n",
       "feature      D_63      D_64      D_66      D_68  \n",
       "score    0.142744  0.213286  0.124187  0.207338  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.set_index(\"feature\").transpose()[CATCOLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_2 0.617505189043585\n",
      "R_1 0.5530326553723249\n",
      "D_48 0.5295061268917556\n",
      "D_44 0.5201321765392717\n",
      "B_9 0.5197216963533271\n",
      "B_2 0.5138580948399069\n",
      "D_61 0.493417849604965\n",
      "D_75 0.4631432057534954\n",
      "B_3 0.4615958401817553\n",
      "D_55 0.4589400214092629\n",
      "D_62 0.4533323838935789\n",
      "B_38 0.4498176124096328\n",
      "B_1 0.4463323025389073\n",
      "B_37 0.445037677043968\n",
      "B_7 0.4434378870880191\n",
      "B_18 0.4407384956867923\n",
      "B_23 0.4363929920755105\n",
      "B_11 0.4341448583985742\n",
      "D_74 0.4325018285776832\n",
      "D_58 0.4304836540962537\n",
      "S_25 0.4255730569270594\n",
      "B_6 0.4213558927414298\n",
      "B_10 0.4188997147425396\n",
      "B_33 0.4182099767898408\n",
      "B_20 0.4168810375724595\n",
      "P_3 0.4165962092687051\n",
      "D_39 0.415916956470467\n",
      "R_10 0.4148703511886054\n",
      "S_23 0.4146236816855438\n",
      "D_78 0.4131480533886151\n",
      "B_4 0.4092640384109716\n",
      "B_30 0.4081677688374197\n",
      "B_40 0.4072006048814324\n",
      "D_77 0.40653624139649\n",
      "B_16 0.399401775629653\n",
      "B_17 0.3951588442258508\n",
      "B_19 0.3905738126093643\n",
      "S_3 0.3850795254092689\n",
      "B_25 0.3802836019205967\n",
      "B_22 0.3801117232894496\n",
      "B_14 0.3762387395123195\n",
      "S_7 0.3762301135620587\n",
      "D_70 0.3692834471482957\n",
      "R_2 0.3690515175000657\n",
      "S_15 0.366798987249765\n",
      "S_22 0.3569737646163402\n",
      "R_6 0.3567731855939453\n",
      "D_53 0.352810194754978\n",
      "S_8 0.3426940406043222\n",
      "D_41 0.3410641194054011\n",
      "D_46 0.3364502069838688\n",
      "D_60 0.3352317542194658\n",
      "R_3 0.3326320206227838\n",
      "D_42 0.3293621765272628\n",
      "D_84 0.329347610979432\n",
      "D_52 0.3268333489172933\n",
      "B_26 0.3265188030274291\n",
      "R_5 0.3174967984247219\n",
      "R_4 0.3155963632199274\n",
      "D_65 0.314576089185871\n",
      "R_7 0.3139032416284534\n",
      "D_43 0.3120502617189211\n",
      "S_24 0.3110998315635767\n",
      "B_5 0.30846300617832\n",
      "R_8 0.305194582086835\n",
      "D_112 0.3003645130233525\n",
      "D_45 0.290040778556897\n",
      "S_27 0.2863647827672043\n",
      "B_28 0.2779599320508644\n",
      "R_16 0.2689744966942358\n",
      "D_47 0.2688157586090072\n",
      "R_27 0.2606194583967751\n",
      "S_11 0.2552427193414721\n",
      "B_8 0.2531739987378786\n",
      "D_59 0.252315024755234\n",
      "S_13 0.2513606605134857\n",
      "P_4 0.2463522446736059\n",
      "D_71 0.2420658729177883\n",
      "D_72 0.236577178957017\n",
      "D_79 0.2287142892169286\n",
      "B_13 0.2284692343991085\n",
      "R_15 0.2282579059474488\n",
      "D_51 0.2222054324524335\n",
      "D_122 0.2208048722533377\n",
      "D_81 0.2188335795913072\n",
      "D_64 0.2132859937027443\n",
      "D_113 0.2094207997913347\n",
      "D_68 0.2073380852409138\n",
      "R_26 0.2047073778416406\n",
      "D_117 0.2029669034906203\n",
      "R_11 0.2023843641470076\n",
      "D_120 0.2015665804346814\n",
      "D_89 0.2006862688233057\n",
      "S_6 0.1983779946498777\n",
      "D_91 0.1975939426446149\n",
      "D_131 0.1925785342035321\n",
      "D_124 0.1911342852925223\n",
      "D_114 0.1908892012898274\n",
      "B_12 0.1898229977350205\n",
      "D_56 0.1887273825450909\n",
      "S_5 0.1883568963446335\n",
      "D_106 0.1874019165418768\n",
      "D_115 0.1870527521616509\n",
      "R_13 0.1861262596843403\n",
      "D_133 0.1839243938272756\n",
      "S_12 0.1832258946784463\n",
      "D_118 0.1803048408981195\n",
      "D_132 0.178299329777448\n",
      "D_49 0.1774298226883528\n",
      "D_123 0.1761172916496731\n",
      "D_119 0.1748761405602876\n",
      "D_129 0.1740399002191279\n",
      "D_121 0.1740000896211678\n",
      "D_125 0.1739436448716357\n",
      "D_107 0.1732663218792177\n",
      "R_14 0.1722783187241418\n",
      "R_24 0.17180056941704\n",
      "R_20 0.1716421024195491\n",
      "D_130 0.1707222275785455\n",
      "D_103 0.1701665074000366\n",
      "D_128 0.1701240403973125\n",
      "D_102 0.1695596698189401\n",
      "D_83 0.169533086064614\n",
      "D_50 0.164425779937893\n",
      "D_54 0.1614684840834127\n",
      "R_9 0.1600206119537382\n",
      "B_21 0.1539417871147181\n",
      "D_143 0.1528041420975319\n",
      "D_139 0.1526611507967955\n",
      "S_16 0.1502658308389035\n",
      "D_138 0.1483070215174005\n",
      "D_135 0.1482705465299741\n",
      "D_136 0.1481795256785778\n",
      "D_137 0.1481764497800693\n",
      "D_134 0.1472137393072667\n",
      "R_17 0.1431356560000099\n",
      "D_116 0.1429902319973397\n",
      "D_63 0.1427437854543444\n",
      "B_24 0.1376562871904068\n",
      "D_140 0.1295626102417247\n",
      "D_104 0.1282827083782595\n",
      "R_21 0.1264996483340864\n",
      "D_66 0.124186985017119\n",
      "B_32 0.1228403365026492\n",
      "D_127 0.1213036308125529\n",
      "D_126 0.1208722348528104\n",
      "D_69 0.1203450833685991\n",
      "D_105 0.1186910732085691\n",
      "S_26 0.1111434761889546\n",
      "R_12 0.1108997241721382\n",
      "S_9 0.108766992730709\n",
      "D_141 0.1054847249192843\n",
      "D_142 0.1043613809808131\n",
      "S_20 0.1037950316724386\n",
      "D_92 0.0971824609870159\n",
      "D_80 0.0967760693923054\n",
      "S_17 0.086003348170171\n",
      "B_41 0.0837180311466172\n",
      "B_15 0.0802389123784225\n",
      "S_19 0.0791029904431854\n",
      "B_31 0.0776935417888588\n",
      "D_76 0.0765056981960208\n",
      "R_25 0.0762637074283252\n",
      "R_22 0.0730432221675825\n",
      "R_19 0.0701870432216816\n",
      "D_82 0.0662818807906534\n",
      "D_144 0.0620208736772541\n",
      "D_108 0.0613353482088164\n",
      "B_27 0.0606552678165706\n",
      "B_36 0.0590531209831747\n",
      "D_96 0.0477580342884766\n",
      "D_94 0.0459864800092461\n",
      "D_86 0.0443413744580818\n",
      "D_111 0.0376757548089337\n",
      "D_88 0.0352925335787213\n",
      "B_39 0.0337100462314294\n",
      "D_110 0.033679270880165\n",
      "D_73 0.0321824043520712\n",
      "D_93 0.0321594801520348\n",
      "D_87 0.0312296679778336\n",
      "B_29 0.0283144795856136\n",
      "B_42 0.0273639543402974\n",
      "R_28 0.0272254470972648\n",
      "D_109 0.027156237650625\n",
      "S_18 0.0257093954920599\n",
      "R_23 0.024985980943686\n",
      "R_18 0.0239667356728942\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(scores.shape[0]):\n",
    "    print(scores.iloc[i].feature, scores.iloc[i].score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['P_2', 'R_1', 'D_48', 'D_44', 'B_9', 'B_2', 'D_61', 'D_75', 'B_3',\n",
       "       'D_55', 'D_62', 'B_38', 'B_1', 'B_37', 'B_7', 'B_18', 'B_23',\n",
       "       'B_11', 'D_74', 'D_58', 'S_25', 'B_6', 'B_10', 'B_33', 'B_20',\n",
       "       'P_3', 'D_39', 'R_10', 'S_23', 'D_78', 'B_4', 'B_30', 'B_40',\n",
       "       'D_77', 'B_16', 'B_17', 'B_19', 'S_3', 'B_25', 'B_22', 'B_14',\n",
       "       'S_7', 'D_70', 'R_2', 'S_15', 'S_22', 'R_6', 'D_53'], dtype=object)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[scores.score>0.35].feature.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('p8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25d4fc9e51127fc9597103904eccbdaaf4ac71170a764a4234e11ed19cb4831a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
