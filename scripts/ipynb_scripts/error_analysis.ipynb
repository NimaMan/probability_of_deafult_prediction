{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalvuate the eror \n",
    "The objective of the eval pred is to first figure out some info about the evaluation metric. The evaluation metric Consists of two parts. Ftist is a rank based one. We rank the customers that have defulted based on the predicted probability. choose the highest .04(#1 + #0*20). \n",
    "\n",
    "This is equvalent to the recall level of 4% the doc states. That is, of all the customers that are going to deafult, we aim to recall at least 96% of them!! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q:\n",
    " - What does a recall of 4% mean? How do we train for a recall of a 4%? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd\n",
    "from pd.params import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pd.nn.model import Conv\n",
    "from pd.data.loader import CustomerData, DTwithLabelRatio\n",
    "from pd.nn.train_utils import train_torch_model\n",
    "from pd.metric import amex_metric\n",
    "from pd.params import *\n",
    "from pd.pred import pred_test_npy as predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true, y_pred, return_components=False):\n",
    "    \"\"\"Amex metric for ndarrays\"\"\"\n",
    "    def top_four_percent_captured(df) -> float:\n",
    "        \"\"\"Corresponds to the recall for a threshold of 4 %\"\"\"\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(df) -> float:\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(df) -> float:\n",
    "        \"\"\"Corresponds to 2 * AUC - 1\"\"\"\n",
    "        df2 = pd.DataFrame({'target': df.target, 'prediction': df.target})\n",
    "        df2.sort_values('prediction', ascending=False, inplace=True)\n",
    "        return weighted_gini(df) / weighted_gini(df2)\n",
    "\n",
    "    df = pd.DataFrame({'target': y_true.ravel(), 'prediction': y_pred.ravel()})\n",
    "    df.sort_values('prediction', ascending=False, inplace=True)\n",
    "    g = normalized_weighted_gini(df)\n",
    "    d = top_four_percent_captured(df)\n",
    "\n",
    "    if return_components: return g, d, 0.5 * (g + d)\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_four_percent_captured(df) -> float:\n",
    "        \"\"\"Corresponds to the recall for a threshold of 4 %\"\"\"\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from pd.nn.model import Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from bes.nn.es_module import ESModule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(OUTDIR+\"train13_raw_all_data.npy\")\n",
    "train_labels = np.load(OUTDIR+\"train13_raw_all_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, dataloader\n",
    "\n",
    "test_data = np.load(OUTDIR+\"train_raw_all_data.npy\", mmap_mode=\"r+\")\n",
    "train_labels = np.load(OUTDIR+\"train_raw_all_labels.npy\")\n",
    "class Data(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):        \n",
    "        return torch.from_numpy(self.data[index]), index\n",
    "\n",
    "train_dataset = Data(test_data, )\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"conv13_32_all\"\n",
    "model = Conv(conv_channels=32)\n",
    "model_param = torch.load(OUTDIR+model_name)\n",
    "model.load_state_dict(model_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=1/9, random_state=1, shuffle=True)\n",
    "validation_data = (X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = validation_data\n",
    "val_features = torch.from_numpy(X_test)\n",
    "val_pred = model(val_features)\n",
    "val_metrix, val_gini, val_recall = amex_metric(y_test, val_pred.detach().numpy(), return_components=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8106466362523513, 0.9281171813848559, 0.6931760911198468)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrix, val_gini, val_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8076595984542532, 0.9279778102472538, 0.6873413866612527)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrix, val_gini, val_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.load(OUTDIR+\"c13_preds.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = amex_metric(train_labels, preds, return_components=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9189522846823319, 0.6626991745056633, 0.7908257295939976)\n",
      "(0.9117926612179975, 0.6322059953881629, 0.7719993283030802)\n",
      "(0.9137001253827733, 0.630639278172394, 0.7721697017775837)\n",
      "(0.910439040794739, 0.6391986391986392, 0.7748188399966891)\n",
      "(0.9143557163604561, 0.6452611218568666, 0.7798084191086614)\n",
      "(0.9117057372875503, 0.6510107015457788, 0.7813582194166646)\n",
      "(0.907421621330389, 0.6261520737327189, 0.7667868475315539)\n",
      "(0.9117512332341383, 0.6370797310278579, 0.7744154821309981)\n",
      "(0.9146816478243461, 0.6332438650306749, 0.7739627564275104)\n",
      "(0.911634259072624, 0.6298299845440495, 0.7707321218083367)\n",
      "(0.9122821673600039, 0.6259376233714963, 0.76910989536575)\n",
      "(0.9091516211022168, 0.630766223612197, 0.769958922357207)\n",
      "(0.9177579459881187, 0.6546998648909477, 0.7862289054395332)\n",
      "(0.9128007189670488, 0.6530612244897959, 0.7829309717284223)\n",
      "(0.9095910951030811, 0.6390001908032819, 0.7742956429531815)\n",
      "(0.9122655531659488, 0.6271450858034321, 0.7697053194846905)\n",
      "(0.9123261570727688, 0.6450300329393528, 0.7786780950060608)\n",
      "(0.9112252009857315, 0.6411483253588517, 0.7761867631722916)\n",
      "(0.9128061165021195, 0.647995396125072, 0.7804007563135957)\n",
      "(0.9111432155093143, 0.6408395449195763, 0.7759913802144454)\n",
      "(0.9123719250710762, 0.6445470282746683, 0.7784594766728723)\n",
      "(0.9146417224381, 0.6593047820241982, 0.7869732522311491)\n",
      "(0.9133694376600467, 0.639054470709147, 0.7762119541845969)\n"
     ]
    }
   ],
   "source": [
    "preds = np.zeros(len(train_dataset))\n",
    "model.eval()\n",
    "for idx, (feat, indices) in enumerate(train_loader):\n",
    "        batch_pred = model(feat)\n",
    "        preds[indices.numpy()] = batch_pred.detach().numpy().reshape(-1, )\n",
    "        #preds.append(batch_pred)\n",
    "        print(amex_metric(train_labels[indices.numpy()], preds[indices.numpy()], return_components=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.47234747244543596, 0.09336767546683837, 0.28285757395613714)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amex_metric(train_labels[indices.numpy()], preds[indices.numpy()], return_components=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=1/9, random_state=0, shuffle=True)\n",
    "validation_data = (X_test, y_test)\n",
    "\n",
    "train_dataset = CustomerData(X_train, train_labels=y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, BCE loss: 0.746, amex train: 0.033, val 0.000\n",
      "0, BCE loss: 0.880, amex train: 0.146, val 0.000\n",
      "0, BCE loss: 0.725, amex train: 0.220, val 0.000\n",
      "0, BCE loss: 0.568, amex train: 0.288, val 0.000\n",
      "0, BCE loss: 0.526, amex train: 0.314, val 0.000\n",
      "0, BCE loss: 0.582, amex train: 0.345, val 0.000\n",
      "0, BCE loss: 0.607, amex train: 0.355, val 0.000\n",
      "0, BCE loss: 0.571, amex train: 0.358, val 0.000\n",
      "0, BCE loss: 0.529, amex train: 0.384, val 0.000\n",
      "0, BCE loss: 0.511, amex train: 0.386, val 0.000\n",
      "0, BCE loss: 0.519, amex train: 0.391, val 0.000\n",
      "0, BCE loss: 0.531, amex train: 0.378, val 0.000\n",
      "0, BCE loss: 0.546, amex train: 0.387, val 0.000\n",
      "0, BCE loss: 0.537, amex train: 0.388, val 0.000\n",
      "0, BCE loss: 0.542, amex train: 0.388, val 0.000\n",
      "0, BCE loss: 0.529, amex train: 0.396, val 0.000\n",
      "0, BCE loss: 0.520, amex train: 0.399, val 0.000\n",
      "0, BCE loss: 0.494, amex train: 0.387, val 0.000\n",
      "1, BCE loss: 0.507, amex train: 0.408, val 0.000\n",
      "1, BCE loss: 0.503, amex train: 0.387, val 0.000\n",
      "1, BCE loss: 0.511, amex train: 0.393, val 0.000\n",
      "1, BCE loss: 0.513, amex train: 0.409, val 0.000\n",
      "1, BCE loss: 0.522, amex train: 0.393, val 0.000\n",
      "1, BCE loss: 0.514, amex train: 0.400, val 0.000\n",
      "1, BCE loss: 0.512, amex train: 0.403, val 0.000\n",
      "1, BCE loss: 0.507, amex train: 0.400, val 0.000\n",
      "1, BCE loss: 0.500, amex train: 0.421, val 0.000\n",
      "1, BCE loss: 0.496, amex train: 0.414, val 0.000\n",
      "1, BCE loss: 0.499, amex train: 0.420, val 0.000\n",
      "1, BCE loss: 0.500, amex train: 0.403, val 0.000\n",
      "1, BCE loss: 0.504, amex train: 0.407, val 0.000\n",
      "1, BCE loss: 0.495, amex train: 0.404, val 0.000\n",
      "1, BCE loss: 0.499, amex train: 0.403, val 0.000\n",
      "1, BCE loss: 0.492, amex train: 0.404, val 0.000\n",
      "1, BCE loss: 0.489, amex train: 0.406, val 0.000\n",
      "1, BCE loss: 0.472, amex train: 0.400, val 0.000\n",
      "2, BCE loss: 0.482, amex train: 0.413, val 0.000\n",
      "2, BCE loss: 0.479, amex train: 0.393, val 0.000\n",
      "2, BCE loss: 0.482, amex train: 0.401, val 0.000\n",
      "2, BCE loss: 0.477, amex train: 0.416, val 0.000\n",
      "2, BCE loss: 0.485, amex train: 0.396, val 0.000\n",
      "2, BCE loss: 0.474, amex train: 0.399, val 0.000\n",
      "2, BCE loss: 0.472, amex train: 0.396, val 0.000\n",
      "2, BCE loss: 0.466, amex train: 0.398, val 0.000\n",
      "2, BCE loss: 0.457, amex train: 0.419, val 0.000\n",
      "2, BCE loss: 0.451, amex train: 0.412, val 0.000\n",
      "2, BCE loss: 0.450, amex train: 0.424, val 0.000\n",
      "2, BCE loss: 0.450, amex train: 0.406, val 0.000\n",
      "2, BCE loss: 0.448, amex train: 0.416, val 0.000\n",
      "2, BCE loss: 0.439, amex train: 0.415, val 0.000\n",
      "2, BCE loss: 0.439, amex train: 0.411, val 0.000\n",
      "2, BCE loss: 0.433, amex train: 0.420, val 0.000\n",
      "2, BCE loss: 0.426, amex train: 0.435, val 0.000\n",
      "2, BCE loss: 0.409, amex train: 0.434, val 0.000\n",
      "3, BCE loss: 0.411, amex train: 0.463, val 0.000\n",
      "3, BCE loss: 0.409, amex train: 0.456, val 0.000\n",
      "3, BCE loss: 0.404, amex train: 0.474, val 0.000\n",
      "3, BCE loss: 0.390, amex train: 0.504, val 0.000\n",
      "3, BCE loss: 0.398, amex train: 0.498, val 0.000\n",
      "3, BCE loss: 0.383, amex train: 0.519, val 0.000\n",
      "3, BCE loss: 0.373, amex train: 0.541, val 0.000\n",
      "3, BCE loss: 0.354, amex train: 0.555, val 0.000\n",
      "3, BCE loss: 0.338, amex train: 0.581, val 0.000\n",
      "3, BCE loss: 0.335, amex train: 0.582, val 0.000\n",
      "3, BCE loss: 0.335, amex train: 0.592, val 0.000\n",
      "3, BCE loss: 0.319, amex train: 0.619, val 0.000\n",
      "3, BCE loss: 0.318, amex train: 0.622, val 0.000\n",
      "3, BCE loss: 0.310, amex train: 0.624, val 0.000\n",
      "3, BCE loss: 0.311, amex train: 0.633, val 0.000\n",
      "3, BCE loss: 0.303, amex train: 0.644, val 0.000\n",
      "3, BCE loss: 0.302, amex train: 0.647, val 0.000\n",
      "3, BCE loss: 0.313, amex train: 0.632, val 0.000\n",
      "4, BCE loss: 0.294, amex train: 0.663, val 0.000\n",
      "4, BCE loss: 0.290, amex train: 0.668, val 0.000\n",
      "4, BCE loss: 0.286, amex train: 0.676, val 0.000\n",
      "4, BCE loss: 0.284, amex train: 0.674, val 0.000\n",
      "4, BCE loss: 0.288, amex train: 0.685, val 0.000\n",
      "4, BCE loss: 0.292, amex train: 0.670, val 0.000\n",
      "4, BCE loss: 0.281, amex train: 0.684, val 0.000\n",
      "4, BCE loss: 0.277, amex train: 0.691, val 0.000\n",
      "4, BCE loss: 0.267, amex train: 0.707, val 0.000\n",
      "4, BCE loss: 0.274, amex train: 0.694, val 0.000\n",
      "4, BCE loss: 0.278, amex train: 0.679, val 0.000\n",
      "4, BCE loss: 0.265, amex train: 0.708, val 0.000\n",
      "4, BCE loss: 0.268, amex train: 0.709, val 0.000\n",
      "4, BCE loss: 0.268, amex train: 0.697, val 0.000\n",
      "4, BCE loss: 0.272, amex train: 0.702, val 0.000\n",
      "4, BCE loss: 0.263, amex train: 0.715, val 0.000\n",
      "4, BCE loss: 0.264, amex train: 0.713, val 0.000\n",
      "4, BCE loss: 0.267, amex train: 0.702, val 0.000\n",
      "5, BCE loss: 0.262, amex train: 0.713, val 0.000\n",
      "5, BCE loss: 0.268, amex train: 0.716, val 0.000\n",
      "5, BCE loss: 0.259, amex train: 0.719, val 0.000\n",
      "5, BCE loss: 0.263, amex train: 0.716, val 0.000\n",
      "5, BCE loss: 0.264, amex train: 0.719, val 0.000\n",
      "5, BCE loss: 0.269, amex train: 0.711, val 0.000\n",
      "5, BCE loss: 0.260, amex train: 0.719, val 0.000\n",
      "5, BCE loss: 0.254, amex train: 0.737, val 0.000\n",
      "5, BCE loss: 0.247, amex train: 0.740, val 0.000\n",
      "5, BCE loss: 0.255, amex train: 0.726, val 0.000\n",
      "5, BCE loss: 0.260, amex train: 0.712, val 0.000\n",
      "5, BCE loss: 0.248, amex train: 0.740, val 0.000\n",
      "5, BCE loss: 0.249, amex train: 0.740, val 0.000\n",
      "5, BCE loss: 0.255, amex train: 0.726, val 0.000\n",
      "5, BCE loss: 0.253, amex train: 0.732, val 0.000\n",
      "5, BCE loss: 0.247, amex train: 0.743, val 0.000\n",
      "5, BCE loss: 0.250, amex train: 0.736, val 0.000\n",
      "5, BCE loss: 0.256, amex train: 0.723, val 0.000\n",
      "6, BCE loss: 0.248, amex train: 0.735, val 0.000\n",
      "6, BCE loss: 0.244, amex train: 0.739, val 0.000\n",
      "6, BCE loss: 0.249, amex train: 0.742, val 0.000\n",
      "6, BCE loss: 0.246, amex train: 0.738, val 0.000\n",
      "6, BCE loss: 0.255, amex train: 0.741, val 0.000\n",
      "6, BCE loss: 0.252, amex train: 0.734, val 0.000\n",
      "6, BCE loss: 0.245, amex train: 0.740, val 0.000\n",
      "6, BCE loss: 0.240, amex train: 0.755, val 0.000\n",
      "6, BCE loss: 0.235, amex train: 0.763, val 0.000\n",
      "6, BCE loss: 0.241, amex train: 0.747, val 0.000\n",
      "6, BCE loss: 0.252, amex train: 0.732, val 0.000\n",
      "6, BCE loss: 0.235, amex train: 0.757, val 0.000\n",
      "6, BCE loss: 0.239, amex train: 0.754, val 0.000\n",
      "6, BCE loss: 0.242, amex train: 0.746, val 0.000\n",
      "6, BCE loss: 0.244, amex train: 0.751, val 0.000\n",
      "6, BCE loss: 0.239, amex train: 0.759, val 0.000\n",
      "6, BCE loss: 0.241, amex train: 0.754, val 0.000\n",
      "6, BCE loss: 0.243, amex train: 0.746, val 0.000\n",
      "7, BCE loss: 0.234, amex train: 0.752, val 0.000\n",
      "7, BCE loss: 0.237, amex train: 0.753, val 0.000\n",
      "7, BCE loss: 0.246, amex train: 0.754, val 0.000\n",
      "7, BCE loss: 0.243, amex train: 0.750, val 0.000\n",
      "7, BCE loss: 0.248, amex train: 0.756, val 0.000\n",
      "7, BCE loss: 0.241, amex train: 0.749, val 0.000\n",
      "7, BCE loss: 0.236, amex train: 0.754, val 0.000\n",
      "7, BCE loss: 0.231, amex train: 0.766, val 0.000\n",
      "7, BCE loss: 0.227, amex train: 0.774, val 0.000\n",
      "7, BCE loss: 0.232, amex train: 0.761, val 0.000\n",
      "7, BCE loss: 0.245, amex train: 0.741, val 0.000\n",
      "7, BCE loss: 0.229, amex train: 0.768, val 0.000\n",
      "7, BCE loss: 0.230, amex train: 0.764, val 0.000\n",
      "7, BCE loss: 0.236, amex train: 0.758, val 0.000\n",
      "7, BCE loss: 0.234, amex train: 0.766, val 0.000\n",
      "7, BCE loss: 0.230, amex train: 0.770, val 0.000\n",
      "7, BCE loss: 0.240, amex train: 0.762, val 0.000\n",
      "7, BCE loss: 0.243, amex train: 0.761, val 0.000\n",
      "8, BCE loss: 0.245, amex train: 0.764, val 0.000\n",
      "8, BCE loss: 0.237, amex train: 0.759, val 0.000\n",
      "8, BCE loss: 0.233, amex train: 0.762, val 0.000\n",
      "8, BCE loss: 0.234, amex train: 0.767, val 0.000\n",
      "8, BCE loss: 0.237, amex train: 0.766, val 0.000\n",
      "8, BCE loss: 0.239, amex train: 0.758, val 0.000\n",
      "8, BCE loss: 0.238, amex train: 0.758, val 0.000\n",
      "8, BCE loss: 0.226, amex train: 0.771, val 0.000\n",
      "8, BCE loss: 0.228, amex train: 0.778, val 0.000\n",
      "8, BCE loss: 0.227, amex train: 0.765, val 0.000\n",
      "8, BCE loss: 0.243, amex train: 0.751, val 0.000\n",
      "8, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "8, BCE loss: 0.225, amex train: 0.770, val 0.000\n",
      "8, BCE loss: 0.234, amex train: 0.766, val 0.000\n",
      "8, BCE loss: 0.229, amex train: 0.772, val 0.000\n",
      "8, BCE loss: 0.228, amex train: 0.775, val 0.000\n",
      "8, BCE loss: 0.234, amex train: 0.767, val 0.000\n",
      "8, BCE loss: 0.230, amex train: 0.767, val 0.000\n",
      "9, BCE loss: 0.224, amex train: 0.772, val 0.000\n",
      "9, BCE loss: 0.225, amex train: 0.766, val 0.000\n",
      "9, BCE loss: 0.230, amex train: 0.767, val 0.000\n",
      "9, BCE loss: 0.228, amex train: 0.770, val 0.000\n",
      "9, BCE loss: 0.234, amex train: 0.770, val 0.000\n",
      "9, BCE loss: 0.232, amex train: 0.768, val 0.000\n",
      "9, BCE loss: 0.229, amex train: 0.769, val 0.000\n",
      "9, BCE loss: 0.221, amex train: 0.779, val 0.000\n",
      "9, BCE loss: 0.218, amex train: 0.786, val 0.769\n",
      "9, BCE loss: 0.224, amex train: 0.774, val 0.000\n",
      "9, BCE loss: 0.235, amex train: 0.757, val 0.000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "model_name = \"conv_all_13\"\n",
    "\n",
    "model = Conv(input_dim=X_train.shape[-1], conv_channels=25)\n",
    "model = train_torch_model(model, train_loader, num_epochs=50, validation_data=validation_data, \n",
    "                        output_model_name=model_name)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(343141, 13, 188)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('q')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05e50049e3eb32775174019135b7208a0d3852fb22829b3658213f387a3fdcbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
