{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "\n",
    "import random\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "from pd.params import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true, y_pred, return_components=False):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    if return_components:\n",
    "        return 0.5 * (gini[1]/gini[0] + top_four), gini[1]/gini[0], top_four\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    score, gini, recall = amex_metric(y_true, y_pred, return_components=True)\n",
    "    return f'amex_metric gini {gini:.3f} recall {recall:.3f}', score, True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': 42,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 4,\n",
    "        'min_data_in_leaf': 40\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(OUTDIR+\"train_data_all.npy\").transpose((0, 2, 1))\n",
    "train_labels = np.load(OUTDIR+\"train_labels_all.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(OUTDIR+\"train13_raw_all_data.npy\")\n",
    "train_labels = np.load(OUTDIR+\"train13_raw_all_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineer...\n"
     ]
    }
   ],
   "source": [
    "from pd.data.preprop import preprocess_data\n",
    "preprocess_data(data_type=\"train\", time_dim=None, all_data=True, fillna=\"mean\", borders=(\"q1\", \"q99\"))\n",
    "train_data = np.load(OUTDIR+\"train_raw_all_mean_q1_q99_data.npy\")\n",
    "train_labels = np.load(OUTDIR+\"train_raw_all_mean_q1_q99_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineer...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (1,1,188)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000008?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpd\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprop\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocess_data\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000008?line=1'>2</a>\u001b[0m preprocess_data(data_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, time_dim\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, all_data\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, fillna\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m, borders\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mq2\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mq98\u001b[39;49m\u001b[39m\"\u001b[39;49m), normalize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000008?line=2'>3</a>\u001b[0m train_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(OUTDIR\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_nonorm_raw_all_mean_q1_q99_data.npy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000008?line=3'>4</a>\u001b[0m train_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(OUTDIR\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_nonorm_raw_all_mean_q1_q99_labels.npy\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/kaggle/pd/pd/pd/data/preprop.py:166\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[0;34m(data_type, feat_type, time_dim, all_data, fillna, borders, normalize)\u001b[0m\n\u001b[1;32m    164\u001b[0m     data_time_dim \u001b[39m=\u001b[39m \u001b[39m13\u001b[39m\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m data_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 166\u001b[0m     data, labels_array, id_dict \u001b[39m=\u001b[39m get_raw_features_fill(customers, data, \n\u001b[1;32m    167\u001b[0m                                     train_labels\u001b[39m=\u001b[39;49mtrain_labels\u001b[39m.\u001b[39;49mset_index(\u001b[39m\"\u001b[39;49m\u001b[39mcustomer_ID\u001b[39;49m\u001b[39m\"\u001b[39;49m), time_dim\u001b[39m=\u001b[39;49mdata_time_dim, \n\u001b[1;32m    168\u001b[0m                                     fillna\u001b[39m=\u001b[39;49mfillna, borders\u001b[39m=\u001b[39;49mborders, normalize\u001b[39m=\u001b[39;49mnormalize)\n\u001b[1;32m    169\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     data, labels_array, id_dict \u001b[39m=\u001b[39m get_raw_features_fill(customers, data, test_mode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, time_dim\u001b[39m=\u001b[39mdata_time_dim, \n\u001b[1;32m    171\u001b[0m                                                         fillna\u001b[39m=\u001b[39mfillna, borders\u001b[39m=\u001b[39mborders, normalize\u001b[39m=\u001b[39mnormalize,)\n",
      "File \u001b[0;32m~/Desktop/kaggle/pd/pd/pd/data/preprop.py:126\u001b[0m, in \u001b[0;36mget_raw_features_fill\u001b[0;34m(customer_ids, train_data, train_labels, test_mode, normalize, time_dim, fillna, borders)\u001b[0m\n\u001b[1;32m    124\u001b[0m labels_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(customer_ids)) ,\u001b[39m1\u001b[39m))\n\u001b[1;32m    125\u001b[0m id_dict \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 126\u001b[0m d \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(customer_ids)), time_dim, \u001b[39mlen\u001b[39m(cols)), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39;49marray(fill_feats, dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat32)\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39mlen\u001b[39;49m(cols))\n\u001b[1;32m    127\u001b[0m \u001b[39mfor\u001b[39;00m idx, c \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mset\u001b[39m(customer_ids)):\n\u001b[1;32m    128\u001b[0m     cd \u001b[39m=\u001b[39m customer_data\u001b[39m.\u001b[39mget_group(c)[cols]\u001b[39m.\u001b[39mvalues\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (1,1,188)"
     ]
    }
   ],
   "source": [
    "from pd.data.preprop import preprocess_data\n",
    "preprocess_data(data_type=\"train\", time_dim=None, all_data=True, fillna=\"mean\", borders=(\"q2\", \"q98\"), normalize=False)\n",
    "train_data = np.load(OUTDIR+\"train_nonorm_raw_all_mean_q1_q99_data.npy\")\n",
    "train_labels = np.load(OUTDIR+\"train_nonorm_raw_all_mean_q1_q99_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=1/9, random_state=0, shuffle=True)\n",
    "validation_data = (X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 105717, number of negative: 302205\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.336949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 326786\n",
      "[LightGBM] [Info] Number of data points in the train set: 407922, number of used features: 2443\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259160 -> initscore=-1.050340\n",
      "[LightGBM] [Info] Start training from score -1.050340\n",
      "[100]\ttraining's binary_logloss: 0.470308\ttraining's amex_metric gini 0.904 recall 0.606: 0.754964\tvalid_1's binary_logloss: 0.470932\tvalid_1's amex_metric gini 0.897 recall 0.592: 0.744795\n",
      "[200]\ttraining's binary_logloss: 0.450319\ttraining's amex_metric gini 0.906 recall 0.613: 0.75945\tvalid_1's binary_logloss: 0.45141\tvalid_1's amex_metric gini 0.899 recall 0.597: 0.748081\n",
      "[300]\ttraining's binary_logloss: 0.39716\ttraining's amex_metric gini 0.908 recall 0.620: 0.763753\tvalid_1's binary_logloss: 0.398993\tvalid_1's amex_metric gini 0.901 recall 0.600: 0.750466\n",
      "[400]\ttraining's binary_logloss: 0.372314\ttraining's amex_metric gini 0.909 recall 0.623: 0.765709\tvalid_1's binary_logloss: 0.374582\tvalid_1's amex_metric gini 0.902 recall 0.603: 0.752389\n",
      "[500]\ttraining's binary_logloss: 0.341696\ttraining's amex_metric gini 0.910 recall 0.629: 0.769511\tvalid_1's binary_logloss: 0.344645\tvalid_1's amex_metric gini 0.903 recall 0.605: 0.753942\n",
      "[600]\ttraining's binary_logloss: 0.323547\ttraining's amex_metric gini 0.912 recall 0.632: 0.771898\tvalid_1's binary_logloss: 0.326983\tvalid_1's amex_metric gini 0.904 recall 0.606: 0.755059\n",
      "[700]\ttraining's binary_logloss: 0.293894\ttraining's amex_metric gini 0.914 recall 0.639: 0.776322\tvalid_1's binary_logloss: 0.298324\tvalid_1's amex_metric gini 0.905 recall 0.610: 0.757654\n",
      "[800]\ttraining's binary_logloss: 0.27326\ttraining's amex_metric gini 0.916 recall 0.645: 0.780443\tvalid_1's binary_logloss: 0.278678\tvalid_1's amex_metric gini 0.907 recall 0.613: 0.760008\n",
      "[900]\ttraining's binary_logloss: 0.258049\ttraining's amex_metric gini 0.918 recall 0.652: 0.784786\tvalid_1's binary_logloss: 0.264548\tvalid_1's amex_metric gini 0.908 recall 0.614: 0.761276\n",
      "[1000]\ttraining's binary_logloss: 0.251477\ttraining's amex_metric gini 0.919 recall 0.657: 0.788046\tvalid_1's binary_logloss: 0.25874\tvalid_1's amex_metric gini 0.909 recall 0.619: 0.763981\n",
      "[1100]\ttraining's binary_logloss: 0.241455\ttraining's amex_metric gini 0.921 recall 0.663: 0.791885\tvalid_1's binary_logloss: 0.249969\tvalid_1's amex_metric gini 0.910 recall 0.622: 0.766141\n",
      "[1200]\ttraining's binary_logloss: 0.238175\ttraining's amex_metric gini 0.922 recall 0.667: 0.794558\tvalid_1's binary_logloss: 0.247306\tvalid_1's amex_metric gini 0.911 recall 0.623: 0.767155\n",
      "[1300]\ttraining's binary_logloss: 0.234127\ttraining's amex_metric gini 0.923 recall 0.671: 0.797183\tvalid_1's binary_logloss: 0.243971\tvalid_1's amex_metric gini 0.912 recall 0.629: 0.770214\n",
      "[1400]\ttraining's binary_logloss: 0.229503\ttraining's amex_metric gini 0.925 recall 0.675: 0.799945\tvalid_1's binary_logloss: 0.240342\tvalid_1's amex_metric gini 0.912 recall 0.628: 0.770424\n",
      "[1500]\ttraining's binary_logloss: 0.227198\ttraining's amex_metric gini 0.926 recall 0.678: 0.802016\tvalid_1's binary_logloss: 0.238657\tvalid_1's amex_metric gini 0.913 recall 0.630: 0.771658\n",
      "[1600]\ttraining's binary_logloss: 0.221971\ttraining's amex_metric gini 0.927 recall 0.683: 0.805283\tvalid_1's binary_logloss: 0.234841\tvalid_1's amex_metric gini 0.914 recall 0.630: 0.771973\n",
      "[1700]\ttraining's binary_logloss: 0.219449\ttraining's amex_metric gini 0.928 recall 0.687: 0.807768\tvalid_1's binary_logloss: 0.23319\tvalid_1's amex_metric gini 0.914 recall 0.632: 0.77312\n",
      "[1800]\ttraining's binary_logloss: 0.218133\ttraining's amex_metric gini 0.929 recall 0.690: 0.80986\tvalid_1's binary_logloss: 0.232532\tvalid_1's amex_metric gini 0.915 recall 0.632: 0.773338\n",
      "[1900]\ttraining's binary_logloss: 0.215929\ttraining's amex_metric gini 0.931 recall 0.694: 0.81234\tvalid_1's binary_logloss: 0.231233\tvalid_1's amex_metric gini 0.915 recall 0.633: 0.774182\n",
      "[2000]\ttraining's binary_logloss: 0.213484\ttraining's amex_metric gini 0.932 recall 0.698: 0.814985\tvalid_1's binary_logloss: 0.229903\tvalid_1's amex_metric gini 0.916 recall 0.635: 0.775305\n",
      "[2100]\ttraining's binary_logloss: 0.212\ttraining's amex_metric gini 0.933 recall 0.701: 0.817014\tvalid_1's binary_logloss: 0.229144\tvalid_1's amex_metric gini 0.916 recall 0.636: 0.776008\n",
      "[2200]\ttraining's binary_logloss: 0.21082\ttraining's amex_metric gini 0.933 recall 0.704: 0.818773\tvalid_1's binary_logloss: 0.228658\tvalid_1's amex_metric gini 0.916 recall 0.637: 0.776527\n",
      "[2300]\ttraining's binary_logloss: 0.208984\ttraining's amex_metric gini 0.934 recall 0.708: 0.821093\tvalid_1's binary_logloss: 0.227812\tvalid_1's amex_metric gini 0.917 recall 0.637: 0.777125\n",
      "[2400]\ttraining's binary_logloss: 0.207865\ttraining's amex_metric gini 0.935 recall 0.711: 0.823216\tvalid_1's binary_logloss: 0.227372\tvalid_1's amex_metric gini 0.917 recall 0.639: 0.777996\n",
      "[2500]\ttraining's binary_logloss: 0.206671\ttraining's amex_metric gini 0.936 recall 0.714: 0.825013\tvalid_1's binary_logloss: 0.226941\tvalid_1's amex_metric gini 0.917 recall 0.640: 0.77857\n",
      "[2600]\ttraining's binary_logloss: 0.205332\ttraining's amex_metric gini 0.937 recall 0.717: 0.826774\tvalid_1's binary_logloss: 0.226408\tvalid_1's amex_metric gini 0.918 recall 0.639: 0.778505\n",
      "[2700]\ttraining's binary_logloss: 0.203806\ttraining's amex_metric gini 0.938 recall 0.720: 0.828608\tvalid_1's binary_logloss: 0.225811\tvalid_1's amex_metric gini 0.918 recall 0.641: 0.779153\n",
      "[2800]\ttraining's binary_logloss: 0.20301\ttraining's amex_metric gini 0.938 recall 0.722: 0.830233\tvalid_1's binary_logloss: 0.225621\tvalid_1's amex_metric gini 0.918 recall 0.641: 0.779322\n",
      "[2900]\ttraining's binary_logloss: 0.201477\ttraining's amex_metric gini 0.939 recall 0.725: 0.832155\tvalid_1's binary_logloss: 0.225071\tvalid_1's amex_metric gini 0.918 recall 0.641: 0.779731\n",
      "[3000]\ttraining's binary_logloss: 0.199972\ttraining's amex_metric gini 0.940 recall 0.728: 0.834082\tvalid_1's binary_logloss: 0.224568\tvalid_1's amex_metric gini 0.918 recall 0.643: 0.780744\n",
      "[3100]\ttraining's binary_logloss: 0.198808\ttraining's amex_metric gini 0.941 recall 0.731: 0.835958\tvalid_1's binary_logloss: 0.224243\tvalid_1's amex_metric gini 0.919 recall 0.644: 0.781144\n",
      "[3200]\ttraining's binary_logloss: 0.197594\ttraining's amex_metric gini 0.942 recall 0.734: 0.837645\tvalid_1's binary_logloss: 0.223916\tvalid_1's amex_metric gini 0.919 recall 0.646: 0.782254\n",
      "[3300]\ttraining's binary_logloss: 0.196486\ttraining's amex_metric gini 0.942 recall 0.736: 0.839175\tvalid_1's binary_logloss: 0.223689\tvalid_1's amex_metric gini 0.919 recall 0.646: 0.782505\n",
      "[3400]\ttraining's binary_logloss: 0.194999\ttraining's amex_metric gini 0.943 recall 0.739: 0.84114\tvalid_1's binary_logloss: 0.223281\tvalid_1's amex_metric gini 0.919 recall 0.648: 0.78328\n",
      "[3500]\ttraining's binary_logloss: 0.193909\ttraining's amex_metric gini 0.944 recall 0.742: 0.843151\tvalid_1's binary_logloss: 0.223034\tvalid_1's amex_metric gini 0.919 recall 0.649: 0.783858\n",
      "[3600]\ttraining's binary_logloss: 0.19284\ttraining's amex_metric gini 0.945 recall 0.745: 0.844876\tvalid_1's binary_logloss: 0.222796\tvalid_1's amex_metric gini 0.919 recall 0.648: 0.783822\n",
      "[3700]\ttraining's binary_logloss: 0.191328\ttraining's amex_metric gini 0.945 recall 0.748: 0.846694\tvalid_1's binary_logloss: 0.22246\tvalid_1's amex_metric gini 0.919 recall 0.649: 0.784032\n",
      "[3800]\ttraining's binary_logloss: 0.190356\ttraining's amex_metric gini 0.946 recall 0.751: 0.848654\tvalid_1's binary_logloss: 0.22226\tvalid_1's amex_metric gini 0.920 recall 0.649: 0.784111\n",
      "[3900]\ttraining's binary_logloss: 0.1895\ttraining's amex_metric gini 0.947 recall 0.753: 0.850117\tvalid_1's binary_logloss: 0.222135\tvalid_1's amex_metric gini 0.920 recall 0.649: 0.784505\n",
      "[4000]\ttraining's binary_logloss: 0.188653\ttraining's amex_metric gini 0.948 recall 0.756: 0.851859\tvalid_1's binary_logloss: 0.222042\tvalid_1's amex_metric gini 0.920 recall 0.650: 0.784844\n",
      "[4100]\ttraining's binary_logloss: 0.187631\ttraining's amex_metric gini 0.948 recall 0.758: 0.853244\tvalid_1's binary_logloss: 0.221863\tvalid_1's amex_metric gini 0.920 recall 0.649: 0.7844\n",
      "[4200]\ttraining's binary_logloss: 0.186373\ttraining's amex_metric gini 0.949 recall 0.761: 0.855154\tvalid_1's binary_logloss: 0.221674\tvalid_1's amex_metric gini 0.920 recall 0.649: 0.78444\n",
      "[4300]\ttraining's binary_logloss: 0.1851\ttraining's amex_metric gini 0.950 recall 0.764: 0.856942\tvalid_1's binary_logloss: 0.221439\tvalid_1's amex_metric gini 0.920 recall 0.649: 0.784764\n",
      "[4400]\ttraining's binary_logloss: 0.184501\ttraining's amex_metric gini 0.950 recall 0.767: 0.858579\tvalid_1's binary_logloss: 0.22138\tvalid_1's amex_metric gini 0.920 recall 0.650: 0.785305\n",
      "[4500]\ttraining's binary_logloss: 0.183508\ttraining's amex_metric gini 0.951 recall 0.770: 0.860495\tvalid_1's binary_logloss: 0.22121\tvalid_1's amex_metric gini 0.920 recall 0.650: 0.785131\n",
      "[4600]\ttraining's binary_logloss: 0.182121\ttraining's amex_metric gini 0.952 recall 0.773: 0.862121\tvalid_1's binary_logloss: 0.221\tvalid_1's amex_metric gini 0.920 recall 0.650: 0.78506\n",
      "[4700]\ttraining's binary_logloss: 0.18133\ttraining's amex_metric gini 0.952 recall 0.775: 0.863898\tvalid_1's binary_logloss: 0.220907\tvalid_1's amex_metric gini 0.920 recall 0.651: 0.785634\n",
      "[4800]\ttraining's binary_logloss: 0.180365\ttraining's amex_metric gini 0.953 recall 0.778: 0.865619\tvalid_1's binary_logloss: 0.220823\tvalid_1's amex_metric gini 0.920 recall 0.651: 0.785504\n",
      "[4900]\ttraining's binary_logloss: 0.179251\ttraining's amex_metric gini 0.954 recall 0.781: 0.867312\tvalid_1's binary_logloss: 0.220678\tvalid_1's amex_metric gini 0.921 recall 0.651: 0.785659\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "lgb_valid = lgb.Dataset(X_test.reshape(X_test.shape[0], -1), y_test,)\n",
    "model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 5000,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 105621, number of negative: 302301\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.637010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 328380\n",
      "[LightGBM] [Info] Number of data points in the train set: 407922, number of used features: 2430\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258925 -> initscore=-1.051566\n",
      "[LightGBM] [Info] Start training from score -1.051566\n",
      "[100]\ttraining's binary_logloss: 0.470496\ttraining's amex_metric gini 0.905 recall 0.610: 0.757488\tvalid_1's binary_logloss: 0.472599\tvalid_1's amex_metric gini 0.898 recall 0.589: 0.743513\n",
      "[200]\ttraining's binary_logloss: 0.450399\ttraining's amex_metric gini 0.907 recall 0.614: 0.760213\tvalid_1's binary_logloss: 0.45306\tvalid_1's amex_metric gini 0.899 recall 0.595: 0.746975\n",
      "[300]\ttraining's binary_logloss: 0.396964\ttraining's amex_metric gini 0.908 recall 0.620: 0.764161\tvalid_1's binary_logloss: 0.400621\tvalid_1's amex_metric gini 0.901 recall 0.601: 0.750812\n",
      "[400]\ttraining's binary_logloss: 0.371919\ttraining's amex_metric gini 0.910 recall 0.624: 0.766749\tvalid_1's binary_logloss: 0.376146\tvalid_1's amex_metric gini 0.902 recall 0.604: 0.753068\n",
      "[500]\ttraining's binary_logloss: 0.340911\ttraining's amex_metric gini 0.911 recall 0.629: 0.769953\tvalid_1's binary_logloss: 0.345989\tvalid_1's amex_metric gini 0.903 recall 0.605: 0.75399\n",
      "[600]\ttraining's binary_logloss: 0.322511\ttraining's amex_metric gini 0.912 recall 0.632: 0.772435\tvalid_1's binary_logloss: 0.328189\tvalid_1's amex_metric gini 0.904 recall 0.610: 0.757096\n",
      "[700]\ttraining's binary_logloss: 0.292944\ttraining's amex_metric gini 0.914 recall 0.639: 0.776373\tvalid_1's binary_logloss: 0.299683\tvalid_1's amex_metric gini 0.905 recall 0.618: 0.761642\n",
      "[800]\ttraining's binary_logloss: 0.272571\ttraining's amex_metric gini 0.916 recall 0.644: 0.779935\tvalid_1's binary_logloss: 0.280326\tvalid_1's amex_metric gini 0.907 recall 0.621: 0.763977\n",
      "[900]\ttraining's binary_logloss: 0.257373\ttraining's amex_metric gini 0.918 recall 0.651: 0.784324\tvalid_1's binary_logloss: 0.266147\tvalid_1's amex_metric gini 0.908 recall 0.625: 0.766861\n",
      "[1000]\ttraining's binary_logloss: 0.250828\ttraining's amex_metric gini 0.919 recall 0.656: 0.787719\tvalid_1's binary_logloss: 0.260206\tvalid_1's amex_metric gini 0.909 recall 0.628: 0.768853\n",
      "[1100]\ttraining's binary_logloss: 0.24088\ttraining's amex_metric gini 0.921 recall 0.662: 0.791533\tvalid_1's binary_logloss: 0.251318\tvalid_1's amex_metric gini 0.910 recall 0.631: 0.770813\n",
      "[1200]\ttraining's binary_logloss: 0.23757\ttraining's amex_metric gini 0.922 recall 0.666: 0.793955\tvalid_1's binary_logloss: 0.248524\tvalid_1's amex_metric gini 0.911 recall 0.634: 0.772795\n",
      "[1300]\ttraining's binary_logloss: 0.233589\ttraining's amex_metric gini 0.924 recall 0.669: 0.796419\tvalid_1's binary_logloss: 0.245176\tvalid_1's amex_metric gini 0.912 recall 0.638: 0.774818\n",
      "[1400]\ttraining's binary_logloss: 0.228967\ttraining's amex_metric gini 0.925 recall 0.673: 0.798747\tvalid_1's binary_logloss: 0.2414\tvalid_1's amex_metric gini 0.913 recall 0.637: 0.774932\n",
      "[1500]\ttraining's binary_logloss: 0.226796\ttraining's amex_metric gini 0.926 recall 0.677: 0.801247\tvalid_1's binary_logloss: 0.239737\tvalid_1's amex_metric gini 0.913 recall 0.640: 0.776747\n",
      "[1600]\ttraining's binary_logloss: 0.221653\ttraining's amex_metric gini 0.927 recall 0.681: 0.804318\tvalid_1's binary_logloss: 0.235797\tvalid_1's amex_metric gini 0.914 recall 0.643: 0.778781\n",
      "[1700]\ttraining's binary_logloss: 0.219245\ttraining's amex_metric gini 0.928 recall 0.686: 0.807061\tvalid_1's binary_logloss: 0.23413\tvalid_1's amex_metric gini 0.915 recall 0.645: 0.77999\n",
      "[1800]\ttraining's binary_logloss: 0.217918\ttraining's amex_metric gini 0.929 recall 0.689: 0.809401\tvalid_1's binary_logloss: 0.233335\tvalid_1's amex_metric gini 0.915 recall 0.646: 0.780549\n",
      "[1900]\ttraining's binary_logloss: 0.215717\ttraining's amex_metric gini 0.931 recall 0.693: 0.811906\tvalid_1's binary_logloss: 0.231936\tvalid_1's amex_metric gini 0.916 recall 0.646: 0.781024\n",
      "[2000]\ttraining's binary_logloss: 0.213297\ttraining's amex_metric gini 0.932 recall 0.697: 0.814089\tvalid_1's binary_logloss: 0.230442\tvalid_1's amex_metric gini 0.916 recall 0.648: 0.782228\n",
      "[2100]\ttraining's binary_logloss: 0.211833\ttraining's amex_metric gini 0.933 recall 0.700: 0.816236\tvalid_1's binary_logloss: 0.229669\tvalid_1's amex_metric gini 0.917 recall 0.648: 0.782503\n",
      "[2200]\ttraining's binary_logloss: 0.210635\ttraining's amex_metric gini 0.933 recall 0.703: 0.818004\tvalid_1's binary_logloss: 0.229067\tvalid_1's amex_metric gini 0.917 recall 0.649: 0.782871\n",
      "[2300]\ttraining's binary_logloss: 0.208816\ttraining's amex_metric gini 0.934 recall 0.706: 0.82003\tvalid_1's binary_logloss: 0.228112\tvalid_1's amex_metric gini 0.917 recall 0.650: 0.783616\n",
      "[2400]\ttraining's binary_logloss: 0.207694\ttraining's amex_metric gini 0.935 recall 0.708: 0.821789\tvalid_1's binary_logloss: 0.227586\tvalid_1's amex_metric gini 0.918 recall 0.651: 0.784592\n",
      "[2500]\ttraining's binary_logloss: 0.206483\ttraining's amex_metric gini 0.936 recall 0.712: 0.82401\tvalid_1's binary_logloss: 0.227088\tvalid_1's amex_metric gini 0.918 recall 0.651: 0.784575\n",
      "[2600]\ttraining's binary_logloss: 0.205159\ttraining's amex_metric gini 0.937 recall 0.715: 0.825755\tvalid_1's binary_logloss: 0.226494\tvalid_1's amex_metric gini 0.918 recall 0.651: 0.784828\n",
      "[2700]\ttraining's binary_logloss: 0.203619\ttraining's amex_metric gini 0.938 recall 0.718: 0.827853\tvalid_1's binary_logloss: 0.225777\tvalid_1's amex_metric gini 0.919 recall 0.652: 0.785437\n",
      "[2800]\ttraining's binary_logloss: 0.202854\ttraining's amex_metric gini 0.938 recall 0.721: 0.8295\tvalid_1's binary_logloss: 0.225566\tvalid_1's amex_metric gini 0.919 recall 0.653: 0.785694\n",
      "[2900]\ttraining's binary_logloss: 0.201321\ttraining's amex_metric gini 0.939 recall 0.724: 0.831464\tvalid_1's binary_logloss: 0.224979\tvalid_1's amex_metric gini 0.919 recall 0.655: 0.786934\n",
      "[3000]\ttraining's binary_logloss: 0.19981\ttraining's amex_metric gini 0.940 recall 0.727: 0.833271\tvalid_1's binary_logloss: 0.224396\tvalid_1's amex_metric gini 0.919 recall 0.655: 0.787322\n",
      "[3100]\ttraining's binary_logloss: 0.198671\ttraining's amex_metric gini 0.941 recall 0.730: 0.835219\tvalid_1's binary_logloss: 0.224044\tvalid_1's amex_metric gini 0.920 recall 0.655: 0.787462\n",
      "[3200]\ttraining's binary_logloss: 0.19748\ttraining's amex_metric gini 0.942 recall 0.732: 0.836932\tvalid_1's binary_logloss: 0.223673\tvalid_1's amex_metric gini 0.920 recall 0.655: 0.787223\n",
      "[3300]\ttraining's binary_logloss: 0.196354\ttraining's amex_metric gini 0.942 recall 0.735: 0.838886\tvalid_1's binary_logloss: 0.223338\tvalid_1's amex_metric gini 0.920 recall 0.654: 0.787127\n",
      "[3400]\ttraining's binary_logloss: 0.194854\ttraining's amex_metric gini 0.943 recall 0.739: 0.841142\tvalid_1's binary_logloss: 0.222895\tvalid_1's amex_metric gini 0.920 recall 0.655: 0.787708\n",
      "[3500]\ttraining's binary_logloss: 0.193783\ttraining's amex_metric gini 0.944 recall 0.741: 0.842626\tvalid_1's binary_logloss: 0.222635\tvalid_1's amex_metric gini 0.920 recall 0.655: 0.787818\n",
      "[3600]\ttraining's binary_logloss: 0.19273\ttraining's amex_metric gini 0.945 recall 0.744: 0.844495\tvalid_1's binary_logloss: 0.222411\tvalid_1's amex_metric gini 0.920 recall 0.655: 0.787692\n",
      "[3700]\ttraining's binary_logloss: 0.191213\ttraining's amex_metric gini 0.945 recall 0.748: 0.846697\tvalid_1's binary_logloss: 0.221999\tvalid_1's amex_metric gini 0.921 recall 0.657: 0.788795\n",
      "[3800]\ttraining's binary_logloss: 0.190242\ttraining's amex_metric gini 0.946 recall 0.750: 0.848288\tvalid_1's binary_logloss: 0.221787\tvalid_1's amex_metric gini 0.921 recall 0.655: 0.787616\n",
      "[3900]\ttraining's binary_logloss: 0.189361\ttraining's amex_metric gini 0.947 recall 0.754: 0.850272\tvalid_1's binary_logloss: 0.221641\tvalid_1's amex_metric gini 0.921 recall 0.655: 0.788056\n",
      "[4000]\ttraining's binary_logloss: 0.188498\ttraining's amex_metric gini 0.947 recall 0.756: 0.851871\tvalid_1's binary_logloss: 0.221475\tvalid_1's amex_metric gini 0.921 recall 0.657: 0.788873\n",
      "[4100]\ttraining's binary_logloss: 0.187494\ttraining's amex_metric gini 0.948 recall 0.759: 0.853559\tvalid_1's binary_logloss: 0.221309\tvalid_1's amex_metric gini 0.921 recall 0.655: 0.788165\n",
      "[4200]\ttraining's binary_logloss: 0.18623\ttraining's amex_metric gini 0.949 recall 0.762: 0.855471\tvalid_1's binary_logloss: 0.221039\tvalid_1's amex_metric gini 0.921 recall 0.657: 0.788834\n",
      "[4300]\ttraining's binary_logloss: 0.184962\ttraining's amex_metric gini 0.950 recall 0.765: 0.857337\tvalid_1's binary_logloss: 0.220762\tvalid_1's amex_metric gini 0.921 recall 0.658: 0.789579\n",
      "[4400]\ttraining's binary_logloss: 0.184353\ttraining's amex_metric gini 0.950 recall 0.768: 0.858903\tvalid_1's binary_logloss: 0.22071\tvalid_1's amex_metric gini 0.921 recall 0.657: 0.789054\n",
      "[4500]\ttraining's binary_logloss: 0.183354\ttraining's amex_metric gini 0.951 recall 0.770: 0.86043\tvalid_1's binary_logloss: 0.220616\tvalid_1's amex_metric gini 0.921 recall 0.656: 0.788886\n",
      "[4600]\ttraining's binary_logloss: 0.18197\ttraining's amex_metric gini 0.952 recall 0.773: 0.862263\tvalid_1's binary_logloss: 0.220389\tvalid_1's amex_metric gini 0.921 recall 0.657: 0.789299\n",
      "[4700]\ttraining's binary_logloss: 0.181181\ttraining's amex_metric gini 0.952 recall 0.775: 0.863786\tvalid_1's binary_logloss: 0.22028\tvalid_1's amex_metric gini 0.922 recall 0.658: 0.789533\n",
      "[4800]\ttraining's binary_logloss: 0.180211\ttraining's amex_metric gini 0.953 recall 0.778: 0.865347\tvalid_1's binary_logloss: 0.220189\tvalid_1's amex_metric gini 0.922 recall 0.658: 0.789629\n",
      "[4900]\ttraining's binary_logloss: 0.179092\ttraining's amex_metric gini 0.954 recall 0.781: 0.86734\tvalid_1's binary_logloss: 0.220072\tvalid_1's amex_metric gini 0.922 recall 0.657: 0.78946\n",
      "[5000]\ttraining's binary_logloss: 0.178281\ttraining's amex_metric gini 0.954 recall 0.783: 0.8688\tvalid_1's binary_logloss: 0.220005\tvalid_1's amex_metric gini 0.922 recall 0.657: 0.789221\n",
      "[5100]\ttraining's binary_logloss: 0.177491\ttraining's amex_metric gini 0.955 recall 0.786: 0.870501\tvalid_1's binary_logloss: 0.219941\tvalid_1's amex_metric gini 0.922 recall 0.658: 0.789969\n",
      "[5200]\ttraining's binary_logloss: 0.176397\ttraining's amex_metric gini 0.956 recall 0.788: 0.871768\tvalid_1's binary_logloss: 0.219828\tvalid_1's amex_metric gini 0.922 recall 0.658: 0.789871\n",
      "[5300]\ttraining's binary_logloss: 0.175817\ttraining's amex_metric gini 0.956 recall 0.791: 0.873355\tvalid_1's binary_logloss: 0.219806\tvalid_1's amex_metric gini 0.922 recall 0.659: 0.790651\n",
      "[5400]\ttraining's binary_logloss: 0.174856\ttraining's amex_metric gini 0.957 recall 0.793: 0.874802\tvalid_1's binary_logloss: 0.219668\tvalid_1's amex_metric gini 0.922 recall 0.658: 0.789964\n",
      "[5500]\ttraining's binary_logloss: 0.173793\ttraining's amex_metric gini 0.957 recall 0.796: 0.876456\tvalid_1's binary_logloss: 0.219559\tvalid_1's amex_metric gini 0.922 recall 0.659: 0.790476\n",
      "[5600]\ttraining's binary_logloss: 0.173004\ttraining's amex_metric gini 0.958 recall 0.798: 0.878204\tvalid_1's binary_logloss: 0.219499\tvalid_1's amex_metric gini 0.922 recall 0.658: 0.790198\n",
      "[5700]\ttraining's binary_logloss: 0.172114\ttraining's amex_metric gini 0.959 recall 0.801: 0.879878\tvalid_1's binary_logloss: 0.219384\tvalid_1's amex_metric gini 0.922 recall 0.661: 0.791291\n",
      "[5800]\ttraining's binary_logloss: 0.171451\ttraining's amex_metric gini 0.959 recall 0.803: 0.881289\tvalid_1's binary_logloss: 0.219358\tvalid_1's amex_metric gini 0.922 recall 0.660: 0.791117\n",
      "[5900]\ttraining's binary_logloss: 0.170498\ttraining's amex_metric gini 0.960 recall 0.806: 0.882844\tvalid_1's binary_logloss: 0.219294\tvalid_1's amex_metric gini 0.922 recall 0.661: 0.791316\n",
      "[6000]\ttraining's binary_logloss: 0.169941\ttraining's amex_metric gini 0.960 recall 0.808: 0.884175\tvalid_1's binary_logloss: 0.219288\tvalid_1's amex_metric gini 0.922 recall 0.661: 0.791783\n",
      "[6100]\ttraining's binary_logloss: 0.169185\ttraining's amex_metric gini 0.961 recall 0.810: 0.885201\tvalid_1's binary_logloss: 0.219265\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792579\n",
      "[6200]\ttraining's binary_logloss: 0.168429\ttraining's amex_metric gini 0.961 recall 0.812: 0.886575\tvalid_1's binary_logloss: 0.219222\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792513\n",
      "[6300]\ttraining's binary_logloss: 0.167551\ttraining's amex_metric gini 0.962 recall 0.814: 0.887935\tvalid_1's binary_logloss: 0.219183\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792739\n",
      "[6400]\ttraining's binary_logloss: 0.166823\ttraining's amex_metric gini 0.962 recall 0.817: 0.889435\tvalid_1's binary_logloss: 0.219156\tvalid_1's amex_metric gini 0.922 recall 0.664: 0.792937\n",
      "[6500]\ttraining's binary_logloss: 0.165872\ttraining's amex_metric gini 0.963 recall 0.819: 0.890929\tvalid_1's binary_logloss: 0.219108\tvalid_1's amex_metric gini 0.922 recall 0.664: 0.792828\n",
      "[6600]\ttraining's binary_logloss: 0.164847\ttraining's amex_metric gini 0.963 recall 0.822: 0.892528\tvalid_1's binary_logloss: 0.219014\tvalid_1's amex_metric gini 0.922 recall 0.664: 0.793004\n",
      "[6700]\ttraining's binary_logloss: 0.163897\ttraining's amex_metric gini 0.964 recall 0.824: 0.894007\tvalid_1's binary_logloss: 0.218938\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792756\n",
      "[6800]\ttraining's binary_logloss: 0.162885\ttraining's amex_metric gini 0.964 recall 0.827: 0.895465\tvalid_1's binary_logloss: 0.218875\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792653\n",
      "[6900]\ttraining's binary_logloss: 0.162051\ttraining's amex_metric gini 0.965 recall 0.829: 0.897003\tvalid_1's binary_logloss: 0.218849\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792694\n",
      "[7000]\ttraining's binary_logloss: 0.161178\ttraining's amex_metric gini 0.965 recall 0.831: 0.898436\tvalid_1's binary_logloss: 0.218853\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792575\n",
      "[7100]\ttraining's binary_logloss: 0.160164\ttraining's amex_metric gini 0.966 recall 0.834: 0.90008\tvalid_1's binary_logloss: 0.21879\tvalid_1's amex_metric gini 0.922 recall 0.664: 0.793233\n",
      "[7200]\ttraining's binary_logloss: 0.159256\ttraining's amex_metric gini 0.967 recall 0.836: 0.901531\tvalid_1's binary_logloss: 0.218745\tvalid_1's amex_metric gini 0.922 recall 0.664: 0.793058\n",
      "[7300]\ttraining's binary_logloss: 0.158425\ttraining's amex_metric gini 0.967 recall 0.839: 0.90312\tvalid_1's binary_logloss: 0.21872\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792798\n",
      "[7400]\ttraining's binary_logloss: 0.157586\ttraining's amex_metric gini 0.968 recall 0.841: 0.904557\tvalid_1's binary_logloss: 0.218667\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792775\n",
      "[7500]\ttraining's binary_logloss: 0.156702\ttraining's amex_metric gini 0.968 recall 0.843: 0.905851\tvalid_1's binary_logloss: 0.218657\tvalid_1's amex_metric gini 0.922 recall 0.664: 0.793155\n",
      "[7600]\ttraining's binary_logloss: 0.155937\ttraining's amex_metric gini 0.969 recall 0.846: 0.907309\tvalid_1's binary_logloss: 0.218603\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792792\n",
      "[7700]\ttraining's binary_logloss: 0.15529\ttraining's amex_metric gini 0.969 recall 0.848: 0.908627\tvalid_1's binary_logloss: 0.218573\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792504\n",
      "[7800]\ttraining's binary_logloss: 0.154537\ttraining's amex_metric gini 0.970 recall 0.850: 0.910102\tvalid_1's binary_logloss: 0.218574\tvalid_1's amex_metric gini 0.922 recall 0.662: 0.79231\n",
      "[7900]\ttraining's binary_logloss: 0.153707\ttraining's amex_metric gini 0.970 recall 0.853: 0.911518\tvalid_1's binary_logloss: 0.218501\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792598\n",
      "[8000]\ttraining's binary_logloss: 0.152714\ttraining's amex_metric gini 0.971 recall 0.855: 0.913048\tvalid_1's binary_logloss: 0.218456\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792794\n",
      "[8100]\ttraining's binary_logloss: 0.152031\ttraining's amex_metric gini 0.971 recall 0.857: 0.914189\tvalid_1's binary_logloss: 0.218429\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792958\n",
      "[8200]\ttraining's binary_logloss: 0.151147\ttraining's amex_metric gini 0.972 recall 0.859: 0.915264\tvalid_1's binary_logloss: 0.218435\tvalid_1's amex_metric gini 0.922 recall 0.664: 0.793062\n",
      "[8300]\ttraining's binary_logloss: 0.150523\ttraining's amex_metric gini 0.972 recall 0.861: 0.916678\tvalid_1's binary_logloss: 0.218394\tvalid_1's amex_metric gini 0.922 recall 0.663: 0.792698\n",
      "[8400]\ttraining's binary_logloss: 0.149928\ttraining's amex_metric gini 0.973 recall 0.863: 0.917735\tvalid_1's binary_logloss: 0.218368\tvalid_1's amex_metric gini 0.923 recall 0.663: 0.792633\n",
      "[8500]\ttraining's binary_logloss: 0.149295\ttraining's amex_metric gini 0.973 recall 0.865: 0.918858\tvalid_1's binary_logloss: 0.218367\tvalid_1's amex_metric gini 0.923 recall 0.663: 0.792519\n",
      "[8600]\ttraining's binary_logloss: 0.148561\ttraining's amex_metric gini 0.973 recall 0.867: 0.920001\tvalid_1's binary_logloss: 0.218345\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793131\n",
      "[8700]\ttraining's binary_logloss: 0.147549\ttraining's amex_metric gini 0.974 recall 0.869: 0.921303\tvalid_1's binary_logloss: 0.218298\tvalid_1's amex_metric gini 0.923 recall 0.663: 0.792801\n",
      "[8800]\ttraining's binary_logloss: 0.146877\ttraining's amex_metric gini 0.974 recall 0.871: 0.922467\tvalid_1's binary_logloss: 0.218274\tvalid_1's amex_metric gini 0.923 recall 0.665: 0.793752\n",
      "[8900]\ttraining's binary_logloss: 0.146185\ttraining's amex_metric gini 0.975 recall 0.873: 0.923749\tvalid_1's binary_logloss: 0.218228\tvalid_1's amex_metric gini 0.923 recall 0.665: 0.793656\n",
      "[9000]\ttraining's binary_logloss: 0.145416\ttraining's amex_metric gini 0.975 recall 0.875: 0.925177\tvalid_1's binary_logloss: 0.218201\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793132\n",
      "[9100]\ttraining's binary_logloss: 0.14486\ttraining's amex_metric gini 0.975 recall 0.877: 0.926016\tvalid_1's binary_logloss: 0.2182\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793398\n",
      "[9200]\ttraining's binary_logloss: 0.14394\ttraining's amex_metric gini 0.976 recall 0.879: 0.927261\tvalid_1's binary_logloss: 0.218177\tvalid_1's amex_metric gini 0.923 recall 0.663: 0.792946\n",
      "[9300]\ttraining's binary_logloss: 0.143232\ttraining's amex_metric gini 0.976 recall 0.881: 0.928405\tvalid_1's binary_logloss: 0.218172\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793325\n",
      "[9400]\ttraining's binary_logloss: 0.142815\ttraining's amex_metric gini 0.977 recall 0.882: 0.929414\tvalid_1's binary_logloss: 0.218198\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793053\n",
      "[9500]\ttraining's binary_logloss: 0.141951\ttraining's amex_metric gini 0.977 recall 0.884: 0.930534\tvalid_1's binary_logloss: 0.218207\tvalid_1's amex_metric gini 0.923 recall 0.663: 0.792932\n",
      "[9600]\ttraining's binary_logloss: 0.141215\ttraining's amex_metric gini 0.977 recall 0.886: 0.931624\tvalid_1's binary_logloss: 0.218149\tvalid_1's amex_metric gini 0.923 recall 0.663: 0.7928\n",
      "[9700]\ttraining's binary_logloss: 0.140497\ttraining's amex_metric gini 0.978 recall 0.887: 0.932559\tvalid_1's binary_logloss: 0.218117\tvalid_1's amex_metric gini 0.923 recall 0.663: 0.792881\n",
      "[9800]\ttraining's binary_logloss: 0.139785\ttraining's amex_metric gini 0.978 recall 0.889: 0.933761\tvalid_1's binary_logloss: 0.218065\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793204\n",
      "[9900]\ttraining's binary_logloss: 0.139094\ttraining's amex_metric gini 0.979 recall 0.891: 0.934743\tvalid_1's binary_logloss: 0.218096\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793079\n",
      "[10000]\ttraining's binary_logloss: 0.138514\ttraining's amex_metric gini 0.979 recall 0.892: 0.935681\tvalid_1's binary_logloss: 0.218094\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793116\n",
      "[10100]\ttraining's binary_logloss: 0.138157\ttraining's amex_metric gini 0.979 recall 0.894: 0.936621\tvalid_1's binary_logloss: 0.218118\tvalid_1's amex_metric gini 0.923 recall 0.663: 0.792766\n",
      "[10200]\ttraining's binary_logloss: 0.137555\ttraining's amex_metric gini 0.980 recall 0.895: 0.937515\tvalid_1's binary_logloss: 0.218143\tvalid_1's amex_metric gini 0.923 recall 0.663: 0.79272\n",
      "[10300]\ttraining's binary_logloss: 0.136849\ttraining's amex_metric gini 0.980 recall 0.897: 0.938323\tvalid_1's binary_logloss: 0.218137\tvalid_1's amex_metric gini 0.923 recall 0.663: 0.792984\n",
      "[10400]\ttraining's binary_logloss: 0.136236\ttraining's amex_metric gini 0.980 recall 0.898: 0.939252\tvalid_1's binary_logloss: 0.218158\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793088\n",
      "[10500]\ttraining's binary_logloss: 0.135508\ttraining's amex_metric gini 0.981 recall 0.900: 0.940139\tvalid_1's binary_logloss: 0.218137\tvalid_1's amex_metric gini 0.923 recall 0.662: 0.792147\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "lgb_valid = lgb.Dataset(X_test.reshape(X_test.shape[0], -1), y_test,)\n",
    "model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 105541, number of negative: 302381\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.232630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 329344\n",
      "[LightGBM] [Info] Number of data points in the train set: 407922, number of used features: 2443\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258728 -> initscore=-1.052588\n",
      "[LightGBM] [Info] Start training from score -1.052588\n",
      "[100]\ttraining's binary_logloss: 0.470607\ttraining's amex_metric gini 0.903 recall 0.604: 0.753508\tvalid_1's binary_logloss: 0.472209\tvalid_1's amex_metric gini 0.900 recall 0.592: 0.745712\n",
      "[200]\ttraining's binary_logloss: 0.450727\ttraining's amex_metric gini 0.905 recall 0.610: 0.757501\tvalid_1's binary_logloss: 0.452516\tvalid_1's amex_metric gini 0.901 recall 0.598: 0.74981\n",
      "[300]\ttraining's binary_logloss: 0.397729\ttraining's amex_metric gini 0.907 recall 0.617: 0.762134\tvalid_1's binary_logloss: 0.400057\tvalid_1's amex_metric gini 0.903 recall 0.603: 0.752913\n",
      "[400]\ttraining's binary_logloss: 0.372899\ttraining's amex_metric gini 0.908 recall 0.622: 0.764969\tvalid_1's binary_logloss: 0.375608\tvalid_1's amex_metric gini 0.904 recall 0.608: 0.755943\n",
      "[500]\ttraining's binary_logloss: 0.342295\ttraining's amex_metric gini 0.910 recall 0.626: 0.767621\tvalid_1's binary_logloss: 0.345522\tvalid_1's amex_metric gini 0.905 recall 0.613: 0.759354\n",
      "[600]\ttraining's binary_logloss: 0.324123\ttraining's amex_metric gini 0.911 recall 0.630: 0.770687\tvalid_1's binary_logloss: 0.327748\tvalid_1's amex_metric gini 0.907 recall 0.616: 0.761217\n",
      "[700]\ttraining's binary_logloss: 0.294551\ttraining's amex_metric gini 0.913 recall 0.637: 0.774782\tvalid_1's binary_logloss: 0.298961\tvalid_1's amex_metric gini 0.908 recall 0.618: 0.76312\n",
      "[800]\ttraining's binary_logloss: 0.273933\ttraining's amex_metric gini 0.915 recall 0.643: 0.778995\tvalid_1's binary_logloss: 0.279143\tvalid_1's amex_metric gini 0.909 recall 0.626: 0.767671\n",
      "[900]\ttraining's binary_logloss: 0.258739\ttraining's amex_metric gini 0.917 recall 0.650: 0.783707\tvalid_1's binary_logloss: 0.264772\tvalid_1's amex_metric gini 0.911 recall 0.629: 0.769663\n",
      "[1000]\ttraining's binary_logloss: 0.252262\ttraining's amex_metric gini 0.918 recall 0.655: 0.786526\tvalid_1's binary_logloss: 0.258815\tvalid_1's amex_metric gini 0.912 recall 0.630: 0.771017\n",
      "[1100]\ttraining's binary_logloss: 0.242277\ttraining's amex_metric gini 0.920 recall 0.660: 0.790247\tvalid_1's binary_logloss: 0.249687\tvalid_1's amex_metric gini 0.913 recall 0.635: 0.773748\n",
      "[1200]\ttraining's binary_logloss: 0.238952\ttraining's amex_metric gini 0.921 recall 0.665: 0.793164\tvalid_1's binary_logloss: 0.246849\tvalid_1's amex_metric gini 0.914 recall 0.635: 0.774361\n",
      "[1300]\ttraining's binary_logloss: 0.235016\ttraining's amex_metric gini 0.923 recall 0.669: 0.795637\tvalid_1's binary_logloss: 0.243516\tvalid_1's amex_metric gini 0.914 recall 0.636: 0.775206\n",
      "[1400]\ttraining's binary_logloss: 0.230468\ttraining's amex_metric gini 0.924 recall 0.672: 0.797942\tvalid_1's binary_logloss: 0.239715\tvalid_1's amex_metric gini 0.915 recall 0.637: 0.776294\n",
      "[1500]\ttraining's binary_logloss: 0.22818\ttraining's amex_metric gini 0.925 recall 0.676: 0.80029\tvalid_1's binary_logloss: 0.237943\tvalid_1's amex_metric gini 0.916 recall 0.639: 0.777326\n",
      "[1600]\ttraining's binary_logloss: 0.223005\ttraining's amex_metric gini 0.926 recall 0.680: 0.802959\tvalid_1's binary_logloss: 0.233857\tvalid_1's amex_metric gini 0.917 recall 0.641: 0.778611\n",
      "[1700]\ttraining's binary_logloss: 0.22053\ttraining's amex_metric gini 0.928 recall 0.684: 0.805627\tvalid_1's binary_logloss: 0.232129\tvalid_1's amex_metric gini 0.917 recall 0.643: 0.779826\n",
      "[1800]\ttraining's binary_logloss: 0.219235\ttraining's amex_metric gini 0.929 recall 0.687: 0.80782\tvalid_1's binary_logloss: 0.231368\tvalid_1's amex_metric gini 0.918 recall 0.644: 0.780673\n",
      "[1900]\ttraining's binary_logloss: 0.217046\ttraining's amex_metric gini 0.930 recall 0.691: 0.810351\tvalid_1's binary_logloss: 0.229964\tvalid_1's amex_metric gini 0.918 recall 0.645: 0.781542\n",
      "[2000]\ttraining's binary_logloss: 0.214628\ttraining's amex_metric gini 0.931 recall 0.694: 0.812406\tvalid_1's binary_logloss: 0.228458\tvalid_1's amex_metric gini 0.919 recall 0.645: 0.781921\n",
      "[2100]\ttraining's binary_logloss: 0.213205\ttraining's amex_metric gini 0.932 recall 0.698: 0.814626\tvalid_1's binary_logloss: 0.227696\tvalid_1's amex_metric gini 0.919 recall 0.645: 0.782194\n",
      "[2200]\ttraining's binary_logloss: 0.212025\ttraining's amex_metric gini 0.932 recall 0.700: 0.816304\tvalid_1's binary_logloss: 0.227161\tvalid_1's amex_metric gini 0.919 recall 0.648: 0.783687\n",
      "[2300]\ttraining's binary_logloss: 0.210243\ttraining's amex_metric gini 0.933 recall 0.704: 0.818451\tvalid_1's binary_logloss: 0.226227\tvalid_1's amex_metric gini 0.920 recall 0.649: 0.784403\n",
      "[2400]\ttraining's binary_logloss: 0.209162\ttraining's amex_metric gini 0.934 recall 0.706: 0.820237\tvalid_1's binary_logloss: 0.225776\tvalid_1's amex_metric gini 0.920 recall 0.650: 0.78485\n",
      "[2500]\ttraining's binary_logloss: 0.208003\ttraining's amex_metric gini 0.935 recall 0.709: 0.82205\tvalid_1's binary_logloss: 0.2253\tvalid_1's amex_metric gini 0.920 recall 0.650: 0.785012\n",
      "[2600]\ttraining's binary_logloss: 0.206689\ttraining's amex_metric gini 0.936 recall 0.712: 0.823726\tvalid_1's binary_logloss: 0.224744\tvalid_1's amex_metric gini 0.920 recall 0.651: 0.785649\n",
      "[2700]\ttraining's binary_logloss: 0.205176\ttraining's amex_metric gini 0.936 recall 0.715: 0.825728\tvalid_1's binary_logloss: 0.224072\tvalid_1's amex_metric gini 0.921 recall 0.652: 0.786147\n",
      "[2800]\ttraining's binary_logloss: 0.204411\ttraining's amex_metric gini 0.937 recall 0.717: 0.827094\tvalid_1's binary_logloss: 0.223878\tvalid_1's amex_metric gini 0.921 recall 0.652: 0.786273\n",
      "[2900]\ttraining's binary_logloss: 0.202924\ttraining's amex_metric gini 0.938 recall 0.720: 0.828845\tvalid_1's binary_logloss: 0.223311\tvalid_1's amex_metric gini 0.921 recall 0.652: 0.786318\n",
      "[3000]\ttraining's binary_logloss: 0.201433\ttraining's amex_metric gini 0.939 recall 0.723: 0.830641\tvalid_1's binary_logloss: 0.222758\tvalid_1's amex_metric gini 0.921 recall 0.654: 0.787474\n",
      "[3100]\ttraining's binary_logloss: 0.200317\ttraining's amex_metric gini 0.939 recall 0.725: 0.832464\tvalid_1's binary_logloss: 0.222368\tvalid_1's amex_metric gini 0.921 recall 0.654: 0.787812\n",
      "[3200]\ttraining's binary_logloss: 0.19915\ttraining's amex_metric gini 0.940 recall 0.729: 0.834375\tvalid_1's binary_logloss: 0.222007\tvalid_1's amex_metric gini 0.921 recall 0.655: 0.788196\n",
      "[3300]\ttraining's binary_logloss: 0.19807\ttraining's amex_metric gini 0.941 recall 0.732: 0.836238\tvalid_1's binary_logloss: 0.221692\tvalid_1's amex_metric gini 0.922 recall 0.656: 0.788885\n",
      "[3400]\ttraining's binary_logloss: 0.196608\ttraining's amex_metric gini 0.942 recall 0.735: 0.838186\tvalid_1's binary_logloss: 0.221262\tvalid_1's amex_metric gini 0.922 recall 0.656: 0.78881\n",
      "[3500]\ttraining's binary_logloss: 0.195554\ttraining's amex_metric gini 0.943 recall 0.738: 0.840266\tvalid_1's binary_logloss: 0.22101\tvalid_1's amex_metric gini 0.922 recall 0.656: 0.788965\n",
      "[3600]\ttraining's binary_logloss: 0.194519\ttraining's amex_metric gini 0.943 recall 0.740: 0.841848\tvalid_1's binary_logloss: 0.220788\tvalid_1's amex_metric gini 0.922 recall 0.656: 0.789024\n",
      "[3700]\ttraining's binary_logloss: 0.193064\ttraining's amex_metric gini 0.944 recall 0.743: 0.8436\tvalid_1's binary_logloss: 0.220364\tvalid_1's amex_metric gini 0.922 recall 0.655: 0.788498\n",
      "[3800]\ttraining's binary_logloss: 0.192118\ttraining's amex_metric gini 0.945 recall 0.747: 0.845735\tvalid_1's binary_logloss: 0.220206\tvalid_1's amex_metric gini 0.922 recall 0.655: 0.78878\n",
      "[3900]\ttraining's binary_logloss: 0.191278\ttraining's amex_metric gini 0.945 recall 0.749: 0.847312\tvalid_1's binary_logloss: 0.220095\tvalid_1's amex_metric gini 0.922 recall 0.656: 0.789086\n",
      "[4000]\ttraining's binary_logloss: 0.190451\ttraining's amex_metric gini 0.946 recall 0.752: 0.848965\tvalid_1's binary_logloss: 0.22\tvalid_1's amex_metric gini 0.922 recall 0.655: 0.788777\n",
      "[4100]\ttraining's binary_logloss: 0.189456\ttraining's amex_metric gini 0.947 recall 0.754: 0.850543\tvalid_1's binary_logloss: 0.219778\tvalid_1's amex_metric gini 0.923 recall 0.656: 0.789249\n",
      "[4200]\ttraining's binary_logloss: 0.188224\ttraining's amex_metric gini 0.947 recall 0.757: 0.852107\tvalid_1's binary_logloss: 0.219556\tvalid_1's amex_metric gini 0.923 recall 0.657: 0.789695\n",
      "[4300]\ttraining's binary_logloss: 0.186998\ttraining's amex_metric gini 0.948 recall 0.760: 0.85394\tvalid_1's binary_logloss: 0.21934\tvalid_1's amex_metric gini 0.923 recall 0.658: 0.790226\n",
      "[4400]\ttraining's binary_logloss: 0.186424\ttraining's amex_metric gini 0.949 recall 0.763: 0.85567\tvalid_1's binary_logloss: 0.219304\tvalid_1's amex_metric gini 0.923 recall 0.656: 0.789549\n",
      "[4500]\ttraining's binary_logloss: 0.185446\ttraining's amex_metric gini 0.949 recall 0.765: 0.857296\tvalid_1's binary_logloss: 0.219167\tvalid_1's amex_metric gini 0.923 recall 0.657: 0.789852\n",
      "[4600]\ttraining's binary_logloss: 0.184102\ttraining's amex_metric gini 0.950 recall 0.769: 0.859394\tvalid_1's binary_logloss: 0.218898\tvalid_1's amex_metric gini 0.923 recall 0.658: 0.790353\n",
      "[4700]\ttraining's binary_logloss: 0.183348\ttraining's amex_metric gini 0.951 recall 0.772: 0.861171\tvalid_1's binary_logloss: 0.21882\tvalid_1's amex_metric gini 0.923 recall 0.657: 0.790201\n",
      "[4800]\ttraining's binary_logloss: 0.182416\ttraining's amex_metric gini 0.951 recall 0.774: 0.862768\tvalid_1's binary_logloss: 0.21874\tvalid_1's amex_metric gini 0.923 recall 0.656: 0.789319\n",
      "[4900]\ttraining's binary_logloss: 0.181349\ttraining's amex_metric gini 0.952 recall 0.777: 0.864449\tvalid_1's binary_logloss: 0.218598\tvalid_1's amex_metric gini 0.923 recall 0.657: 0.790253\n",
      "[5000]\ttraining's binary_logloss: 0.180581\ttraining's amex_metric gini 0.953 recall 0.779: 0.865999\tvalid_1's binary_logloss: 0.218533\tvalid_1's amex_metric gini 0.923 recall 0.657: 0.789975\n",
      "[5100]\ttraining's binary_logloss: 0.179822\ttraining's amex_metric gini 0.953 recall 0.782: 0.867474\tvalid_1's binary_logloss: 0.218466\tvalid_1's amex_metric gini 0.923 recall 0.657: 0.790156\n",
      "[5200]\ttraining's binary_logloss: 0.178759\ttraining's amex_metric gini 0.954 recall 0.784: 0.868963\tvalid_1's binary_logloss: 0.218327\tvalid_1's amex_metric gini 0.923 recall 0.659: 0.791195\n",
      "[5300]\ttraining's binary_logloss: 0.178186\ttraining's amex_metric gini 0.954 recall 0.787: 0.87059\tvalid_1's binary_logloss: 0.218315\tvalid_1's amex_metric gini 0.923 recall 0.658: 0.790801\n",
      "[5400]\ttraining's binary_logloss: 0.177246\ttraining's amex_metric gini 0.955 recall 0.789: 0.871829\tvalid_1's binary_logloss: 0.218224\tvalid_1's amex_metric gini 0.923 recall 0.659: 0.791076\n",
      "[5500]\ttraining's binary_logloss: 0.176224\ttraining's amex_metric gini 0.956 recall 0.791: 0.873478\tvalid_1's binary_logloss: 0.218104\tvalid_1's amex_metric gini 0.923 recall 0.659: 0.790989\n",
      "[5600]\ttraining's binary_logloss: 0.17547\ttraining's amex_metric gini 0.956 recall 0.793: 0.874682\tvalid_1's binary_logloss: 0.218096\tvalid_1's amex_metric gini 0.923 recall 0.658: 0.790842\n",
      "[5700]\ttraining's binary_logloss: 0.174613\ttraining's amex_metric gini 0.957 recall 0.796: 0.876471\tvalid_1's binary_logloss: 0.218033\tvalid_1's amex_metric gini 0.923 recall 0.659: 0.791082\n",
      "[5800]\ttraining's binary_logloss: 0.173975\ttraining's amex_metric gini 0.957 recall 0.798: 0.877815\tvalid_1's binary_logloss: 0.218019\tvalid_1's amex_metric gini 0.923 recall 0.659: 0.791055\n",
      "[5900]\ttraining's binary_logloss: 0.173071\ttraining's amex_metric gini 0.958 recall 0.801: 0.879299\tvalid_1's binary_logloss: 0.217929\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.791598\n",
      "[6000]\ttraining's binary_logloss: 0.17253\ttraining's amex_metric gini 0.958 recall 0.803: 0.880682\tvalid_1's binary_logloss: 0.217923\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.791615\n",
      "[6100]\ttraining's binary_logloss: 0.171788\ttraining's amex_metric gini 0.959 recall 0.805: 0.881813\tvalid_1's binary_logloss: 0.217896\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.791729\n",
      "[6200]\ttraining's binary_logloss: 0.171041\ttraining's amex_metric gini 0.959 recall 0.807: 0.883044\tvalid_1's binary_logloss: 0.21786\tvalid_1's amex_metric gini 0.924 recall 0.659: 0.791208\n",
      "[6300]\ttraining's binary_logloss: 0.170182\ttraining's amex_metric gini 0.960 recall 0.809: 0.884243\tvalid_1's binary_logloss: 0.217756\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.791043\n",
      "[6400]\ttraining's binary_logloss: 0.169492\ttraining's amex_metric gini 0.960 recall 0.811: 0.885666\tvalid_1's binary_logloss: 0.217695\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.791818\n",
      "[6500]\ttraining's binary_logloss: 0.168585\ttraining's amex_metric gini 0.961 recall 0.814: 0.887094\tvalid_1's binary_logloss: 0.217608\tvalid_1's amex_metric gini 0.924 recall 0.659: 0.791273\n",
      "[6600]\ttraining's binary_logloss: 0.167578\ttraining's amex_metric gini 0.961 recall 0.816: 0.888691\tvalid_1's binary_logloss: 0.217545\tvalid_1's amex_metric gini 0.924 recall 0.659: 0.791206\n",
      "[6700]\ttraining's binary_logloss: 0.166661\ttraining's amex_metric gini 0.962 recall 0.818: 0.889917\tvalid_1's binary_logloss: 0.217465\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.79111\n",
      "[6800]\ttraining's binary_logloss: 0.165684\ttraining's amex_metric gini 0.962 recall 0.821: 0.89148\tvalid_1's binary_logloss: 0.217415\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.790963\n",
      "[6900]\ttraining's binary_logloss: 0.164875\ttraining's amex_metric gini 0.963 recall 0.824: 0.893191\tvalid_1's binary_logloss: 0.217358\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.790906\n",
      "[7000]\ttraining's binary_logloss: 0.164019\ttraining's amex_metric gini 0.963 recall 0.826: 0.894715\tvalid_1's binary_logloss: 0.217289\tvalid_1's amex_metric gini 0.924 recall 0.659: 0.791643\n",
      "[7100]\ttraining's binary_logloss: 0.163028\ttraining's amex_metric gini 0.964 recall 0.828: 0.89606\tvalid_1's binary_logloss: 0.217218\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.790833\n",
      "[7200]\ttraining's binary_logloss: 0.162138\ttraining's amex_metric gini 0.965 recall 0.831: 0.897544\tvalid_1's binary_logloss: 0.217173\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.790955\n",
      "[7300]\ttraining's binary_logloss: 0.161339\ttraining's amex_metric gini 0.965 recall 0.833: 0.899058\tvalid_1's binary_logloss: 0.217123\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.790863\n",
      "[7400]\ttraining's binary_logloss: 0.160538\ttraining's amex_metric gini 0.966 recall 0.835: 0.900569\tvalid_1's binary_logloss: 0.217092\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.790834\n",
      "[7500]\ttraining's binary_logloss: 0.15967\ttraining's amex_metric gini 0.966 recall 0.838: 0.901994\tvalid_1's binary_logloss: 0.217061\tvalid_1's amex_metric gini 0.924 recall 0.659: 0.791438\n",
      "[7600]\ttraining's binary_logloss: 0.158926\ttraining's amex_metric gini 0.967 recall 0.840: 0.903379\tvalid_1's binary_logloss: 0.217045\tvalid_1's amex_metric gini 0.924 recall 0.659: 0.791635\n",
      "[7700]\ttraining's binary_logloss: 0.158297\ttraining's amex_metric gini 0.967 recall 0.842: 0.90464\tvalid_1's binary_logloss: 0.217047\tvalid_1's amex_metric gini 0.924 recall 0.659: 0.791638\n",
      "[7800]\ttraining's binary_logloss: 0.157553\ttraining's amex_metric gini 0.968 recall 0.844: 0.90603\tvalid_1's binary_logloss: 0.216993\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.791769\n",
      "[7900]\ttraining's binary_logloss: 0.156732\ttraining's amex_metric gini 0.968 recall 0.847: 0.907313\tvalid_1's binary_logloss: 0.216917\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.791905\n",
      "[8000]\ttraining's binary_logloss: 0.15578\ttraining's amex_metric gini 0.969 recall 0.848: 0.908323\tvalid_1's binary_logloss: 0.21684\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792447\n",
      "[8100]\ttraining's binary_logloss: 0.155107\ttraining's amex_metric gini 0.969 recall 0.850: 0.909554\tvalid_1's binary_logloss: 0.21684\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792597\n",
      "[8200]\ttraining's binary_logloss: 0.154235\ttraining's amex_metric gini 0.970 recall 0.852: 0.910969\tvalid_1's binary_logloss: 0.216804\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.792301\n",
      "[8300]\ttraining's binary_logloss: 0.153627\ttraining's amex_metric gini 0.970 recall 0.855: 0.912363\tvalid_1's binary_logloss: 0.216806\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792377\n",
      "[8400]\ttraining's binary_logloss: 0.153055\ttraining's amex_metric gini 0.970 recall 0.857: 0.913491\tvalid_1's binary_logloss: 0.216813\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.792228\n",
      "[8500]\ttraining's binary_logloss: 0.152446\ttraining's amex_metric gini 0.971 recall 0.858: 0.914621\tvalid_1's binary_logloss: 0.21683\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.792824\n",
      "[8600]\ttraining's binary_logloss: 0.151726\ttraining's amex_metric gini 0.971 recall 0.861: 0.915904\tvalid_1's binary_logloss: 0.216821\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.792936\n",
      "[8700]\ttraining's binary_logloss: 0.15075\ttraining's amex_metric gini 0.972 recall 0.863: 0.917333\tvalid_1's binary_logloss: 0.216767\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793286\n",
      "[8800]\ttraining's binary_logloss: 0.150097\ttraining's amex_metric gini 0.972 recall 0.865: 0.918611\tvalid_1's binary_logloss: 0.21673\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793001\n",
      "[8900]\ttraining's binary_logloss: 0.14943\ttraining's amex_metric gini 0.972 recall 0.867: 0.919678\tvalid_1's binary_logloss: 0.216719\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792437\n",
      "[9000]\ttraining's binary_logloss: 0.148681\ttraining's amex_metric gini 0.973 recall 0.869: 0.920793\tvalid_1's binary_logloss: 0.216733\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.792842\n",
      "[9100]\ttraining's binary_logloss: 0.148127\ttraining's amex_metric gini 0.973 recall 0.871: 0.92202\tvalid_1's binary_logloss: 0.21674\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792577\n",
      "[9200]\ttraining's binary_logloss: 0.147221\ttraining's amex_metric gini 0.974 recall 0.872: 0.923064\tvalid_1's binary_logloss: 0.216714\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.792921\n",
      "[9300]\ttraining's binary_logloss: 0.146527\ttraining's amex_metric gini 0.974 recall 0.874: 0.924324\tvalid_1's binary_logloss: 0.21669\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.792251\n",
      "[9400]\ttraining's binary_logloss: 0.146134\ttraining's amex_metric gini 0.975 recall 0.876: 0.925436\tvalid_1's binary_logloss: 0.216706\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793001\n",
      "[9500]\ttraining's binary_logloss: 0.14529\ttraining's amex_metric gini 0.975 recall 0.878: 0.926264\tvalid_1's binary_logloss: 0.216657\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793014\n",
      "[9600]\ttraining's binary_logloss: 0.144585\ttraining's amex_metric gini 0.975 recall 0.879: 0.927307\tvalid_1's binary_logloss: 0.216624\tvalid_1's amex_metric gini 0.924 recall 0.663: 0.793741\n",
      "[9700]\ttraining's binary_logloss: 0.14388\ttraining's amex_metric gini 0.976 recall 0.881: 0.92856\tvalid_1's binary_logloss: 0.216609\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793333\n",
      "[9800]\ttraining's binary_logloss: 0.143169\ttraining's amex_metric gini 0.976 recall 0.883: 0.929486\tvalid_1's binary_logloss: 0.21659\tvalid_1's amex_metric gini 0.924 recall 0.663: 0.793449\n",
      "[9900]\ttraining's binary_logloss: 0.142503\ttraining's amex_metric gini 0.976 recall 0.884: 0.930429\tvalid_1's binary_logloss: 0.216596\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793032\n",
      "[10000]\ttraining's binary_logloss: 0.141925\ttraining's amex_metric gini 0.977 recall 0.886: 0.931615\tvalid_1's binary_logloss: 0.216581\tvalid_1's amex_metric gini 0.924 recall 0.663: 0.793376\n",
      "[10100]\ttraining's binary_logloss: 0.141573\ttraining's amex_metric gini 0.977 recall 0.888: 0.932693\tvalid_1's binary_logloss: 0.216615\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793062\n",
      "[10200]\ttraining's binary_logloss: 0.140981\ttraining's amex_metric gini 0.977 recall 0.890: 0.933725\tvalid_1's binary_logloss: 0.216619\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793021\n",
      "[10300]\ttraining's binary_logloss: 0.140288\ttraining's amex_metric gini 0.978 recall 0.892: 0.934711\tvalid_1's binary_logloss: 0.216583\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792543\n",
      "[10400]\ttraining's binary_logloss: 0.139693\ttraining's amex_metric gini 0.978 recall 0.893: 0.935584\tvalid_1's binary_logloss: 0.21658\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792506\n",
      "[10500]\ttraining's binary_logloss: 0.138972\ttraining's amex_metric gini 0.978 recall 0.894: 0.93643\tvalid_1's binary_logloss: 0.216569\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792544\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "lgb_valid = lgb.Dataset(X_test.reshape(X_test.shape[0], -1), y_test,)\n",
    "model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 79627, number of negative: 263514\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.150216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 328368\n",
      "[LightGBM] [Info] Number of data points in the train set: 343141, number of used features: 2425\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.232053 -> initscore=-1.196753\n",
      "[LightGBM] [Info] Start training from score -1.196753\n",
      "[100]\ttraining's binary_logloss: 0.447653\ttraining's amex_metric gini 0.918 recall 0.656: 0.787208\tvalid_1's binary_logloss: 0.449236\tvalid_1's amex_metric gini 0.911 recall 0.631: 0.770685\n",
      "[200]\ttraining's binary_logloss: 0.430692\ttraining's amex_metric gini 0.919 recall 0.661: 0.790093\tvalid_1's binary_logloss: 0.433192\tvalid_1's amex_metric gini 0.911 recall 0.635: 0.77314\n",
      "[300]\ttraining's binary_logloss: 0.3763\ttraining's amex_metric gini 0.921 recall 0.668: 0.794409\tvalid_1's binary_logloss: 0.379827\tvalid_1's amex_metric gini 0.913 recall 0.641: 0.77701\n",
      "[400]\ttraining's binary_logloss: 0.350905\ttraining's amex_metric gini 0.922 recall 0.673: 0.797448\tvalid_1's binary_logloss: 0.355111\tvalid_1's amex_metric gini 0.914 recall 0.643: 0.778446\n",
      "[500]\ttraining's binary_logloss: 0.319394\ttraining's amex_metric gini 0.923 recall 0.677: 0.79987\tvalid_1's binary_logloss: 0.324477\tvalid_1's amex_metric gini 0.914 recall 0.645: 0.779684\n",
      "[600]\ttraining's binary_logloss: 0.300473\ttraining's amex_metric gini 0.924 recall 0.682: 0.803044\tvalid_1's binary_logloss: 0.306225\tvalid_1's amex_metric gini 0.915 recall 0.648: 0.781629\n",
      "[700]\ttraining's binary_logloss: 0.269951\ttraining's amex_metric gini 0.926 recall 0.688: 0.806783\tvalid_1's binary_logloss: 0.276793\tvalid_1's amex_metric gini 0.917 recall 0.652: 0.784223\n",
      "[800]\ttraining's binary_logloss: 0.248732\ttraining's amex_metric gini 0.928 recall 0.695: 0.811098\tvalid_1's binary_logloss: 0.256684\tvalid_1's amex_metric gini 0.918 recall 0.657: 0.787324\n",
      "[900]\ttraining's binary_logloss: 0.233053\ttraining's amex_metric gini 0.930 recall 0.702: 0.816039\tvalid_1's binary_logloss: 0.242107\tvalid_1's amex_metric gini 0.919 recall 0.665: 0.792072\n",
      "[1000]\ttraining's binary_logloss: 0.226394\ttraining's amex_metric gini 0.931 recall 0.707: 0.819083\tvalid_1's binary_logloss: 0.236154\tvalid_1's amex_metric gini 0.920 recall 0.667: 0.793794\n",
      "[1100]\ttraining's binary_logloss: 0.216284\ttraining's amex_metric gini 0.933 recall 0.712: 0.822383\tvalid_1's binary_logloss: 0.227138\tvalid_1's amex_metric gini 0.921 recall 0.669: 0.794984\n",
      "[1200]\ttraining's binary_logloss: 0.212874\ttraining's amex_metric gini 0.934 recall 0.717: 0.825206\tvalid_1's binary_logloss: 0.224306\tvalid_1's amex_metric gini 0.922 recall 0.670: 0.796181\n",
      "[1300]\ttraining's binary_logloss: 0.208732\ttraining's amex_metric gini 0.935 recall 0.721: 0.827942\tvalid_1's binary_logloss: 0.220859\tvalid_1's amex_metric gini 0.923 recall 0.673: 0.797787\n",
      "[1400]\ttraining's binary_logloss: 0.203977\ttraining's amex_metric gini 0.936 recall 0.726: 0.830976\tvalid_1's binary_logloss: 0.217035\tvalid_1's amex_metric gini 0.924 recall 0.676: 0.799815\n",
      "[1500]\ttraining's binary_logloss: 0.201638\ttraining's amex_metric gini 0.937 recall 0.729: 0.833205\tvalid_1's binary_logloss: 0.215354\tvalid_1's amex_metric gini 0.924 recall 0.677: 0.800795\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "lgb_valid = lgb.Dataset(X_test.reshape(X_test.shape[0], -1), y_test,)\n",
    "model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 1500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 79627, number of negative: 263514\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.282570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 328368\n",
      "[LightGBM] [Info] Number of data points in the train set: 343141, number of used features: 2425\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.232053 -> initscore=-1.196753\n",
      "[LightGBM] [Info] Start training from score -1.196753\n",
      "[100]\ttraining's binary_logloss: 0.448219\ttraining's amex_metric gini 0.917 recall 0.652: 0.784676\tvalid_1's binary_logloss: 0.449789\tvalid_1's amex_metric gini 0.910 recall 0.628: 0.769098\n",
      "[200]\ttraining's binary_logloss: 0.431256\ttraining's amex_metric gini 0.919 recall 0.658: 0.788337\tvalid_1's binary_logloss: 0.433742\tvalid_1's amex_metric gini 0.911 recall 0.635: 0.772682\n",
      "[300]\ttraining's binary_logloss: 0.376935\ttraining's amex_metric gini 0.920 recall 0.666: 0.7932\tvalid_1's binary_logloss: 0.380447\tvalid_1's amex_metric gini 0.912 recall 0.640: 0.776354\n",
      "[400]\ttraining's binary_logloss: 0.351547\ttraining's amex_metric gini 0.921 recall 0.671: 0.796181\tvalid_1's binary_logloss: 0.355741\tvalid_1's amex_metric gini 0.913 recall 0.640: 0.776788\n",
      "[500]\ttraining's binary_logloss: 0.32007\ttraining's amex_metric gini 0.922 recall 0.676: 0.798998\tvalid_1's binary_logloss: 0.325111\tvalid_1's amex_metric gini 0.914 recall 0.645: 0.779358\n",
      "[600]\ttraining's binary_logloss: 0.30117\ttraining's amex_metric gini 0.924 recall 0.680: 0.801985\tvalid_1's binary_logloss: 0.30685\tvalid_1's amex_metric gini 0.915 recall 0.648: 0.78138\n",
      "[700]\ttraining's binary_logloss: 0.27066\ttraining's amex_metric gini 0.925 recall 0.686: 0.805923\tvalid_1's binary_logloss: 0.277409\tvalid_1's amex_metric gini 0.916 recall 0.652: 0.784031\n",
      "[800]\ttraining's binary_logloss: 0.249452\ttraining's amex_metric gini 0.927 recall 0.693: 0.810251\tvalid_1's binary_logloss: 0.25727\tvalid_1's amex_metric gini 0.918 recall 0.657: 0.787511\n",
      "[900]\ttraining's binary_logloss: 0.233784\ttraining's amex_metric gini 0.929 recall 0.701: 0.815059\tvalid_1's binary_logloss: 0.242608\tvalid_1's amex_metric gini 0.919 recall 0.663: 0.791174\n",
      "[1000]\ttraining's binary_logloss: 0.227146\ttraining's amex_metric gini 0.931 recall 0.705: 0.817753\tvalid_1's binary_logloss: 0.236659\tvalid_1's amex_metric gini 0.920 recall 0.664: 0.791886\n",
      "[1100]\ttraining's binary_logloss: 0.217053\ttraining's amex_metric gini 0.932 recall 0.711: 0.821314\tvalid_1's binary_logloss: 0.227594\tvalid_1's amex_metric gini 0.921 recall 0.667: 0.793959\n",
      "[1200]\ttraining's binary_logloss: 0.213654\ttraining's amex_metric gini 0.933 recall 0.715: 0.823997\tvalid_1's binary_logloss: 0.224717\tvalid_1's amex_metric gini 0.922 recall 0.670: 0.795732\n",
      "[1300]\ttraining's binary_logloss: 0.209549\ttraining's amex_metric gini 0.934 recall 0.720: 0.827082\tvalid_1's binary_logloss: 0.221298\tvalid_1's amex_metric gini 0.923 recall 0.673: 0.79768\n",
      "[1400]\ttraining's binary_logloss: 0.20483\ttraining's amex_metric gini 0.936 recall 0.724: 0.829766\tvalid_1's binary_logloss: 0.217456\tvalid_1's amex_metric gini 0.923 recall 0.676: 0.799813\n",
      "[1500]\ttraining's binary_logloss: 0.202526\ttraining's amex_metric gini 0.937 recall 0.728: 0.832132\tvalid_1's binary_logloss: 0.215767\tvalid_1's amex_metric gini 0.924 recall 0.678: 0.800789\n",
      "[1600]\ttraining's binary_logloss: 0.197347\ttraining's amex_metric gini 0.938 recall 0.733: 0.835428\tvalid_1's binary_logloss: 0.211862\tvalid_1's amex_metric gini 0.925 recall 0.677: 0.800841\n",
      "[1700]\ttraining's binary_logloss: 0.194863\ttraining's amex_metric gini 0.939 recall 0.737: 0.83812\tvalid_1's binary_logloss: 0.210257\tvalid_1's amex_metric gini 0.925 recall 0.679: 0.801963\n",
      "[1800]\ttraining's binary_logloss: 0.193499\ttraining's amex_metric gini 0.940 recall 0.740: 0.840335\tvalid_1's binary_logloss: 0.209455\tvalid_1's amex_metric gini 0.926 recall 0.680: 0.802991\n",
      "[1900]\ttraining's binary_logloss: 0.191254\ttraining's amex_metric gini 0.941 recall 0.744: 0.842783\tvalid_1's binary_logloss: 0.208105\tvalid_1's amex_metric gini 0.926 recall 0.685: 0.805567\n",
      "[2000]\ttraining's binary_logloss: 0.188809\ttraining's amex_metric gini 0.942 recall 0.748: 0.845279\tvalid_1's binary_logloss: 0.206656\tvalid_1's amex_metric gini 0.927 recall 0.684: 0.805561\n",
      "[2100]\ttraining's binary_logloss: 0.187331\ttraining's amex_metric gini 0.943 recall 0.752: 0.847488\tvalid_1's binary_logloss: 0.20593\tvalid_1's amex_metric gini 0.927 recall 0.687: 0.806822\n",
      "[2200]\ttraining's binary_logloss: 0.186105\ttraining's amex_metric gini 0.944 recall 0.755: 0.849666\tvalid_1's binary_logloss: 0.205427\tvalid_1's amex_metric gini 0.927 recall 0.688: 0.807628\n",
      "[2300]\ttraining's binary_logloss: 0.184204\ttraining's amex_metric gini 0.945 recall 0.759: 0.85188\tvalid_1's binary_logloss: 0.2045\tvalid_1's amex_metric gini 0.928 recall 0.687: 0.807391\n",
      "[2400]\ttraining's binary_logloss: 0.183052\ttraining's amex_metric gini 0.946 recall 0.761: 0.853688\tvalid_1's binary_logloss: 0.204027\tvalid_1's amex_metric gini 0.928 recall 0.689: 0.808355\n",
      "[2500]\ttraining's binary_logloss: 0.181821\ttraining's amex_metric gini 0.947 recall 0.765: 0.855691\tvalid_1's binary_logloss: 0.203549\tvalid_1's amex_metric gini 0.928 recall 0.689: 0.808637\n",
      "[2600]\ttraining's binary_logloss: 0.180453\ttraining's amex_metric gini 0.948 recall 0.768: 0.857828\tvalid_1's binary_logloss: 0.20304\tvalid_1's amex_metric gini 0.929 recall 0.689: 0.808744\n",
      "[2700]\ttraining's binary_logloss: 0.178959\ttraining's amex_metric gini 0.948 recall 0.771: 0.859814\tvalid_1's binary_logloss: 0.202501\tvalid_1's amex_metric gini 0.929 recall 0.689: 0.808832\n",
      "[2800]\ttraining's binary_logloss: 0.178097\ttraining's amex_metric gini 0.949 recall 0.774: 0.861596\tvalid_1's binary_logloss: 0.202249\tvalid_1's amex_metric gini 0.929 recall 0.690: 0.809703\n",
      "[2900]\ttraining's binary_logloss: 0.176539\ttraining's amex_metric gini 0.950 recall 0.777: 0.863611\tvalid_1's binary_logloss: 0.201692\tvalid_1's amex_metric gini 0.929 recall 0.690: 0.809401\n",
      "[3000]\ttraining's binary_logloss: 0.174964\ttraining's amex_metric gini 0.951 recall 0.782: 0.866251\tvalid_1's binary_logloss: 0.201193\tvalid_1's amex_metric gini 0.929 recall 0.691: 0.81015\n",
      "[3100]\ttraining's binary_logloss: 0.173792\ttraining's amex_metric gini 0.952 recall 0.785: 0.868207\tvalid_1's binary_logloss: 0.200895\tvalid_1's amex_metric gini 0.929 recall 0.690: 0.809933\n",
      "[3200]\ttraining's binary_logloss: 0.172521\ttraining's amex_metric gini 0.952 recall 0.788: 0.870013\tvalid_1's binary_logloss: 0.200558\tvalid_1's amex_metric gini 0.930 recall 0.693: 0.811435\n",
      "[3300]\ttraining's binary_logloss: 0.171379\ttraining's amex_metric gini 0.953 recall 0.792: 0.872294\tvalid_1's binary_logloss: 0.200269\tvalid_1's amex_metric gini 0.930 recall 0.693: 0.811417\n",
      "[3400]\ttraining's binary_logloss: 0.16983\ttraining's amex_metric gini 0.954 recall 0.794: 0.874094\tvalid_1's binary_logloss: 0.199865\tvalid_1's amex_metric gini 0.930 recall 0.694: 0.811991\n",
      "[3500]\ttraining's binary_logloss: 0.168705\ttraining's amex_metric gini 0.955 recall 0.797: 0.876048\tvalid_1's binary_logloss: 0.199613\tvalid_1's amex_metric gini 0.930 recall 0.693: 0.811726\n",
      "[3600]\ttraining's binary_logloss: 0.167583\ttraining's amex_metric gini 0.955 recall 0.800: 0.877942\tvalid_1's binary_logloss: 0.199398\tvalid_1's amex_metric gini 0.930 recall 0.694: 0.812351\n",
      "[3700]\ttraining's binary_logloss: 0.166016\ttraining's amex_metric gini 0.956 recall 0.805: 0.880359\tvalid_1's binary_logloss: 0.199031\tvalid_1's amex_metric gini 0.930 recall 0.695: 0.812519\n",
      "[3800]\ttraining's binary_logloss: 0.165004\ttraining's amex_metric gini 0.957 recall 0.808: 0.882269\tvalid_1's binary_logloss: 0.198886\tvalid_1's amex_metric gini 0.930 recall 0.694: 0.812473\n",
      "[3900]\ttraining's binary_logloss: 0.164098\ttraining's amex_metric gini 0.958 recall 0.810: 0.88388\tvalid_1's binary_logloss: 0.198783\tvalid_1's amex_metric gini 0.931 recall 0.695: 0.812617\n",
      "[4000]\ttraining's binary_logloss: 0.163218\ttraining's amex_metric gini 0.958 recall 0.812: 0.885381\tvalid_1's binary_logloss: 0.198658\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813273\n",
      "[4100]\ttraining's binary_logloss: 0.16215\ttraining's amex_metric gini 0.959 recall 0.816: 0.887283\tvalid_1's binary_logloss: 0.198475\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813629\n",
      "[4200]\ttraining's binary_logloss: 0.160835\ttraining's amex_metric gini 0.960 recall 0.819: 0.889121\tvalid_1's binary_logloss: 0.198231\tvalid_1's amex_metric gini 0.931 recall 0.697: 0.813891\n",
      "[4300]\ttraining's binary_logloss: 0.159509\ttraining's amex_metric gini 0.960 recall 0.821: 0.890892\tvalid_1's binary_logloss: 0.198029\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813267\n",
      "[4400]\ttraining's binary_logloss: 0.158864\ttraining's amex_metric gini 0.961 recall 0.824: 0.892585\tvalid_1's binary_logloss: 0.198005\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813655\n",
      "[4500]\ttraining's binary_logloss: 0.157824\ttraining's amex_metric gini 0.962 recall 0.827: 0.894472\tvalid_1's binary_logloss: 0.19787\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813498\n",
      "[4600]\ttraining's binary_logloss: 0.156364\ttraining's amex_metric gini 0.962 recall 0.831: 0.896579\tvalid_1's binary_logloss: 0.197663\tvalid_1's amex_metric gini 0.931 recall 0.695: 0.813128\n",
      "[4700]\ttraining's binary_logloss: 0.155542\ttraining's amex_metric gini 0.963 recall 0.833: 0.897946\tvalid_1's binary_logloss: 0.197611\tvalid_1's amex_metric gini 0.931 recall 0.697: 0.814019\n",
      "[4800]\ttraining's binary_logloss: 0.154513\ttraining's amex_metric gini 0.964 recall 0.835: 0.899422\tvalid_1's binary_logloss: 0.197513\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813884\n",
      "[4900]\ttraining's binary_logloss: 0.153318\ttraining's amex_metric gini 0.964 recall 0.839: 0.901451\tvalid_1's binary_logloss: 0.19737\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813916\n",
      "[5000]\ttraining's binary_logloss: 0.152468\ttraining's amex_metric gini 0.965 recall 0.841: 0.90316\tvalid_1's binary_logloss: 0.197337\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813829\n",
      "[5100]\ttraining's binary_logloss: 0.151654\ttraining's amex_metric gini 0.966 recall 0.844: 0.904818\tvalid_1's binary_logloss: 0.197311\tvalid_1's amex_metric gini 0.931 recall 0.697: 0.814249\n",
      "[5200]\ttraining's binary_logloss: 0.150506\ttraining's amex_metric gini 0.966 recall 0.846: 0.906198\tvalid_1's binary_logloss: 0.197176\tvalid_1's amex_metric gini 0.931 recall 0.697: 0.814173\n",
      "[5300]\ttraining's binary_logloss: 0.149871\ttraining's amex_metric gini 0.967 recall 0.849: 0.907685\tvalid_1's binary_logloss: 0.197135\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.814603\n",
      "[5400]\ttraining's binary_logloss: 0.148844\ttraining's amex_metric gini 0.967 recall 0.851: 0.909182\tvalid_1's binary_logloss: 0.197064\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814107\n",
      "[5500]\ttraining's binary_logloss: 0.147741\ttraining's amex_metric gini 0.968 recall 0.854: 0.910914\tvalid_1's binary_logloss: 0.196968\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813928\n",
      "[5600]\ttraining's binary_logloss: 0.146916\ttraining's amex_metric gini 0.969 recall 0.856: 0.912479\tvalid_1's binary_logloss: 0.19694\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813992\n",
      "[5700]\ttraining's binary_logloss: 0.145985\ttraining's amex_metric gini 0.969 recall 0.859: 0.914123\tvalid_1's binary_logloss: 0.196857\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813969\n",
      "[5800]\ttraining's binary_logloss: 0.145286\ttraining's amex_metric gini 0.970 recall 0.862: 0.915715\tvalid_1's binary_logloss: 0.196794\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813993\n",
      "[5900]\ttraining's binary_logloss: 0.144292\ttraining's amex_metric gini 0.970 recall 0.864: 0.917159\tvalid_1's binary_logloss: 0.196722\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813697\n",
      "[6000]\ttraining's binary_logloss: 0.1437\ttraining's amex_metric gini 0.971 recall 0.866: 0.918442\tvalid_1's binary_logloss: 0.196749\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813693\n",
      "[6100]\ttraining's binary_logloss: 0.14291\ttraining's amex_metric gini 0.971 recall 0.868: 0.919693\tvalid_1's binary_logloss: 0.196761\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813887\n",
      "[6200]\ttraining's binary_logloss: 0.142105\ttraining's amex_metric gini 0.972 recall 0.870: 0.920952\tvalid_1's binary_logloss: 0.196696\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813746\n",
      "[6300]\ttraining's binary_logloss: 0.141168\ttraining's amex_metric gini 0.972 recall 0.873: 0.922389\tvalid_1's binary_logloss: 0.196592\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814178\n",
      "[6400]\ttraining's binary_logloss: 0.14041\ttraining's amex_metric gini 0.973 recall 0.875: 0.923694\tvalid_1's binary_logloss: 0.196545\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.81404\n",
      "[6500]\ttraining's binary_logloss: 0.139416\ttraining's amex_metric gini 0.973 recall 0.878: 0.925287\tvalid_1's binary_logloss: 0.196482\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814358\n",
      "[6600]\ttraining's binary_logloss: 0.138329\ttraining's amex_metric gini 0.974 recall 0.880: 0.9266\tvalid_1's binary_logloss: 0.196403\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814326\n",
      "[6700]\ttraining's binary_logloss: 0.137349\ttraining's amex_metric gini 0.974 recall 0.882: 0.928184\tvalid_1's binary_logloss: 0.196311\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813795\n",
      "[6800]\ttraining's binary_logloss: 0.136278\ttraining's amex_metric gini 0.975 recall 0.885: 0.929704\tvalid_1's binary_logloss: 0.196285\tvalid_1's amex_metric gini 0.932 recall 0.695: 0.813693\n",
      "[6900]\ttraining's binary_logloss: 0.135398\ttraining's amex_metric gini 0.975 recall 0.887: 0.931257\tvalid_1's binary_logloss: 0.196261\tvalid_1's amex_metric gini 0.932 recall 0.695: 0.81365\n",
      "[7000]\ttraining's binary_logloss: 0.134489\ttraining's amex_metric gini 0.976 recall 0.889: 0.932541\tvalid_1's binary_logloss: 0.196256\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.8138\n",
      "[7100]\ttraining's binary_logloss: 0.133419\ttraining's amex_metric gini 0.976 recall 0.892: 0.934224\tvalid_1's binary_logloss: 0.196192\tvalid_1's amex_metric gini 0.932 recall 0.695: 0.813566\n",
      "[7200]\ttraining's binary_logloss: 0.132464\ttraining's amex_metric gini 0.977 recall 0.894: 0.935466\tvalid_1's binary_logloss: 0.1961\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.814103\n",
      "[7300]\ttraining's binary_logloss: 0.131603\ttraining's amex_metric gini 0.977 recall 0.897: 0.936967\tvalid_1's binary_logloss: 0.196038\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814382\n",
      "[7400]\ttraining's binary_logloss: 0.130749\ttraining's amex_metric gini 0.978 recall 0.898: 0.938134\tvalid_1's binary_logloss: 0.196009\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814443\n",
      "[7500]\ttraining's binary_logloss: 0.129822\ttraining's amex_metric gini 0.978 recall 0.901: 0.939749\tvalid_1's binary_logloss: 0.195982\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.8144\n",
      "[7600]\ttraining's binary_logloss: 0.129026\ttraining's amex_metric gini 0.979 recall 0.903: 0.940951\tvalid_1's binary_logloss: 0.195907\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814679\n",
      "[7700]\ttraining's binary_logloss: 0.128354\ttraining's amex_metric gini 0.979 recall 0.905: 0.942205\tvalid_1's binary_logloss: 0.1959\tvalid_1's amex_metric gini 0.932 recall 0.699: 0.815391\n",
      "[7800]\ttraining's binary_logloss: 0.127573\ttraining's amex_metric gini 0.980 recall 0.907: 0.943461\tvalid_1's binary_logloss: 0.19587\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815244\n",
      "[7900]\ttraining's binary_logloss: 0.126702\ttraining's amex_metric gini 0.980 recall 0.909: 0.944639\tvalid_1's binary_logloss: 0.195888\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814626\n",
      "[8000]\ttraining's binary_logloss: 0.125672\ttraining's amex_metric gini 0.981 recall 0.911: 0.94601\tvalid_1's binary_logloss: 0.195867\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.814882\n",
      "[8100]\ttraining's binary_logloss: 0.124961\ttraining's amex_metric gini 0.981 recall 0.913: 0.947205\tvalid_1's binary_logloss: 0.195846\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.81469\n",
      "[8200]\ttraining's binary_logloss: 0.124027\ttraining's amex_metric gini 0.981 recall 0.915: 0.948331\tvalid_1's binary_logloss: 0.195852\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814631\n",
      "[8300]\ttraining's binary_logloss: 0.123368\ttraining's amex_metric gini 0.982 recall 0.917: 0.94955\tvalid_1's binary_logloss: 0.195848\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.81433\n",
      "[8400]\ttraining's binary_logloss: 0.122757\ttraining's amex_metric gini 0.982 recall 0.919: 0.950578\tvalid_1's binary_logloss: 0.19581\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814396\n",
      "[8500]\ttraining's binary_logloss: 0.122117\ttraining's amex_metric gini 0.983 recall 0.921: 0.951685\tvalid_1's binary_logloss: 0.195821\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814748\n",
      "[8600]\ttraining's binary_logloss: 0.121356\ttraining's amex_metric gini 0.983 recall 0.922: 0.95253\tvalid_1's binary_logloss: 0.19579\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814756\n",
      "[8700]\ttraining's binary_logloss: 0.120326\ttraining's amex_metric gini 0.983 recall 0.924: 0.953651\tvalid_1's binary_logloss: 0.195773\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814754\n",
      "[8800]\ttraining's binary_logloss: 0.119638\ttraining's amex_metric gini 0.984 recall 0.926: 0.954753\tvalid_1's binary_logloss: 0.195732\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.81528\n",
      "[8900]\ttraining's binary_logloss: 0.118918\ttraining's amex_metric gini 0.984 recall 0.927: 0.955741\tvalid_1's binary_logloss: 0.195718\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814725\n",
      "[9000]\ttraining's binary_logloss: 0.118137\ttraining's amex_metric gini 0.984 recall 0.929: 0.956744\tvalid_1's binary_logloss: 0.195725\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815128\n",
      "[9100]\ttraining's binary_logloss: 0.117544\ttraining's amex_metric gini 0.985 recall 0.931: 0.957681\tvalid_1's binary_logloss: 0.195737\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815224\n",
      "[9200]\ttraining's binary_logloss: 0.116586\ttraining's amex_metric gini 0.985 recall 0.932: 0.958784\tvalid_1's binary_logloss: 0.195684\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815241\n",
      "[9300]\ttraining's binary_logloss: 0.115853\ttraining's amex_metric gini 0.986 recall 0.934: 0.959822\tvalid_1's binary_logloss: 0.195707\tvalid_1's amex_metric gini 0.932 recall 0.699: 0.815435\n",
      "[9400]\ttraining's binary_logloss: 0.115432\ttraining's amex_metric gini 0.986 recall 0.935: 0.960517\tvalid_1's binary_logloss: 0.195704\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815035\n",
      "[9500]\ttraining's binary_logloss: 0.114533\ttraining's amex_metric gini 0.986 recall 0.937: 0.961495\tvalid_1's binary_logloss: 0.195694\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814682\n",
      "[9600]\ttraining's binary_logloss: 0.113801\ttraining's amex_metric gini 0.987 recall 0.938: 0.962383\tvalid_1's binary_logloss: 0.195673\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815198\n",
      "[9700]\ttraining's binary_logloss: 0.113068\ttraining's amex_metric gini 0.987 recall 0.940: 0.963187\tvalid_1's binary_logloss: 0.195663\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814847\n",
      "[9800]\ttraining's binary_logloss: 0.112332\ttraining's amex_metric gini 0.987 recall 0.941: 0.964128\tvalid_1's binary_logloss: 0.195693\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815091\n",
      "[9900]\ttraining's binary_logloss: 0.111635\ttraining's amex_metric gini 0.987 recall 0.943: 0.965088\tvalid_1's binary_logloss: 0.195677\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815096\n",
      "[10000]\ttraining's binary_logloss: 0.111035\ttraining's amex_metric gini 0.988 recall 0.944: 0.965822\tvalid_1's binary_logloss: 0.195639\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814805\n",
      "[10100]\ttraining's binary_logloss: 0.110667\ttraining's amex_metric gini 0.988 recall 0.945: 0.966596\tvalid_1's binary_logloss: 0.195666\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.814391\n",
      "[10200]\ttraining's binary_logloss: 0.110053\ttraining's amex_metric gini 0.988 recall 0.946: 0.967055\tvalid_1's binary_logloss: 0.195645\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815258\n",
      "[10300]\ttraining's binary_logloss: 0.109334\ttraining's amex_metric gini 0.989 recall 0.947: 0.967817\tvalid_1's binary_logloss: 0.195633\tvalid_1's amex_metric gini 0.932 recall 0.699: 0.815412\n",
      "[10400]\ttraining's binary_logloss: 0.108713\ttraining's amex_metric gini 0.989 recall 0.948: 0.968546\tvalid_1's binary_logloss: 0.195641\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.814194\n",
      "[10500]\ttraining's binary_logloss: 0.107965\ttraining's amex_metric gini 0.989 recall 0.950: 0.969371\tvalid_1's binary_logloss: 0.195627\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814504\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "lgb_valid = lgb.Dataset(X_test.reshape(X_test.shape[0], -1), y_test,)\n",
    "model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/nimamanaf/Desktop/kaggle/pd/data/out/lgbm13.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "joblib.dump(model, OUTDIR+f'lgbm13.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train, test=None, n_folds=5, seed=42):\n",
    "    cat_features = [f\"{cf}_last\" for cf in CATCOLS]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        if test is not None:\n",
    "            test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        if test is not None:\n",
    "            test[col + '_round2'] = test[col].round(2)\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', \"S_2\", \"target\"]]\n",
    "    \n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[\"target\"])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[\"target\"].iloc[trn_ind], train[\"target\"].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        # Save best model\n",
    "        joblib.dump(model, OUTDIR+f'Models/lgbm_fold{fold}_seed{seed}.pkl')\n",
    "        val_pred = model.predict(x_val) # Predict validation\n",
    "        oof_predictions[val_ind] = val_pred  # Add to out of folds array\n",
    "        if test is not None:\n",
    "            test_pred = model.predict(test[features]) # Predict the test set\n",
    "            test_predictions += test_pred/n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    score = amex_metric(train[\"target\"], oof_predictions)  # Compute out of folds metric\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[\"target\"], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(OUTDIR+f'oof_lgbm_baseline_{n_folds}fold_seed{seed}.csv', index=False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    if test is not None:\n",
    "        test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "        test_df.to_csv(f'/content/drive/MyDrive/Amex/Predictions/test_lgbm_baseline_{n_folds}fold_seed{seed}.csv', index = False)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(OUTDIR+f'lgbm13.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load(OUTDIR+\"test_raw_all_mean_q1_q99_data.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_data.reshape(test_data.shape[0], -1)) # Predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(OUTDIR+'test_raw_all_mean_q1_q99_id.json', 'r') as f:\n",
    "            test_id_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({\"customer_ID\":test_id_dict.values(), \n",
    "                        \"prediction\":test_pred.reshape(-1)\n",
    "                        }\n",
    "                        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "sub_file_dir = os.path.join(OUTDIR, \"lgbm_med_sub.csv\")\n",
    "result.set_index(\"customer_ID\").to_csv(sub_file_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('q')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05e50049e3eb32775174019135b7208a0d3852fb22829b3658213f387a3fdcbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
