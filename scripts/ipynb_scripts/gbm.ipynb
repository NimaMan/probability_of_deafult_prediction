{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "\n",
    "import random\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "from pd.params import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data_type=\"train\"):\n",
    "    \n",
    "    if data_type == \"train\":\n",
    "        data = pd.read_parquet(OUTDIR+\"train_data.parquet\")\n",
    "        train_labels = pd.read_csv(DATADIR+\"train_labels.csv\")\n",
    "    else:\n",
    "        data = pd.read_parquet(OUTDIR+\"test_data.parquet\")\n",
    "        train_labels = None\n",
    "    \n",
    "    print('Starting feature engineer...')\n",
    "    \n",
    "    data_cont_agg = data.groupby(\"customer_ID\")[ContCols].agg(['mean', 'std', 'min', 'max', 'last'])\n",
    "    data_cont_agg.columns = ['_'.join(x) for x in data_cont_agg.columns]\n",
    "    data_cont_agg.reset_index(inplace=True)\n",
    "\n",
    "    data_cat_agg = data.groupby(\"customer_ID\")[CATCOLS].agg(['count', 'last', 'nunique'])\n",
    "    data_cat_agg.columns = ['_'.join(x) for x in data_cat_agg.columns]\n",
    "    data_cat_agg.reset_index(inplace=True)\n",
    "    data = data_cont_agg.merge(data_cat_agg, how='inner', on='customer_ID')\n",
    "    \n",
    "    if train_labels is None:\n",
    "        data = data_cont_agg.merge(train_labels, how='inner', on='customer_ID')\n",
    "    \n",
    "    del data_cont_agg, data_cont_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    data.to_parquet(OUTDIR+f\"{data_type}_fe.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    return 'amex_metric', amex_metric(y_true, y_pred), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': 42,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 4,\n",
    "        'min_data_in_leaf': 40\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(OUTDIR+\"train_data_all.npy\").transpose((0, 2, 1))\n",
    "train_labels = np.load(OUTDIR+\"train_labels_all.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(OUTDIR+\"c13_data.npy\")\n",
    "train_labels = np.load(OUTDIR+\"c13_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:, :, 1].reshape(X_train.shape[0], 13, -1).shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=1/9, random_state=0, shuffle=True)\n",
    "validation_data = (X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 79538, number of negative: 263603\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.031721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 328485\n",
      "[LightGBM] [Info] Number of data points in the train set: 343141, number of used features: 2424\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.231794 -> initscore=-1.198209\n",
      "[LightGBM] [Info] Start training from score -1.198209\n",
      "[100]\ttraining's binary_logloss: 0.447318\ttraining's amex_metric: 0.78539\tvalid_1's binary_logloss: 0.449321\tvalid_1's amex_metric: 0.76945\n",
      "[200]\ttraining's binary_logloss: 0.430475\ttraining's amex_metric: 0.790134\tvalid_1's binary_logloss: 0.43309\tvalid_1's amex_metric: 0.772277\n",
      "[300]\ttraining's binary_logloss: 0.37635\ttraining's amex_metric: 0.794047\tvalid_1's binary_logloss: 0.380053\tvalid_1's amex_metric: 0.77517\n",
      "[400]\ttraining's binary_logloss: 0.350919\ttraining's amex_metric: 0.797656\tvalid_1's binary_logloss: 0.355269\tvalid_1's amex_metric: 0.777454\n",
      "[500]\ttraining's binary_logloss: 0.31934\ttraining's amex_metric: 0.800607\tvalid_1's binary_logloss: 0.324598\tvalid_1's amex_metric: 0.779231\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "lgb_valid = lgb.Dataset(X_test.reshape(X_test.shape[0], -1), y_test,)\n",
    "model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train, test=None, n_folds=5, seed=42):\n",
    "    cat_features = [f\"{cf}_last\" for cf in CATCOLS]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        if test is not None:\n",
    "            test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        if test is not None:\n",
    "            test[col + '_round2'] = test[col].round(2)\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', \"S_2\", \"target\"]]\n",
    "    \n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[\"target\"])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[\"target\"].iloc[trn_ind], train[\"target\"].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        # Save best model\n",
    "        joblib.dump(model, OUTDIR+f'Models/lgbm_fold{fold}_seed{seed}.pkl')\n",
    "        val_pred = model.predict(x_val) # Predict validation\n",
    "        oof_predictions[val_ind] = val_pred  # Add to out of folds array\n",
    "        if test is not None:\n",
    "            test_pred = model.predict(test[features]) # Predict the test set\n",
    "            test_predictions += test_pred/n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    score = amex_metric(train[\"target\"], oof_predictions)  # Compute out of folds metric\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[\"target\"], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(OUTDIR+f'oof_lgbm_baseline_{n_folds}fold_seed{seed}.csv', index=False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    if test is not None:\n",
    "        test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "        test_df.to_csv(f'/content/drive/MyDrive/Amex/Predictions/test_lgbm_baseline_{n_folds}fold_seed{seed}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm_single_feature(train, feature, n_folds=5, seed=42):\n",
    "    cat_feature = \"auto\"\n",
    "    if feature in CATCOLS:\n",
    "        cat_feature = feature\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[\"target\"])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} of {feature} feature...')\n",
    "        x_train, x_val = train[feature].iloc[trn_ind], train[feature].iloc[val_ind]\n",
    "        y_train, y_val = train[\"target\"].iloc[trn_ind], train[\"target\"].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_feature)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_feature)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        # Save best model\n",
    "        joblib.dump(model, OUTDIR+f'Models/lgbm_fold{fold}_seed{seed}.pkl')\n",
    "        val_pred = model.predict(x_val) # Predict validation\n",
    "        oof_predictions[val_ind] = val_pred  # Add to out of folds array\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    score = amex_metric(train[\"target\"], oof_predictions)  # Compute out of folds metric\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[\"target\"], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(OUTDIR+f'oof_lgbm_baseline_{n_folds}fold_seed{seed}.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('p8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25d4fc9e51127fc9597103904eccbdaaf4ac71170a764a4234e11ed19cb4831a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
