{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "\n",
    "import random\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "from pd.gmb_utils import lgb_amex_metric\n",
    "from pd.params import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pd.data.preprop import get_feat_comb\n",
    "get_feat_comb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agg_data(data_dir=\"train_agg_mean_q5_q95_q5_q95.npz\"):\n",
    "    d = np.load(OUTDIR+data_dir)\n",
    "    #train_data = np.concatenate((d[\"d2\"].astype(np.int32), d[\"d1\"].reshape(d[\"d1\"].shape[0], -1)), axis=1)\n",
    "    train_labels = d[\"labels\"]\n",
    "    df2 = pd.DataFrame(d[\"d2\"].astype(np.int32))\n",
    "    df = pd.DataFrame(d[\"d1\"].reshape(d[\"d1\"].shape[0], -1))\n",
    "    df = pd.concat((df2, df), axis=1,)\n",
    "    df.columns = [f\"c{i}\" for i in range(df.shape[1])]\n",
    "    cat_indices = list(np.arange(d[\"d2\"].shape[1]))\n",
    "\n",
    "    return df, train_labels, cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': 42,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 4,\n",
    "        'min_data_in_leaf': 40\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = np.load(OUTDIR+\"train_logistic_raw_all_mean_q5_q95_q5_q95_data.npy\")\n",
    "#train_labels = np.load(OUTDIR+\"train_logistic_raw_all_mean_q5_q95_q5_q95_labels.npy\")\n",
    "#lgb_train = lgb.Dataset(X_train.reshape(X_train.shape[0], -1), y_train, categorical_feature=cat_indices)\n",
    "#lgb_valid = lgb.Dataset(X_test.reshape(X_test.shape[0], -1), y_test, categorical_feature=cat_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, train_labels, test_size=1/9, random_state=0, shuffle=True)\n",
    "validation_data = (X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train,)\n",
    "lgb_valid = lgb.Dataset(X_test, y_test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 105712, number of negative: 302210\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.903616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 324251\n",
      "[LightGBM] [Info] Number of data points in the train set: 407922, number of used features: 2322\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.259148 -> initscore=-1.050404\n",
      "[LightGBM] [Info] Start training from score -1.050404\n",
      "[100]\ttraining's binary_logloss: 0.471143\ttraining's amex_metric gini 0.905 recall 0.607: 0.755657\tvalid_1's binary_logloss: 0.470878\tvalid_1's amex_metric gini 0.904 recall 0.606: 0.754691\n",
      "[200]\ttraining's binary_logloss: 0.451443\ttraining's amex_metric gini 0.906 recall 0.613: 0.759837\tvalid_1's binary_logloss: 0.45151\tvalid_1's amex_metric gini 0.905 recall 0.609: 0.756774\n",
      "[300]\ttraining's binary_logloss: 0.398248\ttraining's amex_metric gini 0.908 recall 0.619: 0.763336\tvalid_1's binary_logloss: 0.398552\tvalid_1's amex_metric gini 0.906 recall 0.610: 0.758086\n",
      "[400]\ttraining's binary_logloss: 0.372951\ttraining's amex_metric gini 0.909 recall 0.622: 0.765453\tvalid_1's binary_logloss: 0.373519\tvalid_1's amex_metric gini 0.907 recall 0.615: 0.761224\n",
      "[500]\ttraining's binary_logloss: 0.341941\ttraining's amex_metric gini 0.910 recall 0.627: 0.76873\tvalid_1's binary_logloss: 0.342806\tvalid_1's amex_metric gini 0.908 recall 0.616: 0.761932\n",
      "[600]\ttraining's binary_logloss: 0.323463\ttraining's amex_metric gini 0.912 recall 0.632: 0.771637\tvalid_1's binary_logloss: 0.324632\tvalid_1's amex_metric gini 0.909 recall 0.621: 0.764911\n",
      "[700]\ttraining's binary_logloss: 0.294015\ttraining's amex_metric gini 0.913 recall 0.637: 0.775053\tvalid_1's binary_logloss: 0.295797\tvalid_1's amex_metric gini 0.910 recall 0.624: 0.767349\n",
      "[800]\ttraining's binary_logloss: 0.273384\ttraining's amex_metric gini 0.915 recall 0.643: 0.779057\tvalid_1's binary_logloss: 0.27581\tvalid_1's amex_metric gini 0.912 recall 0.631: 0.771252\n",
      "[900]\ttraining's binary_logloss: 0.258297\ttraining's amex_metric gini 0.917 recall 0.649: 0.783345\tvalid_1's binary_logloss: 0.261446\tvalid_1's amex_metric gini 0.913 recall 0.631: 0.771997\n",
      "[1000]\ttraining's binary_logloss: 0.251761\ttraining's amex_metric gini 0.919 recall 0.654: 0.786502\tvalid_1's binary_logloss: 0.255426\tvalid_1's amex_metric gini 0.914 recall 0.636: 0.774941\n",
      "[1100]\ttraining's binary_logloss: 0.241858\ttraining's amex_metric gini 0.920 recall 0.660: 0.790177\tvalid_1's binary_logloss: 0.246414\tvalid_1's amex_metric gini 0.915 recall 0.638: 0.776355\n",
      "[1200]\ttraining's binary_logloss: 0.238587\ttraining's amex_metric gini 0.922 recall 0.664: 0.79278\tvalid_1's binary_logloss: 0.243717\tvalid_1's amex_metric gini 0.916 recall 0.639: 0.7773\n",
      "[1300]\ttraining's binary_logloss: 0.234663\ttraining's amex_metric gini 0.923 recall 0.668: 0.795402\tvalid_1's binary_logloss: 0.240437\tvalid_1's amex_metric gini 0.916 recall 0.642: 0.779279\n",
      "[1400]\ttraining's binary_logloss: 0.230057\ttraining's amex_metric gini 0.924 recall 0.672: 0.798051\tvalid_1's binary_logloss: 0.236542\tvalid_1's amex_metric gini 0.917 recall 0.643: 0.78005\n",
      "[1500]\ttraining's binary_logloss: 0.227861\ttraining's amex_metric gini 0.925 recall 0.676: 0.800338\tvalid_1's binary_logloss: 0.23494\tvalid_1's amex_metric gini 0.918 recall 0.643: 0.780236\n",
      "[1600]\ttraining's binary_logloss: 0.222648\ttraining's amex_metric gini 0.927 recall 0.680: 0.803313\tvalid_1's binary_logloss: 0.230828\tvalid_1's amex_metric gini 0.918 recall 0.647: 0.782743\n",
      "[1700]\ttraining's binary_logloss: 0.220179\ttraining's amex_metric gini 0.928 recall 0.684: 0.80609\tvalid_1's binary_logloss: 0.229173\tvalid_1's amex_metric gini 0.919 recall 0.650: 0.784456\n",
      "[1800]\ttraining's binary_logloss: 0.218851\ttraining's amex_metric gini 0.929 recall 0.688: 0.808361\tvalid_1's binary_logloss: 0.228465\tvalid_1's amex_metric gini 0.919 recall 0.654: 0.78651\n",
      "[1900]\ttraining's binary_logloss: 0.21664\ttraining's amex_metric gini 0.930 recall 0.691: 0.810595\tvalid_1's binary_logloss: 0.227031\tvalid_1's amex_metric gini 0.920 recall 0.656: 0.787703\n",
      "[2000]\ttraining's binary_logloss: 0.214155\ttraining's amex_metric gini 0.931 recall 0.696: 0.813413\tvalid_1's binary_logloss: 0.225478\tvalid_1's amex_metric gini 0.920 recall 0.659: 0.789475\n",
      "[2100]\ttraining's binary_logloss: 0.212734\ttraining's amex_metric gini 0.932 recall 0.699: 0.815653\tvalid_1's binary_logloss: 0.224771\tvalid_1's amex_metric gini 0.921 recall 0.660: 0.790111\n",
      "[2200]\ttraining's binary_logloss: 0.211554\ttraining's amex_metric gini 0.933 recall 0.702: 0.81739\tvalid_1's binary_logloss: 0.224247\tvalid_1's amex_metric gini 0.921 recall 0.661: 0.790741\n",
      "[2300]\ttraining's binary_logloss: 0.209743\ttraining's amex_metric gini 0.934 recall 0.705: 0.819373\tvalid_1's binary_logloss: 0.223363\tvalid_1's amex_metric gini 0.921 recall 0.662: 0.791361\n",
      "[2400]\ttraining's binary_logloss: 0.20865\ttraining's amex_metric gini 0.935 recall 0.708: 0.821346\tvalid_1's binary_logloss: 0.222989\tvalid_1's amex_metric gini 0.921 recall 0.661: 0.790979\n",
      "[2500]\ttraining's binary_logloss: 0.207469\ttraining's amex_metric gini 0.935 recall 0.711: 0.823237\tvalid_1's binary_logloss: 0.222512\tvalid_1's amex_metric gini 0.922 recall 0.662: 0.792025\n",
      "[2600]\ttraining's binary_logloss: 0.206144\ttraining's amex_metric gini 0.936 recall 0.713: 0.82468\tvalid_1's binary_logloss: 0.221925\tvalid_1's amex_metric gini 0.922 recall 0.662: 0.792049\n",
      "[2700]\ttraining's binary_logloss: 0.204613\ttraining's amex_metric gini 0.937 recall 0.716: 0.826626\tvalid_1's binary_logloss: 0.221244\tvalid_1's amex_metric gini 0.922 recall 0.662: 0.792097\n",
      "[2800]\ttraining's binary_logloss: 0.203844\ttraining's amex_metric gini 0.938 recall 0.719: 0.828248\tvalid_1's binary_logloss: 0.221096\tvalid_1's amex_metric gini 0.922 recall 0.662: 0.792071\n",
      "[2900]\ttraining's binary_logloss: 0.202317\ttraining's amex_metric gini 0.939 recall 0.721: 0.829988\tvalid_1's binary_logloss: 0.220513\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793072\n",
      "[3000]\ttraining's binary_logloss: 0.200797\ttraining's amex_metric gini 0.939 recall 0.725: 0.832174\tvalid_1's binary_logloss: 0.220001\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793137\n",
      "[3100]\ttraining's binary_logloss: 0.199673\ttraining's amex_metric gini 0.940 recall 0.728: 0.834211\tvalid_1's binary_logloss: 0.219684\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793193\n",
      "[3200]\ttraining's binary_logloss: 0.198451\ttraining's amex_metric gini 0.941 recall 0.731: 0.836146\tvalid_1's binary_logloss: 0.219326\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793308\n",
      "[3300]\ttraining's binary_logloss: 0.197346\ttraining's amex_metric gini 0.942 recall 0.735: 0.838119\tvalid_1's binary_logloss: 0.219055\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793493\n",
      "[3400]\ttraining's binary_logloss: 0.195853\ttraining's amex_metric gini 0.943 recall 0.737: 0.839864\tvalid_1's binary_logloss: 0.2186\tvalid_1's amex_metric gini 0.923 recall 0.665: 0.79403\n",
      "[3500]\ttraining's binary_logloss: 0.194768\ttraining's amex_metric gini 0.943 recall 0.740: 0.841785\tvalid_1's binary_logloss: 0.218348\tvalid_1's amex_metric gini 0.923 recall 0.664: 0.793537\n",
      "[3600]\ttraining's binary_logloss: 0.19367\ttraining's amex_metric gini 0.944 recall 0.743: 0.843582\tvalid_1's binary_logloss: 0.218079\tvalid_1's amex_metric gini 0.924 recall 0.664: 0.794029\n",
      "[3700]\ttraining's binary_logloss: 0.192163\ttraining's amex_metric gini 0.945 recall 0.746: 0.845279\tvalid_1's binary_logloss: 0.217675\tvalid_1's amex_metric gini 0.924 recall 0.665: 0.794277\n",
      "[3800]\ttraining's binary_logloss: 0.191192\ttraining's amex_metric gini 0.946 recall 0.749: 0.847442\tvalid_1's binary_logloss: 0.217565\tvalid_1's amex_metric gini 0.924 recall 0.667: 0.795235\n",
      "[3900]\ttraining's binary_logloss: 0.190317\ttraining's amex_metric gini 0.946 recall 0.752: 0.848955\tvalid_1's binary_logloss: 0.217519\tvalid_1's amex_metric gini 0.924 recall 0.666: 0.794912\n",
      "[4000]\ttraining's binary_logloss: 0.189469\ttraining's amex_metric gini 0.947 recall 0.754: 0.850361\tvalid_1's binary_logloss: 0.217391\tvalid_1's amex_metric gini 0.924 recall 0.666: 0.795032\n",
      "[4100]\ttraining's binary_logloss: 0.188473\ttraining's amex_metric gini 0.948 recall 0.757: 0.852298\tvalid_1's binary_logloss: 0.21723\tvalid_1's amex_metric gini 0.924 recall 0.665: 0.79465\n",
      "[4200]\ttraining's binary_logloss: 0.187224\ttraining's amex_metric gini 0.948 recall 0.760: 0.854059\tvalid_1's binary_logloss: 0.216984\tvalid_1's amex_metric gini 0.924 recall 0.667: 0.795381\n",
      "[4300]\ttraining's binary_logloss: 0.185973\ttraining's amex_metric gini 0.949 recall 0.763: 0.855978\tvalid_1's binary_logloss: 0.21674\tvalid_1's amex_metric gini 0.924 recall 0.666: 0.795078\n",
      "[4400]\ttraining's binary_logloss: 0.185381\ttraining's amex_metric gini 0.950 recall 0.765: 0.85739\tvalid_1's binary_logloss: 0.21676\tvalid_1's amex_metric gini 0.924 recall 0.664: 0.794261\n",
      "[4500]\ttraining's binary_logloss: 0.184383\ttraining's amex_metric gini 0.950 recall 0.768: 0.85911\tvalid_1's binary_logloss: 0.216648\tvalid_1's amex_metric gini 0.924 recall 0.664: 0.794359\n",
      "[4600]\ttraining's binary_logloss: 0.182998\ttraining's amex_metric gini 0.951 recall 0.771: 0.861081\tvalid_1's binary_logloss: 0.216394\tvalid_1's amex_metric gini 0.924 recall 0.666: 0.795124\n",
      "[4700]\ttraining's binary_logloss: 0.182213\ttraining's amex_metric gini 0.952 recall 0.774: 0.862768\tvalid_1's binary_logloss: 0.216323\tvalid_1's amex_metric gini 0.924 recall 0.667: 0.795543\n",
      "[4800]\ttraining's binary_logloss: 0.181245\ttraining's amex_metric gini 0.952 recall 0.777: 0.864594\tvalid_1's binary_logloss: 0.216255\tvalid_1's amex_metric gini 0.924 recall 0.667: 0.795744\n",
      "[4900]\ttraining's binary_logloss: 0.180114\ttraining's amex_metric gini 0.953 recall 0.780: 0.866563\tvalid_1's binary_logloss: 0.216129\tvalid_1's amex_metric gini 0.924 recall 0.666: 0.79546\n",
      "[5000]\ttraining's binary_logloss: 0.179312\ttraining's amex_metric gini 0.954 recall 0.783: 0.868176\tvalid_1's binary_logloss: 0.216061\tvalid_1's amex_metric gini 0.925 recall 0.666: 0.795451\n",
      "[5100]\ttraining's binary_logloss: 0.178532\ttraining's amex_metric gini 0.954 recall 0.785: 0.869636\tvalid_1's binary_logloss: 0.216028\tvalid_1's amex_metric gini 0.925 recall 0.667: 0.795619\n",
      "[5200]\ttraining's binary_logloss: 0.177433\ttraining's amex_metric gini 0.955 recall 0.787: 0.871149\tvalid_1's binary_logloss: 0.215859\tvalid_1's amex_metric gini 0.925 recall 0.667: 0.795958\n",
      "[5300]\ttraining's binary_logloss: 0.176838\ttraining's amex_metric gini 0.956 recall 0.789: 0.872439\tvalid_1's binary_logloss: 0.215876\tvalid_1's amex_metric gini 0.925 recall 0.668: 0.796194\n",
      "[5400]\ttraining's binary_logloss: 0.175856\ttraining's amex_metric gini 0.956 recall 0.791: 0.873839\tvalid_1's binary_logloss: 0.21575\tvalid_1's amex_metric gini 0.925 recall 0.668: 0.796447\n",
      "[5500]\ttraining's binary_logloss: 0.174808\ttraining's amex_metric gini 0.957 recall 0.794: 0.875603\tvalid_1's binary_logloss: 0.215595\tvalid_1's amex_metric gini 0.925 recall 0.669: 0.796751\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 5500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric, \n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7fb45f406c40>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_model(OUTDIR+\"lgbm_agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': 42,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.001,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 8,\n",
    "        'min_data_in_leaf': 40\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 105541, number of negative: 302381\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.270654 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 329344\n",
      "[LightGBM] [Info] Number of data points in the train set: 407922, number of used features: 2443\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258728 -> initscore=-1.052588\n",
      "[LightGBM] [Info] Start training from score -1.052588\n",
      "[100]\ttraining's binary_logloss: 0.593543\ttraining's amex_metric gini 0.900 recall 0.592: 0.746145\tvalid_1's binary_logloss: 0.594411\tvalid_1's amex_metric gini 0.897 recall 0.583: 0.740103\n",
      "[200]\ttraining's binary_logloss: 0.609187\ttraining's amex_metric gini 0.901 recall 0.594: 0.74746\tvalid_1's binary_logloss: 0.609827\tvalid_1's amex_metric gini 0.898 recall 0.586: 0.742201\n",
      "[300]\ttraining's binary_logloss: 0.600042\ttraining's amex_metric gini 0.901 recall 0.597: 0.749256\tvalid_1's binary_logloss: 0.60072\tvalid_1's amex_metric gini 0.899 recall 0.589: 0.743876\n",
      "[400]\ttraining's binary_logloss: 0.596775\ttraining's amex_metric gini 0.902 recall 0.597: 0.749559\tvalid_1's binary_logloss: 0.597456\tvalid_1's amex_metric gini 0.899 recall 0.589: 0.743938\n",
      "[500]\ttraining's binary_logloss: 0.589401\ttraining's amex_metric gini 0.902 recall 0.598: 0.750268\tvalid_1's binary_logloss: 0.590118\tvalid_1's amex_metric gini 0.899 recall 0.591: 0.745198\n",
      "[600]\ttraining's binary_logloss: 0.583846\ttraining's amex_metric gini 0.902 recall 0.599: 0.750302\tvalid_1's binary_logloss: 0.584588\tvalid_1's amex_metric gini 0.899 recall 0.591: 0.744914\n",
      "[700]\ttraining's binary_logloss: 0.570204\ttraining's amex_metric gini 0.902 recall 0.601: 0.751439\tvalid_1's binary_logloss: 0.571037\tvalid_1's amex_metric gini 0.899 recall 0.590: 0.744887\n",
      "[800]\ttraining's binary_logloss: 0.55603\ttraining's amex_metric gini 0.903 recall 0.602: 0.752412\tvalid_1's binary_logloss: 0.556945\tvalid_1's amex_metric gini 0.900 recall 0.593: 0.746485\n",
      "[900]\ttraining's binary_logloss: 0.54153\ttraining's amex_metric gini 0.903 recall 0.603: 0.75318\tvalid_1's binary_logloss: 0.542544\tvalid_1's amex_metric gini 0.900 recall 0.593: 0.746656\n",
      "[1000]\ttraining's binary_logloss: 0.534161\ttraining's amex_metric gini 0.903 recall 0.605: 0.754302\tvalid_1's binary_logloss: 0.53523\tvalid_1's amex_metric gini 0.901 recall 0.596: 0.748063\n",
      "[1100]\ttraining's binary_logloss: 0.519976\ttraining's amex_metric gini 0.904 recall 0.607: 0.755376\tvalid_1's binary_logloss: 0.52116\tvalid_1's amex_metric gini 0.901 recall 0.597: 0.748874\n",
      "[1200]\ttraining's binary_logloss: 0.514494\ttraining's amex_metric gini 0.904 recall 0.608: 0.756063\tvalid_1's binary_logloss: 0.515725\tvalid_1's amex_metric gini 0.901 recall 0.597: 0.749219\n",
      "[1300]\ttraining's binary_logloss: 0.506961\ttraining's amex_metric gini 0.904 recall 0.609: 0.756603\tvalid_1's binary_logloss: 0.508253\tvalid_1's amex_metric gini 0.901 recall 0.599: 0.749914\n",
      "[1400]\ttraining's binary_logloss: 0.496632\ttraining's amex_metric gini 0.905 recall 0.609: 0.756966\tvalid_1's binary_logloss: 0.498018\tvalid_1's amex_metric gini 0.901 recall 0.598: 0.749625\n",
      "[1500]\ttraining's binary_logloss: 0.490092\ttraining's amex_metric gini 0.905 recall 0.610: 0.757553\tvalid_1's binary_logloss: 0.491534\tvalid_1's amex_metric gini 0.902 recall 0.599: 0.75016\n",
      "[1600]\ttraining's binary_logloss: 0.47626\ttraining's amex_metric gini 0.905 recall 0.612: 0.7585\tvalid_1's binary_logloss: 0.477828\tvalid_1's amex_metric gini 0.902 recall 0.598: 0.750178\n",
      "[1700]\ttraining's binary_logloss: 0.467545\ttraining's amex_metric gini 0.906 recall 0.613: 0.759068\tvalid_1's binary_logloss: 0.469189\tvalid_1's amex_metric gini 0.902 recall 0.598: 0.750259\n",
      "[1800]\ttraining's binary_logloss: 0.462377\ttraining's amex_metric gini 0.906 recall 0.613: 0.759495\tvalid_1's binary_logloss: 0.464068\tvalid_1's amex_metric gini 0.902 recall 0.599: 0.750555\n",
      "[1900]\ttraining's binary_logloss: 0.454386\ttraining's amex_metric gini 0.906 recall 0.614: 0.759923\tvalid_1's binary_logloss: 0.456153\tvalid_1's amex_metric gini 0.903 recall 0.599: 0.750997\n",
      "[2000]\ttraining's binary_logloss: 0.445029\ttraining's amex_metric gini 0.906 recall 0.615: 0.760556\tvalid_1's binary_logloss: 0.446884\tvalid_1's amex_metric gini 0.903 recall 0.602: 0.752195\n",
      "[2100]\ttraining's binary_logloss: 0.439346\ttraining's amex_metric gini 0.906 recall 0.616: 0.761109\tvalid_1's binary_logloss: 0.441265\tvalid_1's amex_metric gini 0.903 recall 0.603: 0.75305\n",
      "[2200]\ttraining's binary_logloss: 0.434246\ttraining's amex_metric gini 0.907 recall 0.616: 0.76147\tvalid_1's binary_logloss: 0.436214\tvalid_1's amex_metric gini 0.903 recall 0.604: 0.753323\n",
      "[2300]\ttraining's binary_logloss: 0.426343\ttraining's amex_metric gini 0.907 recall 0.617: 0.761939\tvalid_1's binary_logloss: 0.428395\tvalid_1's amex_metric gini 0.903 recall 0.603: 0.753221\n",
      "[2400]\ttraining's binary_logloss: 0.421311\ttraining's amex_metric gini 0.907 recall 0.618: 0.762468\tvalid_1's binary_logloss: 0.423426\tvalid_1's amex_metric gini 0.903 recall 0.603: 0.753268\n",
      "[2500]\ttraining's binary_logloss: 0.415945\ttraining's amex_metric gini 0.907 recall 0.619: 0.763024\tvalid_1's binary_logloss: 0.418125\tvalid_1's amex_metric gini 0.904 recall 0.605: 0.754118\n",
      "[2600]\ttraining's binary_logloss: 0.409919\ttraining's amex_metric gini 0.908 recall 0.619: 0.763335\tvalid_1's binary_logloss: 0.41217\tvalid_1's amex_metric gini 0.904 recall 0.605: 0.75425\n",
      "[2700]\ttraining's binary_logloss: 0.403256\ttraining's amex_metric gini 0.908 recall 0.620: 0.763925\tvalid_1's binary_logloss: 0.405594\tvalid_1's amex_metric gini 0.904 recall 0.605: 0.754563\n",
      "[2800]\ttraining's binary_logloss: 0.399516\ttraining's amex_metric gini 0.908 recall 0.621: 0.764389\tvalid_1's binary_logloss: 0.401906\tvalid_1's amex_metric gini 0.904 recall 0.606: 0.754979\n",
      "[2900]\ttraining's binary_logloss: 0.392816\ttraining's amex_metric gini 0.908 recall 0.621: 0.764704\tvalid_1's binary_logloss: 0.395295\tvalid_1's amex_metric gini 0.904 recall 0.607: 0.755594\n",
      "[3000]\ttraining's binary_logloss: 0.386285\ttraining's amex_metric gini 0.908 recall 0.621: 0.764904\tvalid_1's binary_logloss: 0.388851\tvalid_1's amex_metric gini 0.905 recall 0.609: 0.756536\n",
      "[3100]\ttraining's binary_logloss: 0.381307\ttraining's amex_metric gini 0.909 recall 0.623: 0.76568\tvalid_1's binary_logloss: 0.383937\tvalid_1's amex_metric gini 0.905 recall 0.610: 0.75758\n",
      "[3200]\ttraining's binary_logloss: 0.376102\ttraining's amex_metric gini 0.909 recall 0.623: 0.765978\tvalid_1's binary_logloss: 0.378813\tvalid_1's amex_metric gini 0.905 recall 0.611: 0.757822\n",
      "[3300]\ttraining's binary_logloss: 0.371189\ttraining's amex_metric gini 0.909 recall 0.624: 0.766456\tvalid_1's binary_logloss: 0.373966\tvalid_1's amex_metric gini 0.905 recall 0.612: 0.758431\n",
      "[3400]\ttraining's binary_logloss: 0.365002\ttraining's amex_metric gini 0.909 recall 0.624: 0.766868\tvalid_1's binary_logloss: 0.367872\tvalid_1's amex_metric gini 0.905 recall 0.611: 0.758256\n",
      "[3500]\ttraining's binary_logloss: 0.360655\ttraining's amex_metric gini 0.910 recall 0.625: 0.767423\tvalid_1's binary_logloss: 0.363605\tvalid_1's amex_metric gini 0.905 recall 0.612: 0.758678\n",
      "[3600]\ttraining's binary_logloss: 0.356402\ttraining's amex_metric gini 0.910 recall 0.626: 0.767889\tvalid_1's binary_logloss: 0.359418\tvalid_1's amex_metric gini 0.906 recall 0.613: 0.759153\n",
      "[3700]\ttraining's binary_logloss: 0.350278\ttraining's amex_metric gini 0.910 recall 0.627: 0.768534\tvalid_1's binary_logloss: 0.353383\tvalid_1's amex_metric gini 0.906 recall 0.614: 0.759811\n",
      "[3800]\ttraining's binary_logloss: 0.346366\ttraining's amex_metric gini 0.910 recall 0.628: 0.768872\tvalid_1's binary_logloss: 0.349534\tvalid_1's amex_metric gini 0.906 recall 0.616: 0.760917\n",
      "[3900]\ttraining's binary_logloss: 0.343056\ttraining's amex_metric gini 0.910 recall 0.628: 0.769433\tvalid_1's binary_logloss: 0.34629\tvalid_1's amex_metric gini 0.906 recall 0.615: 0.760809\n",
      "[4000]\ttraining's binary_logloss: 0.339775\ttraining's amex_metric gini 0.911 recall 0.629: 0.769719\tvalid_1's binary_logloss: 0.34308\tvalid_1's amex_metric gini 0.906 recall 0.616: 0.760957\n",
      "[4100]\ttraining's binary_logloss: 0.336152\ttraining's amex_metric gini 0.911 recall 0.630: 0.770169\tvalid_1's binary_logloss: 0.339531\tvalid_1's amex_metric gini 0.906 recall 0.616: 0.761192\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=0'>1</a>\u001b[0m lgb_train \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(X_train\u001b[39m.\u001b[39mreshape(X_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=1'>2</a>\u001b[0m lgb_valid \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(X_test\u001b[39m.\u001b[39mreshape(X_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), y_test,)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=2'>3</a>\u001b[0m model2 \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=3'>4</a>\u001b[0m             params \u001b[39m=\u001b[39;49m params,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=4'>5</a>\u001b[0m             train_set \u001b[39m=\u001b[39;49m lgb_train,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=5'>6</a>\u001b[0m             num_boost_round \u001b[39m=\u001b[39;49m \u001b[39m5000\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=6'>7</a>\u001b[0m             valid_sets \u001b[39m=\u001b[39;49m [lgb_train, lgb_valid],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=7'>8</a>\u001b[0m             early_stopping_rounds \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=8'>9</a>\u001b[0m             verbose_eval \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=9'>10</a>\u001b[0m             feval \u001b[39m=\u001b[39;49m lgb_amex_metric\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=10'>11</a>\u001b[0m             )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/q/lib/python3.9/site-packages/lightgbm/engine.py:255\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[39mif\u001b[39;00m valid_sets \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     \u001b[39mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m--> 255\u001b[0m         evaluation_result_list\u001b[39m.\u001b[39mextend(booster\u001b[39m.\u001b[39;49meval_train(feval))\n\u001b[1;32m    256\u001b[0m     evaluation_result_list\u001b[39m.\u001b[39mextend(booster\u001b[39m.\u001b[39meval_valid(feval))\n\u001b[1;32m    257\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/q/lib/python3.9/site-packages/lightgbm/basic.py:2856\u001b[0m, in \u001b[0;36mBooster.eval_train\u001b[0;34m(self, feval)\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval_train\u001b[39m(\u001b[39mself\u001b[39m, feval\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2827\u001b[0m     \u001b[39m\"\"\"Evaluate for training data.\u001b[39;00m\n\u001b[1;32m   2828\u001b[0m \n\u001b[1;32m   2829\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2854\u001b[0m \u001b[39m        List with evaluation results.\u001b[39;00m\n\u001b[1;32m   2855\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2856\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__inner_eval(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_data_name, \u001b[39m0\u001b[39;49m, feval)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/q/lib/python3.9/site-packages/lightgbm/basic.py:3402\u001b[0m, in \u001b[0;36mBooster.__inner_eval\u001b[0;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[1;32m   3400\u001b[0m \u001b[39mif\u001b[39;00m eval_function \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3401\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m-> 3402\u001b[0m feval_ret \u001b[39m=\u001b[39m eval_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__inner_predict(data_idx), cur_data)\n\u001b[1;32m   3403\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feval_ret, \u001b[39mlist\u001b[39m):\n\u001b[1;32m   3404\u001b[0m     \u001b[39mfor\u001b[39;00m eval_name, val, is_higher_better \u001b[39min\u001b[39;00m feval_ret:\n",
      "\u001b[1;32m/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb Cell 13\u001b[0m in \u001b[0;36mlgb_amex_metric\u001b[0;34m(y_pred, y_true)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlgb_amex_metric\u001b[39m(y_pred, y_true):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=22'>23</a>\u001b[0m     y_true \u001b[39m=\u001b[39m y_true\u001b[39m.\u001b[39mget_label()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=23'>24</a>\u001b[0m     score, gini, recall \u001b[39m=\u001b[39m amex_metric(y_true, y_pred, return_components\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mamex_metric gini \u001b[39m\u001b[39m{\u001b[39;00mgini\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m recall \u001b[39m\u001b[39m{\u001b[39;00mrecall\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, score, \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;32m/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb Cell 13\u001b[0m in \u001b[0;36mamex_metric\u001b[0;34m(y_true, y_pred, return_components)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m]:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=8'>9</a>\u001b[0m     labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(np\u001b[39m.\u001b[39marray([y_true, y_pred]))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=9'>10</a>\u001b[0m     labels \u001b[39m=\u001b[39m labels[labels[:, i]\u001b[39m.\u001b[39;49margsort()[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=10'>11</a>\u001b[0m     weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(labels[:,\u001b[39m0\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/gbm.ipynb#ch0000011?line=11'>12</a>\u001b[0m     weight_random \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcumsum(weight \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(weight))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "lgb_valid = lgb.Dataset(X_test.reshape(X_test.shape[0], -1), y_test,)\n",
    "model2 = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 5000,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 105541, number of negative: 302381\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.232630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 329344\n",
      "[LightGBM] [Info] Number of data points in the train set: 407922, number of used features: 2443\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258728 -> initscore=-1.052588\n",
      "[LightGBM] [Info] Start training from score -1.052588\n",
      "[100]\ttraining's binary_logloss: 0.470607\ttraining's amex_metric gini 0.903 recall 0.604: 0.753508\tvalid_1's binary_logloss: 0.472209\tvalid_1's amex_metric gini 0.900 recall 0.592: 0.745712\n",
      "[200]\ttraining's binary_logloss: 0.450727\ttraining's amex_metric gini 0.905 recall 0.610: 0.757501\tvalid_1's binary_logloss: 0.452516\tvalid_1's amex_metric gini 0.901 recall 0.598: 0.74981\n",
      "[300]\ttraining's binary_logloss: 0.397729\ttraining's amex_metric gini 0.907 recall 0.617: 0.762134\tvalid_1's binary_logloss: 0.400057\tvalid_1's amex_metric gini 0.903 recall 0.603: 0.752913\n",
      "[400]\ttraining's binary_logloss: 0.372899\ttraining's amex_metric gini 0.908 recall 0.622: 0.764969\tvalid_1's binary_logloss: 0.375608\tvalid_1's amex_metric gini 0.904 recall 0.608: 0.755943\n",
      "[500]\ttraining's binary_logloss: 0.342295\ttraining's amex_metric gini 0.910 recall 0.626: 0.767621\tvalid_1's binary_logloss: 0.345522\tvalid_1's amex_metric gini 0.905 recall 0.613: 0.759354\n",
      "[600]\ttraining's binary_logloss: 0.324123\ttraining's amex_metric gini 0.911 recall 0.630: 0.770687\tvalid_1's binary_logloss: 0.327748\tvalid_1's amex_metric gini 0.907 recall 0.616: 0.761217\n",
      "[700]\ttraining's binary_logloss: 0.294551\ttraining's amex_metric gini 0.913 recall 0.637: 0.774782\tvalid_1's binary_logloss: 0.298961\tvalid_1's amex_metric gini 0.908 recall 0.618: 0.76312\n",
      "[800]\ttraining's binary_logloss: 0.273933\ttraining's amex_metric gini 0.915 recall 0.643: 0.778995\tvalid_1's binary_logloss: 0.279143\tvalid_1's amex_metric gini 0.909 recall 0.626: 0.767671\n",
      "[900]\ttraining's binary_logloss: 0.258739\ttraining's amex_metric gini 0.917 recall 0.650: 0.783707\tvalid_1's binary_logloss: 0.264772\tvalid_1's amex_metric gini 0.911 recall 0.629: 0.769663\n",
      "[1000]\ttraining's binary_logloss: 0.252262\ttraining's amex_metric gini 0.918 recall 0.655: 0.786526\tvalid_1's binary_logloss: 0.258815\tvalid_1's amex_metric gini 0.912 recall 0.630: 0.771017\n",
      "[1100]\ttraining's binary_logloss: 0.242277\ttraining's amex_metric gini 0.920 recall 0.660: 0.790247\tvalid_1's binary_logloss: 0.249687\tvalid_1's amex_metric gini 0.913 recall 0.635: 0.773748\n",
      "[1200]\ttraining's binary_logloss: 0.238952\ttraining's amex_metric gini 0.921 recall 0.665: 0.793164\tvalid_1's binary_logloss: 0.246849\tvalid_1's amex_metric gini 0.914 recall 0.635: 0.774361\n",
      "[1300]\ttraining's binary_logloss: 0.235016\ttraining's amex_metric gini 0.923 recall 0.669: 0.795637\tvalid_1's binary_logloss: 0.243516\tvalid_1's amex_metric gini 0.914 recall 0.636: 0.775206\n",
      "[1400]\ttraining's binary_logloss: 0.230468\ttraining's amex_metric gini 0.924 recall 0.672: 0.797942\tvalid_1's binary_logloss: 0.239715\tvalid_1's amex_metric gini 0.915 recall 0.637: 0.776294\n",
      "[1500]\ttraining's binary_logloss: 0.22818\ttraining's amex_metric gini 0.925 recall 0.676: 0.80029\tvalid_1's binary_logloss: 0.237943\tvalid_1's amex_metric gini 0.916 recall 0.639: 0.777326\n",
      "[1600]\ttraining's binary_logloss: 0.223005\ttraining's amex_metric gini 0.926 recall 0.680: 0.802959\tvalid_1's binary_logloss: 0.233857\tvalid_1's amex_metric gini 0.917 recall 0.641: 0.778611\n",
      "[1700]\ttraining's binary_logloss: 0.22053\ttraining's amex_metric gini 0.928 recall 0.684: 0.805627\tvalid_1's binary_logloss: 0.232129\tvalid_1's amex_metric gini 0.917 recall 0.643: 0.779826\n",
      "[1800]\ttraining's binary_logloss: 0.219235\ttraining's amex_metric gini 0.929 recall 0.687: 0.80782\tvalid_1's binary_logloss: 0.231368\tvalid_1's amex_metric gini 0.918 recall 0.644: 0.780673\n",
      "[1900]\ttraining's binary_logloss: 0.217046\ttraining's amex_metric gini 0.930 recall 0.691: 0.810351\tvalid_1's binary_logloss: 0.229964\tvalid_1's amex_metric gini 0.918 recall 0.645: 0.781542\n",
      "[2000]\ttraining's binary_logloss: 0.214628\ttraining's amex_metric gini 0.931 recall 0.694: 0.812406\tvalid_1's binary_logloss: 0.228458\tvalid_1's amex_metric gini 0.919 recall 0.645: 0.781921\n",
      "[2100]\ttraining's binary_logloss: 0.213205\ttraining's amex_metric gini 0.932 recall 0.698: 0.814626\tvalid_1's binary_logloss: 0.227696\tvalid_1's amex_metric gini 0.919 recall 0.645: 0.782194\n",
      "[2200]\ttraining's binary_logloss: 0.212025\ttraining's amex_metric gini 0.932 recall 0.700: 0.816304\tvalid_1's binary_logloss: 0.227161\tvalid_1's amex_metric gini 0.919 recall 0.648: 0.783687\n",
      "[2300]\ttraining's binary_logloss: 0.210243\ttraining's amex_metric gini 0.933 recall 0.704: 0.818451\tvalid_1's binary_logloss: 0.226227\tvalid_1's amex_metric gini 0.920 recall 0.649: 0.784403\n",
      "[2400]\ttraining's binary_logloss: 0.209162\ttraining's amex_metric gini 0.934 recall 0.706: 0.820237\tvalid_1's binary_logloss: 0.225776\tvalid_1's amex_metric gini 0.920 recall 0.650: 0.78485\n",
      "[2500]\ttraining's binary_logloss: 0.208003\ttraining's amex_metric gini 0.935 recall 0.709: 0.82205\tvalid_1's binary_logloss: 0.2253\tvalid_1's amex_metric gini 0.920 recall 0.650: 0.785012\n",
      "[2600]\ttraining's binary_logloss: 0.206689\ttraining's amex_metric gini 0.936 recall 0.712: 0.823726\tvalid_1's binary_logloss: 0.224744\tvalid_1's amex_metric gini 0.920 recall 0.651: 0.785649\n",
      "[2700]\ttraining's binary_logloss: 0.205176\ttraining's amex_metric gini 0.936 recall 0.715: 0.825728\tvalid_1's binary_logloss: 0.224072\tvalid_1's amex_metric gini 0.921 recall 0.652: 0.786147\n",
      "[2800]\ttraining's binary_logloss: 0.204411\ttraining's amex_metric gini 0.937 recall 0.717: 0.827094\tvalid_1's binary_logloss: 0.223878\tvalid_1's amex_metric gini 0.921 recall 0.652: 0.786273\n",
      "[2900]\ttraining's binary_logloss: 0.202924\ttraining's amex_metric gini 0.938 recall 0.720: 0.828845\tvalid_1's binary_logloss: 0.223311\tvalid_1's amex_metric gini 0.921 recall 0.652: 0.786318\n",
      "[3000]\ttraining's binary_logloss: 0.201433\ttraining's amex_metric gini 0.939 recall 0.723: 0.830641\tvalid_1's binary_logloss: 0.222758\tvalid_1's amex_metric gini 0.921 recall 0.654: 0.787474\n",
      "[3100]\ttraining's binary_logloss: 0.200317\ttraining's amex_metric gini 0.939 recall 0.725: 0.832464\tvalid_1's binary_logloss: 0.222368\tvalid_1's amex_metric gini 0.921 recall 0.654: 0.787812\n",
      "[3200]\ttraining's binary_logloss: 0.19915\ttraining's amex_metric gini 0.940 recall 0.729: 0.834375\tvalid_1's binary_logloss: 0.222007\tvalid_1's amex_metric gini 0.921 recall 0.655: 0.788196\n",
      "[3300]\ttraining's binary_logloss: 0.19807\ttraining's amex_metric gini 0.941 recall 0.732: 0.836238\tvalid_1's binary_logloss: 0.221692\tvalid_1's amex_metric gini 0.922 recall 0.656: 0.788885\n",
      "[3400]\ttraining's binary_logloss: 0.196608\ttraining's amex_metric gini 0.942 recall 0.735: 0.838186\tvalid_1's binary_logloss: 0.221262\tvalid_1's amex_metric gini 0.922 recall 0.656: 0.78881\n",
      "[3500]\ttraining's binary_logloss: 0.195554\ttraining's amex_metric gini 0.943 recall 0.738: 0.840266\tvalid_1's binary_logloss: 0.22101\tvalid_1's amex_metric gini 0.922 recall 0.656: 0.788965\n",
      "[3600]\ttraining's binary_logloss: 0.194519\ttraining's amex_metric gini 0.943 recall 0.740: 0.841848\tvalid_1's binary_logloss: 0.220788\tvalid_1's amex_metric gini 0.922 recall 0.656: 0.789024\n",
      "[3700]\ttraining's binary_logloss: 0.193064\ttraining's amex_metric gini 0.944 recall 0.743: 0.8436\tvalid_1's binary_logloss: 0.220364\tvalid_1's amex_metric gini 0.922 recall 0.655: 0.788498\n",
      "[3800]\ttraining's binary_logloss: 0.192118\ttraining's amex_metric gini 0.945 recall 0.747: 0.845735\tvalid_1's binary_logloss: 0.220206\tvalid_1's amex_metric gini 0.922 recall 0.655: 0.78878\n",
      "[3900]\ttraining's binary_logloss: 0.191278\ttraining's amex_metric gini 0.945 recall 0.749: 0.847312\tvalid_1's binary_logloss: 0.220095\tvalid_1's amex_metric gini 0.922 recall 0.656: 0.789086\n",
      "[4000]\ttraining's binary_logloss: 0.190451\ttraining's amex_metric gini 0.946 recall 0.752: 0.848965\tvalid_1's binary_logloss: 0.22\tvalid_1's amex_metric gini 0.922 recall 0.655: 0.788777\n",
      "[4100]\ttraining's binary_logloss: 0.189456\ttraining's amex_metric gini 0.947 recall 0.754: 0.850543\tvalid_1's binary_logloss: 0.219778\tvalid_1's amex_metric gini 0.923 recall 0.656: 0.789249\n",
      "[4200]\ttraining's binary_logloss: 0.188224\ttraining's amex_metric gini 0.947 recall 0.757: 0.852107\tvalid_1's binary_logloss: 0.219556\tvalid_1's amex_metric gini 0.923 recall 0.657: 0.789695\n",
      "[4300]\ttraining's binary_logloss: 0.186998\ttraining's amex_metric gini 0.948 recall 0.760: 0.85394\tvalid_1's binary_logloss: 0.21934\tvalid_1's amex_metric gini 0.923 recall 0.658: 0.790226\n",
      "[4400]\ttraining's binary_logloss: 0.186424\ttraining's amex_metric gini 0.949 recall 0.763: 0.85567\tvalid_1's binary_logloss: 0.219304\tvalid_1's amex_metric gini 0.923 recall 0.656: 0.789549\n",
      "[4500]\ttraining's binary_logloss: 0.185446\ttraining's amex_metric gini 0.949 recall 0.765: 0.857296\tvalid_1's binary_logloss: 0.219167\tvalid_1's amex_metric gini 0.923 recall 0.657: 0.789852\n",
      "[4600]\ttraining's binary_logloss: 0.184102\ttraining's amex_metric gini 0.950 recall 0.769: 0.859394\tvalid_1's binary_logloss: 0.218898\tvalid_1's amex_metric gini 0.923 recall 0.658: 0.790353\n",
      "[4700]\ttraining's binary_logloss: 0.183348\ttraining's amex_metric gini 0.951 recall 0.772: 0.861171\tvalid_1's binary_logloss: 0.21882\tvalid_1's amex_metric gini 0.923 recall 0.657: 0.790201\n",
      "[4800]\ttraining's binary_logloss: 0.182416\ttraining's amex_metric gini 0.951 recall 0.774: 0.862768\tvalid_1's binary_logloss: 0.21874\tvalid_1's amex_metric gini 0.923 recall 0.656: 0.789319\n",
      "[4900]\ttraining's binary_logloss: 0.181349\ttraining's amex_metric gini 0.952 recall 0.777: 0.864449\tvalid_1's binary_logloss: 0.218598\tvalid_1's amex_metric gini 0.923 recall 0.657: 0.790253\n",
      "[5000]\ttraining's binary_logloss: 0.180581\ttraining's amex_metric gini 0.953 recall 0.779: 0.865999\tvalid_1's binary_logloss: 0.218533\tvalid_1's amex_metric gini 0.923 recall 0.657: 0.789975\n",
      "[5100]\ttraining's binary_logloss: 0.179822\ttraining's amex_metric gini 0.953 recall 0.782: 0.867474\tvalid_1's binary_logloss: 0.218466\tvalid_1's amex_metric gini 0.923 recall 0.657: 0.790156\n",
      "[5200]\ttraining's binary_logloss: 0.178759\ttraining's amex_metric gini 0.954 recall 0.784: 0.868963\tvalid_1's binary_logloss: 0.218327\tvalid_1's amex_metric gini 0.923 recall 0.659: 0.791195\n",
      "[5300]\ttraining's binary_logloss: 0.178186\ttraining's amex_metric gini 0.954 recall 0.787: 0.87059\tvalid_1's binary_logloss: 0.218315\tvalid_1's amex_metric gini 0.923 recall 0.658: 0.790801\n",
      "[5400]\ttraining's binary_logloss: 0.177246\ttraining's amex_metric gini 0.955 recall 0.789: 0.871829\tvalid_1's binary_logloss: 0.218224\tvalid_1's amex_metric gini 0.923 recall 0.659: 0.791076\n",
      "[5500]\ttraining's binary_logloss: 0.176224\ttraining's amex_metric gini 0.956 recall 0.791: 0.873478\tvalid_1's binary_logloss: 0.218104\tvalid_1's amex_metric gini 0.923 recall 0.659: 0.790989\n",
      "[5600]\ttraining's binary_logloss: 0.17547\ttraining's amex_metric gini 0.956 recall 0.793: 0.874682\tvalid_1's binary_logloss: 0.218096\tvalid_1's amex_metric gini 0.923 recall 0.658: 0.790842\n",
      "[5700]\ttraining's binary_logloss: 0.174613\ttraining's amex_metric gini 0.957 recall 0.796: 0.876471\tvalid_1's binary_logloss: 0.218033\tvalid_1's amex_metric gini 0.923 recall 0.659: 0.791082\n",
      "[5800]\ttraining's binary_logloss: 0.173975\ttraining's amex_metric gini 0.957 recall 0.798: 0.877815\tvalid_1's binary_logloss: 0.218019\tvalid_1's amex_metric gini 0.923 recall 0.659: 0.791055\n",
      "[5900]\ttraining's binary_logloss: 0.173071\ttraining's amex_metric gini 0.958 recall 0.801: 0.879299\tvalid_1's binary_logloss: 0.217929\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.791598\n",
      "[6000]\ttraining's binary_logloss: 0.17253\ttraining's amex_metric gini 0.958 recall 0.803: 0.880682\tvalid_1's binary_logloss: 0.217923\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.791615\n",
      "[6100]\ttraining's binary_logloss: 0.171788\ttraining's amex_metric gini 0.959 recall 0.805: 0.881813\tvalid_1's binary_logloss: 0.217896\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.791729\n",
      "[6200]\ttraining's binary_logloss: 0.171041\ttraining's amex_metric gini 0.959 recall 0.807: 0.883044\tvalid_1's binary_logloss: 0.21786\tvalid_1's amex_metric gini 0.924 recall 0.659: 0.791208\n",
      "[6300]\ttraining's binary_logloss: 0.170182\ttraining's amex_metric gini 0.960 recall 0.809: 0.884243\tvalid_1's binary_logloss: 0.217756\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.791043\n",
      "[6400]\ttraining's binary_logloss: 0.169492\ttraining's amex_metric gini 0.960 recall 0.811: 0.885666\tvalid_1's binary_logloss: 0.217695\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.791818\n",
      "[6500]\ttraining's binary_logloss: 0.168585\ttraining's amex_metric gini 0.961 recall 0.814: 0.887094\tvalid_1's binary_logloss: 0.217608\tvalid_1's amex_metric gini 0.924 recall 0.659: 0.791273\n",
      "[6600]\ttraining's binary_logloss: 0.167578\ttraining's amex_metric gini 0.961 recall 0.816: 0.888691\tvalid_1's binary_logloss: 0.217545\tvalid_1's amex_metric gini 0.924 recall 0.659: 0.791206\n",
      "[6700]\ttraining's binary_logloss: 0.166661\ttraining's amex_metric gini 0.962 recall 0.818: 0.889917\tvalid_1's binary_logloss: 0.217465\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.79111\n",
      "[6800]\ttraining's binary_logloss: 0.165684\ttraining's amex_metric gini 0.962 recall 0.821: 0.89148\tvalid_1's binary_logloss: 0.217415\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.790963\n",
      "[6900]\ttraining's binary_logloss: 0.164875\ttraining's amex_metric gini 0.963 recall 0.824: 0.893191\tvalid_1's binary_logloss: 0.217358\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.790906\n",
      "[7000]\ttraining's binary_logloss: 0.164019\ttraining's amex_metric gini 0.963 recall 0.826: 0.894715\tvalid_1's binary_logloss: 0.217289\tvalid_1's amex_metric gini 0.924 recall 0.659: 0.791643\n",
      "[7100]\ttraining's binary_logloss: 0.163028\ttraining's amex_metric gini 0.964 recall 0.828: 0.89606\tvalid_1's binary_logloss: 0.217218\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.790833\n",
      "[7200]\ttraining's binary_logloss: 0.162138\ttraining's amex_metric gini 0.965 recall 0.831: 0.897544\tvalid_1's binary_logloss: 0.217173\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.790955\n",
      "[7300]\ttraining's binary_logloss: 0.161339\ttraining's amex_metric gini 0.965 recall 0.833: 0.899058\tvalid_1's binary_logloss: 0.217123\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.790863\n",
      "[7400]\ttraining's binary_logloss: 0.160538\ttraining's amex_metric gini 0.966 recall 0.835: 0.900569\tvalid_1's binary_logloss: 0.217092\tvalid_1's amex_metric gini 0.924 recall 0.658: 0.790834\n",
      "[7500]\ttraining's binary_logloss: 0.15967\ttraining's amex_metric gini 0.966 recall 0.838: 0.901994\tvalid_1's binary_logloss: 0.217061\tvalid_1's amex_metric gini 0.924 recall 0.659: 0.791438\n",
      "[7600]\ttraining's binary_logloss: 0.158926\ttraining's amex_metric gini 0.967 recall 0.840: 0.903379\tvalid_1's binary_logloss: 0.217045\tvalid_1's amex_metric gini 0.924 recall 0.659: 0.791635\n",
      "[7700]\ttraining's binary_logloss: 0.158297\ttraining's amex_metric gini 0.967 recall 0.842: 0.90464\tvalid_1's binary_logloss: 0.217047\tvalid_1's amex_metric gini 0.924 recall 0.659: 0.791638\n",
      "[7800]\ttraining's binary_logloss: 0.157553\ttraining's amex_metric gini 0.968 recall 0.844: 0.90603\tvalid_1's binary_logloss: 0.216993\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.791769\n",
      "[7900]\ttraining's binary_logloss: 0.156732\ttraining's amex_metric gini 0.968 recall 0.847: 0.907313\tvalid_1's binary_logloss: 0.216917\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.791905\n",
      "[8000]\ttraining's binary_logloss: 0.15578\ttraining's amex_metric gini 0.969 recall 0.848: 0.908323\tvalid_1's binary_logloss: 0.21684\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792447\n",
      "[8100]\ttraining's binary_logloss: 0.155107\ttraining's amex_metric gini 0.969 recall 0.850: 0.909554\tvalid_1's binary_logloss: 0.21684\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792597\n",
      "[8200]\ttraining's binary_logloss: 0.154235\ttraining's amex_metric gini 0.970 recall 0.852: 0.910969\tvalid_1's binary_logloss: 0.216804\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.792301\n",
      "[8300]\ttraining's binary_logloss: 0.153627\ttraining's amex_metric gini 0.970 recall 0.855: 0.912363\tvalid_1's binary_logloss: 0.216806\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792377\n",
      "[8400]\ttraining's binary_logloss: 0.153055\ttraining's amex_metric gini 0.970 recall 0.857: 0.913491\tvalid_1's binary_logloss: 0.216813\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.792228\n",
      "[8500]\ttraining's binary_logloss: 0.152446\ttraining's amex_metric gini 0.971 recall 0.858: 0.914621\tvalid_1's binary_logloss: 0.21683\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.792824\n",
      "[8600]\ttraining's binary_logloss: 0.151726\ttraining's amex_metric gini 0.971 recall 0.861: 0.915904\tvalid_1's binary_logloss: 0.216821\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.792936\n",
      "[8700]\ttraining's binary_logloss: 0.15075\ttraining's amex_metric gini 0.972 recall 0.863: 0.917333\tvalid_1's binary_logloss: 0.216767\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793286\n",
      "[8800]\ttraining's binary_logloss: 0.150097\ttraining's amex_metric gini 0.972 recall 0.865: 0.918611\tvalid_1's binary_logloss: 0.21673\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793001\n",
      "[8900]\ttraining's binary_logloss: 0.14943\ttraining's amex_metric gini 0.972 recall 0.867: 0.919678\tvalid_1's binary_logloss: 0.216719\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792437\n",
      "[9000]\ttraining's binary_logloss: 0.148681\ttraining's amex_metric gini 0.973 recall 0.869: 0.920793\tvalid_1's binary_logloss: 0.216733\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.792842\n",
      "[9100]\ttraining's binary_logloss: 0.148127\ttraining's amex_metric gini 0.973 recall 0.871: 0.92202\tvalid_1's binary_logloss: 0.21674\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792577\n",
      "[9200]\ttraining's binary_logloss: 0.147221\ttraining's amex_metric gini 0.974 recall 0.872: 0.923064\tvalid_1's binary_logloss: 0.216714\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.792921\n",
      "[9300]\ttraining's binary_logloss: 0.146527\ttraining's amex_metric gini 0.974 recall 0.874: 0.924324\tvalid_1's binary_logloss: 0.21669\tvalid_1's amex_metric gini 0.924 recall 0.660: 0.792251\n",
      "[9400]\ttraining's binary_logloss: 0.146134\ttraining's amex_metric gini 0.975 recall 0.876: 0.925436\tvalid_1's binary_logloss: 0.216706\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793001\n",
      "[9500]\ttraining's binary_logloss: 0.14529\ttraining's amex_metric gini 0.975 recall 0.878: 0.926264\tvalid_1's binary_logloss: 0.216657\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793014\n",
      "[9600]\ttraining's binary_logloss: 0.144585\ttraining's amex_metric gini 0.975 recall 0.879: 0.927307\tvalid_1's binary_logloss: 0.216624\tvalid_1's amex_metric gini 0.924 recall 0.663: 0.793741\n",
      "[9700]\ttraining's binary_logloss: 0.14388\ttraining's amex_metric gini 0.976 recall 0.881: 0.92856\tvalid_1's binary_logloss: 0.216609\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793333\n",
      "[9800]\ttraining's binary_logloss: 0.143169\ttraining's amex_metric gini 0.976 recall 0.883: 0.929486\tvalid_1's binary_logloss: 0.21659\tvalid_1's amex_metric gini 0.924 recall 0.663: 0.793449\n",
      "[9900]\ttraining's binary_logloss: 0.142503\ttraining's amex_metric gini 0.976 recall 0.884: 0.930429\tvalid_1's binary_logloss: 0.216596\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793032\n",
      "[10000]\ttraining's binary_logloss: 0.141925\ttraining's amex_metric gini 0.977 recall 0.886: 0.931615\tvalid_1's binary_logloss: 0.216581\tvalid_1's amex_metric gini 0.924 recall 0.663: 0.793376\n",
      "[10100]\ttraining's binary_logloss: 0.141573\ttraining's amex_metric gini 0.977 recall 0.888: 0.932693\tvalid_1's binary_logloss: 0.216615\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793062\n",
      "[10200]\ttraining's binary_logloss: 0.140981\ttraining's amex_metric gini 0.977 recall 0.890: 0.933725\tvalid_1's binary_logloss: 0.216619\tvalid_1's amex_metric gini 0.924 recall 0.662: 0.793021\n",
      "[10300]\ttraining's binary_logloss: 0.140288\ttraining's amex_metric gini 0.978 recall 0.892: 0.934711\tvalid_1's binary_logloss: 0.216583\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792543\n",
      "[10400]\ttraining's binary_logloss: 0.139693\ttraining's amex_metric gini 0.978 recall 0.893: 0.935584\tvalid_1's binary_logloss: 0.21658\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792506\n",
      "[10500]\ttraining's binary_logloss: 0.138972\ttraining's amex_metric gini 0.978 recall 0.894: 0.93643\tvalid_1's binary_logloss: 0.216569\tvalid_1's amex_metric gini 0.924 recall 0.661: 0.792544\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "lgb_valid = lgb.Dataset(X_test.reshape(X_test.shape[0], -1), y_test,)\n",
    "model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 79627, number of negative: 263514\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.150216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 328368\n",
      "[LightGBM] [Info] Number of data points in the train set: 343141, number of used features: 2425\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.232053 -> initscore=-1.196753\n",
      "[LightGBM] [Info] Start training from score -1.196753\n",
      "[100]\ttraining's binary_logloss: 0.447653\ttraining's amex_metric gini 0.918 recall 0.656: 0.787208\tvalid_1's binary_logloss: 0.449236\tvalid_1's amex_metric gini 0.911 recall 0.631: 0.770685\n",
      "[200]\ttraining's binary_logloss: 0.430692\ttraining's amex_metric gini 0.919 recall 0.661: 0.790093\tvalid_1's binary_logloss: 0.433192\tvalid_1's amex_metric gini 0.911 recall 0.635: 0.77314\n",
      "[300]\ttraining's binary_logloss: 0.3763\ttraining's amex_metric gini 0.921 recall 0.668: 0.794409\tvalid_1's binary_logloss: 0.379827\tvalid_1's amex_metric gini 0.913 recall 0.641: 0.77701\n",
      "[400]\ttraining's binary_logloss: 0.350905\ttraining's amex_metric gini 0.922 recall 0.673: 0.797448\tvalid_1's binary_logloss: 0.355111\tvalid_1's amex_metric gini 0.914 recall 0.643: 0.778446\n",
      "[500]\ttraining's binary_logloss: 0.319394\ttraining's amex_metric gini 0.923 recall 0.677: 0.79987\tvalid_1's binary_logloss: 0.324477\tvalid_1's amex_metric gini 0.914 recall 0.645: 0.779684\n",
      "[600]\ttraining's binary_logloss: 0.300473\ttraining's amex_metric gini 0.924 recall 0.682: 0.803044\tvalid_1's binary_logloss: 0.306225\tvalid_1's amex_metric gini 0.915 recall 0.648: 0.781629\n",
      "[700]\ttraining's binary_logloss: 0.269951\ttraining's amex_metric gini 0.926 recall 0.688: 0.806783\tvalid_1's binary_logloss: 0.276793\tvalid_1's amex_metric gini 0.917 recall 0.652: 0.784223\n",
      "[800]\ttraining's binary_logloss: 0.248732\ttraining's amex_metric gini 0.928 recall 0.695: 0.811098\tvalid_1's binary_logloss: 0.256684\tvalid_1's amex_metric gini 0.918 recall 0.657: 0.787324\n",
      "[900]\ttraining's binary_logloss: 0.233053\ttraining's amex_metric gini 0.930 recall 0.702: 0.816039\tvalid_1's binary_logloss: 0.242107\tvalid_1's amex_metric gini 0.919 recall 0.665: 0.792072\n",
      "[1000]\ttraining's binary_logloss: 0.226394\ttraining's amex_metric gini 0.931 recall 0.707: 0.819083\tvalid_1's binary_logloss: 0.236154\tvalid_1's amex_metric gini 0.920 recall 0.667: 0.793794\n",
      "[1100]\ttraining's binary_logloss: 0.216284\ttraining's amex_metric gini 0.933 recall 0.712: 0.822383\tvalid_1's binary_logloss: 0.227138\tvalid_1's amex_metric gini 0.921 recall 0.669: 0.794984\n",
      "[1200]\ttraining's binary_logloss: 0.212874\ttraining's amex_metric gini 0.934 recall 0.717: 0.825206\tvalid_1's binary_logloss: 0.224306\tvalid_1's amex_metric gini 0.922 recall 0.670: 0.796181\n",
      "[1300]\ttraining's binary_logloss: 0.208732\ttraining's amex_metric gini 0.935 recall 0.721: 0.827942\tvalid_1's binary_logloss: 0.220859\tvalid_1's amex_metric gini 0.923 recall 0.673: 0.797787\n",
      "[1400]\ttraining's binary_logloss: 0.203977\ttraining's amex_metric gini 0.936 recall 0.726: 0.830976\tvalid_1's binary_logloss: 0.217035\tvalid_1's amex_metric gini 0.924 recall 0.676: 0.799815\n",
      "[1500]\ttraining's binary_logloss: 0.201638\ttraining's amex_metric gini 0.937 recall 0.729: 0.833205\tvalid_1's binary_logloss: 0.215354\tvalid_1's amex_metric gini 0.924 recall 0.677: 0.800795\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "lgb_valid = lgb.Dataset(X_test.reshape(X_test.shape[0], -1), y_test,)\n",
    "model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 1500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 79627, number of negative: 263514\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.282570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 328368\n",
      "[LightGBM] [Info] Number of data points in the train set: 343141, number of used features: 2425\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.232053 -> initscore=-1.196753\n",
      "[LightGBM] [Info] Start training from score -1.196753\n",
      "[100]\ttraining's binary_logloss: 0.448219\ttraining's amex_metric gini 0.917 recall 0.652: 0.784676\tvalid_1's binary_logloss: 0.449789\tvalid_1's amex_metric gini 0.910 recall 0.628: 0.769098\n",
      "[200]\ttraining's binary_logloss: 0.431256\ttraining's amex_metric gini 0.919 recall 0.658: 0.788337\tvalid_1's binary_logloss: 0.433742\tvalid_1's amex_metric gini 0.911 recall 0.635: 0.772682\n",
      "[300]\ttraining's binary_logloss: 0.376935\ttraining's amex_metric gini 0.920 recall 0.666: 0.7932\tvalid_1's binary_logloss: 0.380447\tvalid_1's amex_metric gini 0.912 recall 0.640: 0.776354\n",
      "[400]\ttraining's binary_logloss: 0.351547\ttraining's amex_metric gini 0.921 recall 0.671: 0.796181\tvalid_1's binary_logloss: 0.355741\tvalid_1's amex_metric gini 0.913 recall 0.640: 0.776788\n",
      "[500]\ttraining's binary_logloss: 0.32007\ttraining's amex_metric gini 0.922 recall 0.676: 0.798998\tvalid_1's binary_logloss: 0.325111\tvalid_1's amex_metric gini 0.914 recall 0.645: 0.779358\n",
      "[600]\ttraining's binary_logloss: 0.30117\ttraining's amex_metric gini 0.924 recall 0.680: 0.801985\tvalid_1's binary_logloss: 0.30685\tvalid_1's amex_metric gini 0.915 recall 0.648: 0.78138\n",
      "[700]\ttraining's binary_logloss: 0.27066\ttraining's amex_metric gini 0.925 recall 0.686: 0.805923\tvalid_1's binary_logloss: 0.277409\tvalid_1's amex_metric gini 0.916 recall 0.652: 0.784031\n",
      "[800]\ttraining's binary_logloss: 0.249452\ttraining's amex_metric gini 0.927 recall 0.693: 0.810251\tvalid_1's binary_logloss: 0.25727\tvalid_1's amex_metric gini 0.918 recall 0.657: 0.787511\n",
      "[900]\ttraining's binary_logloss: 0.233784\ttraining's amex_metric gini 0.929 recall 0.701: 0.815059\tvalid_1's binary_logloss: 0.242608\tvalid_1's amex_metric gini 0.919 recall 0.663: 0.791174\n",
      "[1000]\ttraining's binary_logloss: 0.227146\ttraining's amex_metric gini 0.931 recall 0.705: 0.817753\tvalid_1's binary_logloss: 0.236659\tvalid_1's amex_metric gini 0.920 recall 0.664: 0.791886\n",
      "[1100]\ttraining's binary_logloss: 0.217053\ttraining's amex_metric gini 0.932 recall 0.711: 0.821314\tvalid_1's binary_logloss: 0.227594\tvalid_1's amex_metric gini 0.921 recall 0.667: 0.793959\n",
      "[1200]\ttraining's binary_logloss: 0.213654\ttraining's amex_metric gini 0.933 recall 0.715: 0.823997\tvalid_1's binary_logloss: 0.224717\tvalid_1's amex_metric gini 0.922 recall 0.670: 0.795732\n",
      "[1300]\ttraining's binary_logloss: 0.209549\ttraining's amex_metric gini 0.934 recall 0.720: 0.827082\tvalid_1's binary_logloss: 0.221298\tvalid_1's amex_metric gini 0.923 recall 0.673: 0.79768\n",
      "[1400]\ttraining's binary_logloss: 0.20483\ttraining's amex_metric gini 0.936 recall 0.724: 0.829766\tvalid_1's binary_logloss: 0.217456\tvalid_1's amex_metric gini 0.923 recall 0.676: 0.799813\n",
      "[1500]\ttraining's binary_logloss: 0.202526\ttraining's amex_metric gini 0.937 recall 0.728: 0.832132\tvalid_1's binary_logloss: 0.215767\tvalid_1's amex_metric gini 0.924 recall 0.678: 0.800789\n",
      "[1600]\ttraining's binary_logloss: 0.197347\ttraining's amex_metric gini 0.938 recall 0.733: 0.835428\tvalid_1's binary_logloss: 0.211862\tvalid_1's amex_metric gini 0.925 recall 0.677: 0.800841\n",
      "[1700]\ttraining's binary_logloss: 0.194863\ttraining's amex_metric gini 0.939 recall 0.737: 0.83812\tvalid_1's binary_logloss: 0.210257\tvalid_1's amex_metric gini 0.925 recall 0.679: 0.801963\n",
      "[1800]\ttraining's binary_logloss: 0.193499\ttraining's amex_metric gini 0.940 recall 0.740: 0.840335\tvalid_1's binary_logloss: 0.209455\tvalid_1's amex_metric gini 0.926 recall 0.680: 0.802991\n",
      "[1900]\ttraining's binary_logloss: 0.191254\ttraining's amex_metric gini 0.941 recall 0.744: 0.842783\tvalid_1's binary_logloss: 0.208105\tvalid_1's amex_metric gini 0.926 recall 0.685: 0.805567\n",
      "[2000]\ttraining's binary_logloss: 0.188809\ttraining's amex_metric gini 0.942 recall 0.748: 0.845279\tvalid_1's binary_logloss: 0.206656\tvalid_1's amex_metric gini 0.927 recall 0.684: 0.805561\n",
      "[2100]\ttraining's binary_logloss: 0.187331\ttraining's amex_metric gini 0.943 recall 0.752: 0.847488\tvalid_1's binary_logloss: 0.20593\tvalid_1's amex_metric gini 0.927 recall 0.687: 0.806822\n",
      "[2200]\ttraining's binary_logloss: 0.186105\ttraining's amex_metric gini 0.944 recall 0.755: 0.849666\tvalid_1's binary_logloss: 0.205427\tvalid_1's amex_metric gini 0.927 recall 0.688: 0.807628\n",
      "[2300]\ttraining's binary_logloss: 0.184204\ttraining's amex_metric gini 0.945 recall 0.759: 0.85188\tvalid_1's binary_logloss: 0.2045\tvalid_1's amex_metric gini 0.928 recall 0.687: 0.807391\n",
      "[2400]\ttraining's binary_logloss: 0.183052\ttraining's amex_metric gini 0.946 recall 0.761: 0.853688\tvalid_1's binary_logloss: 0.204027\tvalid_1's amex_metric gini 0.928 recall 0.689: 0.808355\n",
      "[2500]\ttraining's binary_logloss: 0.181821\ttraining's amex_metric gini 0.947 recall 0.765: 0.855691\tvalid_1's binary_logloss: 0.203549\tvalid_1's amex_metric gini 0.928 recall 0.689: 0.808637\n",
      "[2600]\ttraining's binary_logloss: 0.180453\ttraining's amex_metric gini 0.948 recall 0.768: 0.857828\tvalid_1's binary_logloss: 0.20304\tvalid_1's amex_metric gini 0.929 recall 0.689: 0.808744\n",
      "[2700]\ttraining's binary_logloss: 0.178959\ttraining's amex_metric gini 0.948 recall 0.771: 0.859814\tvalid_1's binary_logloss: 0.202501\tvalid_1's amex_metric gini 0.929 recall 0.689: 0.808832\n",
      "[2800]\ttraining's binary_logloss: 0.178097\ttraining's amex_metric gini 0.949 recall 0.774: 0.861596\tvalid_1's binary_logloss: 0.202249\tvalid_1's amex_metric gini 0.929 recall 0.690: 0.809703\n",
      "[2900]\ttraining's binary_logloss: 0.176539\ttraining's amex_metric gini 0.950 recall 0.777: 0.863611\tvalid_1's binary_logloss: 0.201692\tvalid_1's amex_metric gini 0.929 recall 0.690: 0.809401\n",
      "[3000]\ttraining's binary_logloss: 0.174964\ttraining's amex_metric gini 0.951 recall 0.782: 0.866251\tvalid_1's binary_logloss: 0.201193\tvalid_1's amex_metric gini 0.929 recall 0.691: 0.81015\n",
      "[3100]\ttraining's binary_logloss: 0.173792\ttraining's amex_metric gini 0.952 recall 0.785: 0.868207\tvalid_1's binary_logloss: 0.200895\tvalid_1's amex_metric gini 0.929 recall 0.690: 0.809933\n",
      "[3200]\ttraining's binary_logloss: 0.172521\ttraining's amex_metric gini 0.952 recall 0.788: 0.870013\tvalid_1's binary_logloss: 0.200558\tvalid_1's amex_metric gini 0.930 recall 0.693: 0.811435\n",
      "[3300]\ttraining's binary_logloss: 0.171379\ttraining's amex_metric gini 0.953 recall 0.792: 0.872294\tvalid_1's binary_logloss: 0.200269\tvalid_1's amex_metric gini 0.930 recall 0.693: 0.811417\n",
      "[3400]\ttraining's binary_logloss: 0.16983\ttraining's amex_metric gini 0.954 recall 0.794: 0.874094\tvalid_1's binary_logloss: 0.199865\tvalid_1's amex_metric gini 0.930 recall 0.694: 0.811991\n",
      "[3500]\ttraining's binary_logloss: 0.168705\ttraining's amex_metric gini 0.955 recall 0.797: 0.876048\tvalid_1's binary_logloss: 0.199613\tvalid_1's amex_metric gini 0.930 recall 0.693: 0.811726\n",
      "[3600]\ttraining's binary_logloss: 0.167583\ttraining's amex_metric gini 0.955 recall 0.800: 0.877942\tvalid_1's binary_logloss: 0.199398\tvalid_1's amex_metric gini 0.930 recall 0.694: 0.812351\n",
      "[3700]\ttraining's binary_logloss: 0.166016\ttraining's amex_metric gini 0.956 recall 0.805: 0.880359\tvalid_1's binary_logloss: 0.199031\tvalid_1's amex_metric gini 0.930 recall 0.695: 0.812519\n",
      "[3800]\ttraining's binary_logloss: 0.165004\ttraining's amex_metric gini 0.957 recall 0.808: 0.882269\tvalid_1's binary_logloss: 0.198886\tvalid_1's amex_metric gini 0.930 recall 0.694: 0.812473\n",
      "[3900]\ttraining's binary_logloss: 0.164098\ttraining's amex_metric gini 0.958 recall 0.810: 0.88388\tvalid_1's binary_logloss: 0.198783\tvalid_1's amex_metric gini 0.931 recall 0.695: 0.812617\n",
      "[4000]\ttraining's binary_logloss: 0.163218\ttraining's amex_metric gini 0.958 recall 0.812: 0.885381\tvalid_1's binary_logloss: 0.198658\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813273\n",
      "[4100]\ttraining's binary_logloss: 0.16215\ttraining's amex_metric gini 0.959 recall 0.816: 0.887283\tvalid_1's binary_logloss: 0.198475\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813629\n",
      "[4200]\ttraining's binary_logloss: 0.160835\ttraining's amex_metric gini 0.960 recall 0.819: 0.889121\tvalid_1's binary_logloss: 0.198231\tvalid_1's amex_metric gini 0.931 recall 0.697: 0.813891\n",
      "[4300]\ttraining's binary_logloss: 0.159509\ttraining's amex_metric gini 0.960 recall 0.821: 0.890892\tvalid_1's binary_logloss: 0.198029\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813267\n",
      "[4400]\ttraining's binary_logloss: 0.158864\ttraining's amex_metric gini 0.961 recall 0.824: 0.892585\tvalid_1's binary_logloss: 0.198005\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813655\n",
      "[4500]\ttraining's binary_logloss: 0.157824\ttraining's amex_metric gini 0.962 recall 0.827: 0.894472\tvalid_1's binary_logloss: 0.19787\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813498\n",
      "[4600]\ttraining's binary_logloss: 0.156364\ttraining's amex_metric gini 0.962 recall 0.831: 0.896579\tvalid_1's binary_logloss: 0.197663\tvalid_1's amex_metric gini 0.931 recall 0.695: 0.813128\n",
      "[4700]\ttraining's binary_logloss: 0.155542\ttraining's amex_metric gini 0.963 recall 0.833: 0.897946\tvalid_1's binary_logloss: 0.197611\tvalid_1's amex_metric gini 0.931 recall 0.697: 0.814019\n",
      "[4800]\ttraining's binary_logloss: 0.154513\ttraining's amex_metric gini 0.964 recall 0.835: 0.899422\tvalid_1's binary_logloss: 0.197513\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813884\n",
      "[4900]\ttraining's binary_logloss: 0.153318\ttraining's amex_metric gini 0.964 recall 0.839: 0.901451\tvalid_1's binary_logloss: 0.19737\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813916\n",
      "[5000]\ttraining's binary_logloss: 0.152468\ttraining's amex_metric gini 0.965 recall 0.841: 0.90316\tvalid_1's binary_logloss: 0.197337\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813829\n",
      "[5100]\ttraining's binary_logloss: 0.151654\ttraining's amex_metric gini 0.966 recall 0.844: 0.904818\tvalid_1's binary_logloss: 0.197311\tvalid_1's amex_metric gini 0.931 recall 0.697: 0.814249\n",
      "[5200]\ttraining's binary_logloss: 0.150506\ttraining's amex_metric gini 0.966 recall 0.846: 0.906198\tvalid_1's binary_logloss: 0.197176\tvalid_1's amex_metric gini 0.931 recall 0.697: 0.814173\n",
      "[5300]\ttraining's binary_logloss: 0.149871\ttraining's amex_metric gini 0.967 recall 0.849: 0.907685\tvalid_1's binary_logloss: 0.197135\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.814603\n",
      "[5400]\ttraining's binary_logloss: 0.148844\ttraining's amex_metric gini 0.967 recall 0.851: 0.909182\tvalid_1's binary_logloss: 0.197064\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814107\n",
      "[5500]\ttraining's binary_logloss: 0.147741\ttraining's amex_metric gini 0.968 recall 0.854: 0.910914\tvalid_1's binary_logloss: 0.196968\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813928\n",
      "[5600]\ttraining's binary_logloss: 0.146916\ttraining's amex_metric gini 0.969 recall 0.856: 0.912479\tvalid_1's binary_logloss: 0.19694\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813992\n",
      "[5700]\ttraining's binary_logloss: 0.145985\ttraining's amex_metric gini 0.969 recall 0.859: 0.914123\tvalid_1's binary_logloss: 0.196857\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813969\n",
      "[5800]\ttraining's binary_logloss: 0.145286\ttraining's amex_metric gini 0.970 recall 0.862: 0.915715\tvalid_1's binary_logloss: 0.196794\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813993\n",
      "[5900]\ttraining's binary_logloss: 0.144292\ttraining's amex_metric gini 0.970 recall 0.864: 0.917159\tvalid_1's binary_logloss: 0.196722\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813697\n",
      "[6000]\ttraining's binary_logloss: 0.1437\ttraining's amex_metric gini 0.971 recall 0.866: 0.918442\tvalid_1's binary_logloss: 0.196749\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813693\n",
      "[6100]\ttraining's binary_logloss: 0.14291\ttraining's amex_metric gini 0.971 recall 0.868: 0.919693\tvalid_1's binary_logloss: 0.196761\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813887\n",
      "[6200]\ttraining's binary_logloss: 0.142105\ttraining's amex_metric gini 0.972 recall 0.870: 0.920952\tvalid_1's binary_logloss: 0.196696\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813746\n",
      "[6300]\ttraining's binary_logloss: 0.141168\ttraining's amex_metric gini 0.972 recall 0.873: 0.922389\tvalid_1's binary_logloss: 0.196592\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814178\n",
      "[6400]\ttraining's binary_logloss: 0.14041\ttraining's amex_metric gini 0.973 recall 0.875: 0.923694\tvalid_1's binary_logloss: 0.196545\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.81404\n",
      "[6500]\ttraining's binary_logloss: 0.139416\ttraining's amex_metric gini 0.973 recall 0.878: 0.925287\tvalid_1's binary_logloss: 0.196482\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814358\n",
      "[6600]\ttraining's binary_logloss: 0.138329\ttraining's amex_metric gini 0.974 recall 0.880: 0.9266\tvalid_1's binary_logloss: 0.196403\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814326\n",
      "[6700]\ttraining's binary_logloss: 0.137349\ttraining's amex_metric gini 0.974 recall 0.882: 0.928184\tvalid_1's binary_logloss: 0.196311\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813795\n",
      "[6800]\ttraining's binary_logloss: 0.136278\ttraining's amex_metric gini 0.975 recall 0.885: 0.929704\tvalid_1's binary_logloss: 0.196285\tvalid_1's amex_metric gini 0.932 recall 0.695: 0.813693\n",
      "[6900]\ttraining's binary_logloss: 0.135398\ttraining's amex_metric gini 0.975 recall 0.887: 0.931257\tvalid_1's binary_logloss: 0.196261\tvalid_1's amex_metric gini 0.932 recall 0.695: 0.81365\n",
      "[7000]\ttraining's binary_logloss: 0.134489\ttraining's amex_metric gini 0.976 recall 0.889: 0.932541\tvalid_1's binary_logloss: 0.196256\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.8138\n",
      "[7100]\ttraining's binary_logloss: 0.133419\ttraining's amex_metric gini 0.976 recall 0.892: 0.934224\tvalid_1's binary_logloss: 0.196192\tvalid_1's amex_metric gini 0.932 recall 0.695: 0.813566\n",
      "[7200]\ttraining's binary_logloss: 0.132464\ttraining's amex_metric gini 0.977 recall 0.894: 0.935466\tvalid_1's binary_logloss: 0.1961\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.814103\n",
      "[7300]\ttraining's binary_logloss: 0.131603\ttraining's amex_metric gini 0.977 recall 0.897: 0.936967\tvalid_1's binary_logloss: 0.196038\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814382\n",
      "[7400]\ttraining's binary_logloss: 0.130749\ttraining's amex_metric gini 0.978 recall 0.898: 0.938134\tvalid_1's binary_logloss: 0.196009\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814443\n",
      "[7500]\ttraining's binary_logloss: 0.129822\ttraining's amex_metric gini 0.978 recall 0.901: 0.939749\tvalid_1's binary_logloss: 0.195982\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.8144\n",
      "[7600]\ttraining's binary_logloss: 0.129026\ttraining's amex_metric gini 0.979 recall 0.903: 0.940951\tvalid_1's binary_logloss: 0.195907\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814679\n",
      "[7700]\ttraining's binary_logloss: 0.128354\ttraining's amex_metric gini 0.979 recall 0.905: 0.942205\tvalid_1's binary_logloss: 0.1959\tvalid_1's amex_metric gini 0.932 recall 0.699: 0.815391\n",
      "[7800]\ttraining's binary_logloss: 0.127573\ttraining's amex_metric gini 0.980 recall 0.907: 0.943461\tvalid_1's binary_logloss: 0.19587\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815244\n",
      "[7900]\ttraining's binary_logloss: 0.126702\ttraining's amex_metric gini 0.980 recall 0.909: 0.944639\tvalid_1's binary_logloss: 0.195888\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814626\n",
      "[8000]\ttraining's binary_logloss: 0.125672\ttraining's amex_metric gini 0.981 recall 0.911: 0.94601\tvalid_1's binary_logloss: 0.195867\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.814882\n",
      "[8100]\ttraining's binary_logloss: 0.124961\ttraining's amex_metric gini 0.981 recall 0.913: 0.947205\tvalid_1's binary_logloss: 0.195846\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.81469\n",
      "[8200]\ttraining's binary_logloss: 0.124027\ttraining's amex_metric gini 0.981 recall 0.915: 0.948331\tvalid_1's binary_logloss: 0.195852\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814631\n",
      "[8300]\ttraining's binary_logloss: 0.123368\ttraining's amex_metric gini 0.982 recall 0.917: 0.94955\tvalid_1's binary_logloss: 0.195848\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.81433\n",
      "[8400]\ttraining's binary_logloss: 0.122757\ttraining's amex_metric gini 0.982 recall 0.919: 0.950578\tvalid_1's binary_logloss: 0.19581\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814396\n",
      "[8500]\ttraining's binary_logloss: 0.122117\ttraining's amex_metric gini 0.983 recall 0.921: 0.951685\tvalid_1's binary_logloss: 0.195821\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814748\n",
      "[8600]\ttraining's binary_logloss: 0.121356\ttraining's amex_metric gini 0.983 recall 0.922: 0.95253\tvalid_1's binary_logloss: 0.19579\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814756\n",
      "[8700]\ttraining's binary_logloss: 0.120326\ttraining's amex_metric gini 0.983 recall 0.924: 0.953651\tvalid_1's binary_logloss: 0.195773\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814754\n",
      "[8800]\ttraining's binary_logloss: 0.119638\ttraining's amex_metric gini 0.984 recall 0.926: 0.954753\tvalid_1's binary_logloss: 0.195732\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.81528\n",
      "[8900]\ttraining's binary_logloss: 0.118918\ttraining's amex_metric gini 0.984 recall 0.927: 0.955741\tvalid_1's binary_logloss: 0.195718\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814725\n",
      "[9000]\ttraining's binary_logloss: 0.118137\ttraining's amex_metric gini 0.984 recall 0.929: 0.956744\tvalid_1's binary_logloss: 0.195725\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815128\n",
      "[9100]\ttraining's binary_logloss: 0.117544\ttraining's amex_metric gini 0.985 recall 0.931: 0.957681\tvalid_1's binary_logloss: 0.195737\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815224\n",
      "[9200]\ttraining's binary_logloss: 0.116586\ttraining's amex_metric gini 0.985 recall 0.932: 0.958784\tvalid_1's binary_logloss: 0.195684\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815241\n",
      "[9300]\ttraining's binary_logloss: 0.115853\ttraining's amex_metric gini 0.986 recall 0.934: 0.959822\tvalid_1's binary_logloss: 0.195707\tvalid_1's amex_metric gini 0.932 recall 0.699: 0.815435\n",
      "[9400]\ttraining's binary_logloss: 0.115432\ttraining's amex_metric gini 0.986 recall 0.935: 0.960517\tvalid_1's binary_logloss: 0.195704\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815035\n",
      "[9500]\ttraining's binary_logloss: 0.114533\ttraining's amex_metric gini 0.986 recall 0.937: 0.961495\tvalid_1's binary_logloss: 0.195694\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814682\n",
      "[9600]\ttraining's binary_logloss: 0.113801\ttraining's amex_metric gini 0.987 recall 0.938: 0.962383\tvalid_1's binary_logloss: 0.195673\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815198\n",
      "[9700]\ttraining's binary_logloss: 0.113068\ttraining's amex_metric gini 0.987 recall 0.940: 0.963187\tvalid_1's binary_logloss: 0.195663\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814847\n",
      "[9800]\ttraining's binary_logloss: 0.112332\ttraining's amex_metric gini 0.987 recall 0.941: 0.964128\tvalid_1's binary_logloss: 0.195693\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815091\n",
      "[9900]\ttraining's binary_logloss: 0.111635\ttraining's amex_metric gini 0.987 recall 0.943: 0.965088\tvalid_1's binary_logloss: 0.195677\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815096\n",
      "[10000]\ttraining's binary_logloss: 0.111035\ttraining's amex_metric gini 0.988 recall 0.944: 0.965822\tvalid_1's binary_logloss: 0.195639\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814805\n",
      "[10100]\ttraining's binary_logloss: 0.110667\ttraining's amex_metric gini 0.988 recall 0.945: 0.966596\tvalid_1's binary_logloss: 0.195666\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.814391\n",
      "[10200]\ttraining's binary_logloss: 0.110053\ttraining's amex_metric gini 0.988 recall 0.946: 0.967055\tvalid_1's binary_logloss: 0.195645\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815258\n",
      "[10300]\ttraining's binary_logloss: 0.109334\ttraining's amex_metric gini 0.989 recall 0.947: 0.967817\tvalid_1's binary_logloss: 0.195633\tvalid_1's amex_metric gini 0.932 recall 0.699: 0.815412\n",
      "[10400]\ttraining's binary_logloss: 0.108713\ttraining's amex_metric gini 0.989 recall 0.948: 0.968546\tvalid_1's binary_logloss: 0.195641\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.814194\n",
      "[10500]\ttraining's binary_logloss: 0.107965\ttraining's amex_metric gini 0.989 recall 0.950: 0.969371\tvalid_1's binary_logloss: 0.195627\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814504\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "lgb_valid = lgb.Dataset(X_test.reshape(X_test.shape[0], -1), y_test,)\n",
    "model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/nimamanaf/Desktop/kaggle/pd/data/out/lgbm13.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "joblib.dump(model, OUTDIR+f'lgbm13.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train, test=None, n_folds=5, seed=42):\n",
    "    cat_features = [f\"{cf}_last\" for cf in CATCOLS]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        if test is not None:\n",
    "            test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        if test is not None:\n",
    "            test[col + '_round2'] = test[col].round(2)\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', \"S_2\", \"target\"]]\n",
    "    \n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[\"target\"])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[\"target\"].iloc[trn_ind], train[\"target\"].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        # Save best model\n",
    "        joblib.dump(model, OUTDIR+f'Models/lgbm_fold{fold}_seed{seed}.pkl')\n",
    "        val_pred = model.predict(x_val) # Predict validation\n",
    "        oof_predictions[val_ind] = val_pred  # Add to out of folds array\n",
    "        if test is not None:\n",
    "            test_pred = model.predict(test[features]) # Predict the test set\n",
    "            test_predictions += test_pred/n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    score = amex_metric(train[\"target\"], oof_predictions)  # Compute out of folds metric\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[\"target\"], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(OUTDIR+f'oof_lgbm_baseline_{n_folds}fold_seed{seed}.csv', index=False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    if test is not None:\n",
    "        test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "        test_df.to_csv(f'/content/drive/MyDrive/Amex/Predictions/test_lgbm_baseline_{n_folds}fold_seed{seed}.csv', index = False)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(OUTDIR+f'lgbm13.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load(OUTDIR+\"test_raw_all_mean_q1_q99_data.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_data.reshape(test_data.shape[0], -1)) # Predict the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(OUTDIR+'test_raw_all_mean_q1_q99_id.json', 'r') as f:\n",
    "            test_id_dict = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({\"customer_ID\":test_id_dict.values(), \n",
    "                        \"prediction\":test_pred.reshape(-1)\n",
    "                        }\n",
    "                        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "sub_file_dir = os.path.join(OUTDIR, \"lgbm_med_sub.csv\")\n",
    "result.set_index(\"customer_ID\").to_csv(sub_file_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('p8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25d4fc9e51127fc9597103904eccbdaaf4ac71170a764a4234e11ed19cb4831a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
