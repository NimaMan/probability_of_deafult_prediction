{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "\n",
    "import random\n",
    "import joblib\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "\n",
    "from pd.params import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true, y_pred, return_components=False):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    if return_components:\n",
    "        return 0.5 * (gini[1]/gini[0] + top_four), gini[1]/gini[0], top_four\n",
    "\n",
    "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
    "\n",
    "def lgb_amex_metric(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    score, gini, recall = amex_metric(y_true, y_pred, return_components=True)\n",
    "    return f'amex_metric gini {gini:.3f} recall {recall:.3f}', score, True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': \"binary_logloss\",\n",
    "        'boosting': 'dart',\n",
    "        'seed': 42,\n",
    "        'num_leaves': 100,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.20,\n",
    "        'bagging_freq': 10,\n",
    "        'bagging_fraction': 0.50,\n",
    "        'n_jobs': -1,\n",
    "        'lambda_l2': 8,\n",
    "        'min_data_in_leaf': 40\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(OUTDIR+\"train_data_all.npy\").transpose((0, 2, 1))\n",
    "train_labels = np.load(OUTDIR+\"train_labels_all.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(OUTDIR+\"train13_raw_all_data.npy\")\n",
    "train_labels = np.load(OUTDIR+\"train13_raw_all_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=1/9, random_state=0, shuffle=True)\n",
    "validation_data = (X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 79627, number of negative: 263514\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.150216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 328368\n",
      "[LightGBM] [Info] Number of data points in the train set: 343141, number of used features: 2425\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.232053 -> initscore=-1.196753\n",
      "[LightGBM] [Info] Start training from score -1.196753\n",
      "[100]\ttraining's binary_logloss: 0.447653\ttraining's amex_metric gini 0.918 recall 0.656: 0.787208\tvalid_1's binary_logloss: 0.449236\tvalid_1's amex_metric gini 0.911 recall 0.631: 0.770685\n",
      "[200]\ttraining's binary_logloss: 0.430692\ttraining's amex_metric gini 0.919 recall 0.661: 0.790093\tvalid_1's binary_logloss: 0.433192\tvalid_1's amex_metric gini 0.911 recall 0.635: 0.77314\n",
      "[300]\ttraining's binary_logloss: 0.3763\ttraining's amex_metric gini 0.921 recall 0.668: 0.794409\tvalid_1's binary_logloss: 0.379827\tvalid_1's amex_metric gini 0.913 recall 0.641: 0.77701\n",
      "[400]\ttraining's binary_logloss: 0.350905\ttraining's amex_metric gini 0.922 recall 0.673: 0.797448\tvalid_1's binary_logloss: 0.355111\tvalid_1's amex_metric gini 0.914 recall 0.643: 0.778446\n",
      "[500]\ttraining's binary_logloss: 0.319394\ttraining's amex_metric gini 0.923 recall 0.677: 0.79987\tvalid_1's binary_logloss: 0.324477\tvalid_1's amex_metric gini 0.914 recall 0.645: 0.779684\n",
      "[600]\ttraining's binary_logloss: 0.300473\ttraining's amex_metric gini 0.924 recall 0.682: 0.803044\tvalid_1's binary_logloss: 0.306225\tvalid_1's amex_metric gini 0.915 recall 0.648: 0.781629\n",
      "[700]\ttraining's binary_logloss: 0.269951\ttraining's amex_metric gini 0.926 recall 0.688: 0.806783\tvalid_1's binary_logloss: 0.276793\tvalid_1's amex_metric gini 0.917 recall 0.652: 0.784223\n",
      "[800]\ttraining's binary_logloss: 0.248732\ttraining's amex_metric gini 0.928 recall 0.695: 0.811098\tvalid_1's binary_logloss: 0.256684\tvalid_1's amex_metric gini 0.918 recall 0.657: 0.787324\n",
      "[900]\ttraining's binary_logloss: 0.233053\ttraining's amex_metric gini 0.930 recall 0.702: 0.816039\tvalid_1's binary_logloss: 0.242107\tvalid_1's amex_metric gini 0.919 recall 0.665: 0.792072\n",
      "[1000]\ttraining's binary_logloss: 0.226394\ttraining's amex_metric gini 0.931 recall 0.707: 0.819083\tvalid_1's binary_logloss: 0.236154\tvalid_1's amex_metric gini 0.920 recall 0.667: 0.793794\n",
      "[1100]\ttraining's binary_logloss: 0.216284\ttraining's amex_metric gini 0.933 recall 0.712: 0.822383\tvalid_1's binary_logloss: 0.227138\tvalid_1's amex_metric gini 0.921 recall 0.669: 0.794984\n",
      "[1200]\ttraining's binary_logloss: 0.212874\ttraining's amex_metric gini 0.934 recall 0.717: 0.825206\tvalid_1's binary_logloss: 0.224306\tvalid_1's amex_metric gini 0.922 recall 0.670: 0.796181\n",
      "[1300]\ttraining's binary_logloss: 0.208732\ttraining's amex_metric gini 0.935 recall 0.721: 0.827942\tvalid_1's binary_logloss: 0.220859\tvalid_1's amex_metric gini 0.923 recall 0.673: 0.797787\n",
      "[1400]\ttraining's binary_logloss: 0.203977\ttraining's amex_metric gini 0.936 recall 0.726: 0.830976\tvalid_1's binary_logloss: 0.217035\tvalid_1's amex_metric gini 0.924 recall 0.676: 0.799815\n",
      "[1500]\ttraining's binary_logloss: 0.201638\ttraining's amex_metric gini 0.937 recall 0.729: 0.833205\tvalid_1's binary_logloss: 0.215354\tvalid_1's amex_metric gini 0.924 recall 0.677: 0.800795\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "lgb_valid = lgb.Dataset(X_test.reshape(X_test.shape[0], -1), y_test,)\n",
    "model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 1500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 79627, number of negative: 263514\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.282570 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 328368\n",
      "[LightGBM] [Info] Number of data points in the train set: 343141, number of used features: 2425\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.232053 -> initscore=-1.196753\n",
      "[LightGBM] [Info] Start training from score -1.196753\n",
      "[100]\ttraining's binary_logloss: 0.448219\ttraining's amex_metric gini 0.917 recall 0.652: 0.784676\tvalid_1's binary_logloss: 0.449789\tvalid_1's amex_metric gini 0.910 recall 0.628: 0.769098\n",
      "[200]\ttraining's binary_logloss: 0.431256\ttraining's amex_metric gini 0.919 recall 0.658: 0.788337\tvalid_1's binary_logloss: 0.433742\tvalid_1's amex_metric gini 0.911 recall 0.635: 0.772682\n",
      "[300]\ttraining's binary_logloss: 0.376935\ttraining's amex_metric gini 0.920 recall 0.666: 0.7932\tvalid_1's binary_logloss: 0.380447\tvalid_1's amex_metric gini 0.912 recall 0.640: 0.776354\n",
      "[400]\ttraining's binary_logloss: 0.351547\ttraining's amex_metric gini 0.921 recall 0.671: 0.796181\tvalid_1's binary_logloss: 0.355741\tvalid_1's amex_metric gini 0.913 recall 0.640: 0.776788\n",
      "[500]\ttraining's binary_logloss: 0.32007\ttraining's amex_metric gini 0.922 recall 0.676: 0.798998\tvalid_1's binary_logloss: 0.325111\tvalid_1's amex_metric gini 0.914 recall 0.645: 0.779358\n",
      "[600]\ttraining's binary_logloss: 0.30117\ttraining's amex_metric gini 0.924 recall 0.680: 0.801985\tvalid_1's binary_logloss: 0.30685\tvalid_1's amex_metric gini 0.915 recall 0.648: 0.78138\n",
      "[700]\ttraining's binary_logloss: 0.27066\ttraining's amex_metric gini 0.925 recall 0.686: 0.805923\tvalid_1's binary_logloss: 0.277409\tvalid_1's amex_metric gini 0.916 recall 0.652: 0.784031\n",
      "[800]\ttraining's binary_logloss: 0.249452\ttraining's amex_metric gini 0.927 recall 0.693: 0.810251\tvalid_1's binary_logloss: 0.25727\tvalid_1's amex_metric gini 0.918 recall 0.657: 0.787511\n",
      "[900]\ttraining's binary_logloss: 0.233784\ttraining's amex_metric gini 0.929 recall 0.701: 0.815059\tvalid_1's binary_logloss: 0.242608\tvalid_1's amex_metric gini 0.919 recall 0.663: 0.791174\n",
      "[1000]\ttraining's binary_logloss: 0.227146\ttraining's amex_metric gini 0.931 recall 0.705: 0.817753\tvalid_1's binary_logloss: 0.236659\tvalid_1's amex_metric gini 0.920 recall 0.664: 0.791886\n",
      "[1100]\ttraining's binary_logloss: 0.217053\ttraining's amex_metric gini 0.932 recall 0.711: 0.821314\tvalid_1's binary_logloss: 0.227594\tvalid_1's amex_metric gini 0.921 recall 0.667: 0.793959\n",
      "[1200]\ttraining's binary_logloss: 0.213654\ttraining's amex_metric gini 0.933 recall 0.715: 0.823997\tvalid_1's binary_logloss: 0.224717\tvalid_1's amex_metric gini 0.922 recall 0.670: 0.795732\n",
      "[1300]\ttraining's binary_logloss: 0.209549\ttraining's amex_metric gini 0.934 recall 0.720: 0.827082\tvalid_1's binary_logloss: 0.221298\tvalid_1's amex_metric gini 0.923 recall 0.673: 0.79768\n",
      "[1400]\ttraining's binary_logloss: 0.20483\ttraining's amex_metric gini 0.936 recall 0.724: 0.829766\tvalid_1's binary_logloss: 0.217456\tvalid_1's amex_metric gini 0.923 recall 0.676: 0.799813\n",
      "[1500]\ttraining's binary_logloss: 0.202526\ttraining's amex_metric gini 0.937 recall 0.728: 0.832132\tvalid_1's binary_logloss: 0.215767\tvalid_1's amex_metric gini 0.924 recall 0.678: 0.800789\n",
      "[1600]\ttraining's binary_logloss: 0.197347\ttraining's amex_metric gini 0.938 recall 0.733: 0.835428\tvalid_1's binary_logloss: 0.211862\tvalid_1's amex_metric gini 0.925 recall 0.677: 0.800841\n",
      "[1700]\ttraining's binary_logloss: 0.194863\ttraining's amex_metric gini 0.939 recall 0.737: 0.83812\tvalid_1's binary_logloss: 0.210257\tvalid_1's amex_metric gini 0.925 recall 0.679: 0.801963\n",
      "[1800]\ttraining's binary_logloss: 0.193499\ttraining's amex_metric gini 0.940 recall 0.740: 0.840335\tvalid_1's binary_logloss: 0.209455\tvalid_1's amex_metric gini 0.926 recall 0.680: 0.802991\n",
      "[1900]\ttraining's binary_logloss: 0.191254\ttraining's amex_metric gini 0.941 recall 0.744: 0.842783\tvalid_1's binary_logloss: 0.208105\tvalid_1's amex_metric gini 0.926 recall 0.685: 0.805567\n",
      "[2000]\ttraining's binary_logloss: 0.188809\ttraining's amex_metric gini 0.942 recall 0.748: 0.845279\tvalid_1's binary_logloss: 0.206656\tvalid_1's amex_metric gini 0.927 recall 0.684: 0.805561\n",
      "[2100]\ttraining's binary_logloss: 0.187331\ttraining's amex_metric gini 0.943 recall 0.752: 0.847488\tvalid_1's binary_logloss: 0.20593\tvalid_1's amex_metric gini 0.927 recall 0.687: 0.806822\n",
      "[2200]\ttraining's binary_logloss: 0.186105\ttraining's amex_metric gini 0.944 recall 0.755: 0.849666\tvalid_1's binary_logloss: 0.205427\tvalid_1's amex_metric gini 0.927 recall 0.688: 0.807628\n",
      "[2300]\ttraining's binary_logloss: 0.184204\ttraining's amex_metric gini 0.945 recall 0.759: 0.85188\tvalid_1's binary_logloss: 0.2045\tvalid_1's amex_metric gini 0.928 recall 0.687: 0.807391\n",
      "[2400]\ttraining's binary_logloss: 0.183052\ttraining's amex_metric gini 0.946 recall 0.761: 0.853688\tvalid_1's binary_logloss: 0.204027\tvalid_1's amex_metric gini 0.928 recall 0.689: 0.808355\n",
      "[2500]\ttraining's binary_logloss: 0.181821\ttraining's amex_metric gini 0.947 recall 0.765: 0.855691\tvalid_1's binary_logloss: 0.203549\tvalid_1's amex_metric gini 0.928 recall 0.689: 0.808637\n",
      "[2600]\ttraining's binary_logloss: 0.180453\ttraining's amex_metric gini 0.948 recall 0.768: 0.857828\tvalid_1's binary_logloss: 0.20304\tvalid_1's amex_metric gini 0.929 recall 0.689: 0.808744\n",
      "[2700]\ttraining's binary_logloss: 0.178959\ttraining's amex_metric gini 0.948 recall 0.771: 0.859814\tvalid_1's binary_logloss: 0.202501\tvalid_1's amex_metric gini 0.929 recall 0.689: 0.808832\n",
      "[2800]\ttraining's binary_logloss: 0.178097\ttraining's amex_metric gini 0.949 recall 0.774: 0.861596\tvalid_1's binary_logloss: 0.202249\tvalid_1's amex_metric gini 0.929 recall 0.690: 0.809703\n",
      "[2900]\ttraining's binary_logloss: 0.176539\ttraining's amex_metric gini 0.950 recall 0.777: 0.863611\tvalid_1's binary_logloss: 0.201692\tvalid_1's amex_metric gini 0.929 recall 0.690: 0.809401\n",
      "[3000]\ttraining's binary_logloss: 0.174964\ttraining's amex_metric gini 0.951 recall 0.782: 0.866251\tvalid_1's binary_logloss: 0.201193\tvalid_1's amex_metric gini 0.929 recall 0.691: 0.81015\n",
      "[3100]\ttraining's binary_logloss: 0.173792\ttraining's amex_metric gini 0.952 recall 0.785: 0.868207\tvalid_1's binary_logloss: 0.200895\tvalid_1's amex_metric gini 0.929 recall 0.690: 0.809933\n",
      "[3200]\ttraining's binary_logloss: 0.172521\ttraining's amex_metric gini 0.952 recall 0.788: 0.870013\tvalid_1's binary_logloss: 0.200558\tvalid_1's amex_metric gini 0.930 recall 0.693: 0.811435\n",
      "[3300]\ttraining's binary_logloss: 0.171379\ttraining's amex_metric gini 0.953 recall 0.792: 0.872294\tvalid_1's binary_logloss: 0.200269\tvalid_1's amex_metric gini 0.930 recall 0.693: 0.811417\n",
      "[3400]\ttraining's binary_logloss: 0.16983\ttraining's amex_metric gini 0.954 recall 0.794: 0.874094\tvalid_1's binary_logloss: 0.199865\tvalid_1's amex_metric gini 0.930 recall 0.694: 0.811991\n",
      "[3500]\ttraining's binary_logloss: 0.168705\ttraining's amex_metric gini 0.955 recall 0.797: 0.876048\tvalid_1's binary_logloss: 0.199613\tvalid_1's amex_metric gini 0.930 recall 0.693: 0.811726\n",
      "[3600]\ttraining's binary_logloss: 0.167583\ttraining's amex_metric gini 0.955 recall 0.800: 0.877942\tvalid_1's binary_logloss: 0.199398\tvalid_1's amex_metric gini 0.930 recall 0.694: 0.812351\n",
      "[3700]\ttraining's binary_logloss: 0.166016\ttraining's amex_metric gini 0.956 recall 0.805: 0.880359\tvalid_1's binary_logloss: 0.199031\tvalid_1's amex_metric gini 0.930 recall 0.695: 0.812519\n",
      "[3800]\ttraining's binary_logloss: 0.165004\ttraining's amex_metric gini 0.957 recall 0.808: 0.882269\tvalid_1's binary_logloss: 0.198886\tvalid_1's amex_metric gini 0.930 recall 0.694: 0.812473\n",
      "[3900]\ttraining's binary_logloss: 0.164098\ttraining's amex_metric gini 0.958 recall 0.810: 0.88388\tvalid_1's binary_logloss: 0.198783\tvalid_1's amex_metric gini 0.931 recall 0.695: 0.812617\n",
      "[4000]\ttraining's binary_logloss: 0.163218\ttraining's amex_metric gini 0.958 recall 0.812: 0.885381\tvalid_1's binary_logloss: 0.198658\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813273\n",
      "[4100]\ttraining's binary_logloss: 0.16215\ttraining's amex_metric gini 0.959 recall 0.816: 0.887283\tvalid_1's binary_logloss: 0.198475\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813629\n",
      "[4200]\ttraining's binary_logloss: 0.160835\ttraining's amex_metric gini 0.960 recall 0.819: 0.889121\tvalid_1's binary_logloss: 0.198231\tvalid_1's amex_metric gini 0.931 recall 0.697: 0.813891\n",
      "[4300]\ttraining's binary_logloss: 0.159509\ttraining's amex_metric gini 0.960 recall 0.821: 0.890892\tvalid_1's binary_logloss: 0.198029\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813267\n",
      "[4400]\ttraining's binary_logloss: 0.158864\ttraining's amex_metric gini 0.961 recall 0.824: 0.892585\tvalid_1's binary_logloss: 0.198005\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813655\n",
      "[4500]\ttraining's binary_logloss: 0.157824\ttraining's amex_metric gini 0.962 recall 0.827: 0.894472\tvalid_1's binary_logloss: 0.19787\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813498\n",
      "[4600]\ttraining's binary_logloss: 0.156364\ttraining's amex_metric gini 0.962 recall 0.831: 0.896579\tvalid_1's binary_logloss: 0.197663\tvalid_1's amex_metric gini 0.931 recall 0.695: 0.813128\n",
      "[4700]\ttraining's binary_logloss: 0.155542\ttraining's amex_metric gini 0.963 recall 0.833: 0.897946\tvalid_1's binary_logloss: 0.197611\tvalid_1's amex_metric gini 0.931 recall 0.697: 0.814019\n",
      "[4800]\ttraining's binary_logloss: 0.154513\ttraining's amex_metric gini 0.964 recall 0.835: 0.899422\tvalid_1's binary_logloss: 0.197513\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813884\n",
      "[4900]\ttraining's binary_logloss: 0.153318\ttraining's amex_metric gini 0.964 recall 0.839: 0.901451\tvalid_1's binary_logloss: 0.19737\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813916\n",
      "[5000]\ttraining's binary_logloss: 0.152468\ttraining's amex_metric gini 0.965 recall 0.841: 0.90316\tvalid_1's binary_logloss: 0.197337\tvalid_1's amex_metric gini 0.931 recall 0.696: 0.813829\n",
      "[5100]\ttraining's binary_logloss: 0.151654\ttraining's amex_metric gini 0.966 recall 0.844: 0.904818\tvalid_1's binary_logloss: 0.197311\tvalid_1's amex_metric gini 0.931 recall 0.697: 0.814249\n",
      "[5200]\ttraining's binary_logloss: 0.150506\ttraining's amex_metric gini 0.966 recall 0.846: 0.906198\tvalid_1's binary_logloss: 0.197176\tvalid_1's amex_metric gini 0.931 recall 0.697: 0.814173\n",
      "[5300]\ttraining's binary_logloss: 0.149871\ttraining's amex_metric gini 0.967 recall 0.849: 0.907685\tvalid_1's binary_logloss: 0.197135\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.814603\n",
      "[5400]\ttraining's binary_logloss: 0.148844\ttraining's amex_metric gini 0.967 recall 0.851: 0.909182\tvalid_1's binary_logloss: 0.197064\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814107\n",
      "[5500]\ttraining's binary_logloss: 0.147741\ttraining's amex_metric gini 0.968 recall 0.854: 0.910914\tvalid_1's binary_logloss: 0.196968\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813928\n",
      "[5600]\ttraining's binary_logloss: 0.146916\ttraining's amex_metric gini 0.969 recall 0.856: 0.912479\tvalid_1's binary_logloss: 0.19694\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813992\n",
      "[5700]\ttraining's binary_logloss: 0.145985\ttraining's amex_metric gini 0.969 recall 0.859: 0.914123\tvalid_1's binary_logloss: 0.196857\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813969\n",
      "[5800]\ttraining's binary_logloss: 0.145286\ttraining's amex_metric gini 0.970 recall 0.862: 0.915715\tvalid_1's binary_logloss: 0.196794\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813993\n",
      "[5900]\ttraining's binary_logloss: 0.144292\ttraining's amex_metric gini 0.970 recall 0.864: 0.917159\tvalid_1's binary_logloss: 0.196722\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813697\n",
      "[6000]\ttraining's binary_logloss: 0.1437\ttraining's amex_metric gini 0.971 recall 0.866: 0.918442\tvalid_1's binary_logloss: 0.196749\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813693\n",
      "[6100]\ttraining's binary_logloss: 0.14291\ttraining's amex_metric gini 0.971 recall 0.868: 0.919693\tvalid_1's binary_logloss: 0.196761\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813887\n",
      "[6200]\ttraining's binary_logloss: 0.142105\ttraining's amex_metric gini 0.972 recall 0.870: 0.920952\tvalid_1's binary_logloss: 0.196696\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813746\n",
      "[6300]\ttraining's binary_logloss: 0.141168\ttraining's amex_metric gini 0.972 recall 0.873: 0.922389\tvalid_1's binary_logloss: 0.196592\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814178\n",
      "[6400]\ttraining's binary_logloss: 0.14041\ttraining's amex_metric gini 0.973 recall 0.875: 0.923694\tvalid_1's binary_logloss: 0.196545\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.81404\n",
      "[6500]\ttraining's binary_logloss: 0.139416\ttraining's amex_metric gini 0.973 recall 0.878: 0.925287\tvalid_1's binary_logloss: 0.196482\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814358\n",
      "[6600]\ttraining's binary_logloss: 0.138329\ttraining's amex_metric gini 0.974 recall 0.880: 0.9266\tvalid_1's binary_logloss: 0.196403\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814326\n",
      "[6700]\ttraining's binary_logloss: 0.137349\ttraining's amex_metric gini 0.974 recall 0.882: 0.928184\tvalid_1's binary_logloss: 0.196311\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.813795\n",
      "[6800]\ttraining's binary_logloss: 0.136278\ttraining's amex_metric gini 0.975 recall 0.885: 0.929704\tvalid_1's binary_logloss: 0.196285\tvalid_1's amex_metric gini 0.932 recall 0.695: 0.813693\n",
      "[6900]\ttraining's binary_logloss: 0.135398\ttraining's amex_metric gini 0.975 recall 0.887: 0.931257\tvalid_1's binary_logloss: 0.196261\tvalid_1's amex_metric gini 0.932 recall 0.695: 0.81365\n",
      "[7000]\ttraining's binary_logloss: 0.134489\ttraining's amex_metric gini 0.976 recall 0.889: 0.932541\tvalid_1's binary_logloss: 0.196256\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.8138\n",
      "[7100]\ttraining's binary_logloss: 0.133419\ttraining's amex_metric gini 0.976 recall 0.892: 0.934224\tvalid_1's binary_logloss: 0.196192\tvalid_1's amex_metric gini 0.932 recall 0.695: 0.813566\n",
      "[7200]\ttraining's binary_logloss: 0.132464\ttraining's amex_metric gini 0.977 recall 0.894: 0.935466\tvalid_1's binary_logloss: 0.1961\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.814103\n",
      "[7300]\ttraining's binary_logloss: 0.131603\ttraining's amex_metric gini 0.977 recall 0.897: 0.936967\tvalid_1's binary_logloss: 0.196038\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814382\n",
      "[7400]\ttraining's binary_logloss: 0.130749\ttraining's amex_metric gini 0.978 recall 0.898: 0.938134\tvalid_1's binary_logloss: 0.196009\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814443\n",
      "[7500]\ttraining's binary_logloss: 0.129822\ttraining's amex_metric gini 0.978 recall 0.901: 0.939749\tvalid_1's binary_logloss: 0.195982\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.8144\n",
      "[7600]\ttraining's binary_logloss: 0.129026\ttraining's amex_metric gini 0.979 recall 0.903: 0.940951\tvalid_1's binary_logloss: 0.195907\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814679\n",
      "[7700]\ttraining's binary_logloss: 0.128354\ttraining's amex_metric gini 0.979 recall 0.905: 0.942205\tvalid_1's binary_logloss: 0.1959\tvalid_1's amex_metric gini 0.932 recall 0.699: 0.815391\n",
      "[7800]\ttraining's binary_logloss: 0.127573\ttraining's amex_metric gini 0.980 recall 0.907: 0.943461\tvalid_1's binary_logloss: 0.19587\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815244\n",
      "[7900]\ttraining's binary_logloss: 0.126702\ttraining's amex_metric gini 0.980 recall 0.909: 0.944639\tvalid_1's binary_logloss: 0.195888\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814626\n",
      "[8000]\ttraining's binary_logloss: 0.125672\ttraining's amex_metric gini 0.981 recall 0.911: 0.94601\tvalid_1's binary_logloss: 0.195867\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.814882\n",
      "[8100]\ttraining's binary_logloss: 0.124961\ttraining's amex_metric gini 0.981 recall 0.913: 0.947205\tvalid_1's binary_logloss: 0.195846\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.81469\n",
      "[8200]\ttraining's binary_logloss: 0.124027\ttraining's amex_metric gini 0.981 recall 0.915: 0.948331\tvalid_1's binary_logloss: 0.195852\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814631\n",
      "[8300]\ttraining's binary_logloss: 0.123368\ttraining's amex_metric gini 0.982 recall 0.917: 0.94955\tvalid_1's binary_logloss: 0.195848\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.81433\n",
      "[8400]\ttraining's binary_logloss: 0.122757\ttraining's amex_metric gini 0.982 recall 0.919: 0.950578\tvalid_1's binary_logloss: 0.19581\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814396\n",
      "[8500]\ttraining's binary_logloss: 0.122117\ttraining's amex_metric gini 0.983 recall 0.921: 0.951685\tvalid_1's binary_logloss: 0.195821\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814748\n",
      "[8600]\ttraining's binary_logloss: 0.121356\ttraining's amex_metric gini 0.983 recall 0.922: 0.95253\tvalid_1's binary_logloss: 0.19579\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814756\n",
      "[8700]\ttraining's binary_logloss: 0.120326\ttraining's amex_metric gini 0.983 recall 0.924: 0.953651\tvalid_1's binary_logloss: 0.195773\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814754\n",
      "[8800]\ttraining's binary_logloss: 0.119638\ttraining's amex_metric gini 0.984 recall 0.926: 0.954753\tvalid_1's binary_logloss: 0.195732\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.81528\n",
      "[8900]\ttraining's binary_logloss: 0.118918\ttraining's amex_metric gini 0.984 recall 0.927: 0.955741\tvalid_1's binary_logloss: 0.195718\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814725\n",
      "[9000]\ttraining's binary_logloss: 0.118137\ttraining's amex_metric gini 0.984 recall 0.929: 0.956744\tvalid_1's binary_logloss: 0.195725\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815128\n",
      "[9100]\ttraining's binary_logloss: 0.117544\ttraining's amex_metric gini 0.985 recall 0.931: 0.957681\tvalid_1's binary_logloss: 0.195737\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815224\n",
      "[9200]\ttraining's binary_logloss: 0.116586\ttraining's amex_metric gini 0.985 recall 0.932: 0.958784\tvalid_1's binary_logloss: 0.195684\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815241\n",
      "[9300]\ttraining's binary_logloss: 0.115853\ttraining's amex_metric gini 0.986 recall 0.934: 0.959822\tvalid_1's binary_logloss: 0.195707\tvalid_1's amex_metric gini 0.932 recall 0.699: 0.815435\n",
      "[9400]\ttraining's binary_logloss: 0.115432\ttraining's amex_metric gini 0.986 recall 0.935: 0.960517\tvalid_1's binary_logloss: 0.195704\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815035\n",
      "[9500]\ttraining's binary_logloss: 0.114533\ttraining's amex_metric gini 0.986 recall 0.937: 0.961495\tvalid_1's binary_logloss: 0.195694\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814682\n",
      "[9600]\ttraining's binary_logloss: 0.113801\ttraining's amex_metric gini 0.987 recall 0.938: 0.962383\tvalid_1's binary_logloss: 0.195673\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815198\n",
      "[9700]\ttraining's binary_logloss: 0.113068\ttraining's amex_metric gini 0.987 recall 0.940: 0.963187\tvalid_1's binary_logloss: 0.195663\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814847\n",
      "[9800]\ttraining's binary_logloss: 0.112332\ttraining's amex_metric gini 0.987 recall 0.941: 0.964128\tvalid_1's binary_logloss: 0.195693\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815091\n",
      "[9900]\ttraining's binary_logloss: 0.111635\ttraining's amex_metric gini 0.987 recall 0.943: 0.965088\tvalid_1's binary_logloss: 0.195677\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815096\n",
      "[10000]\ttraining's binary_logloss: 0.111035\ttraining's amex_metric gini 0.988 recall 0.944: 0.965822\tvalid_1's binary_logloss: 0.195639\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814805\n",
      "[10100]\ttraining's binary_logloss: 0.110667\ttraining's amex_metric gini 0.988 recall 0.945: 0.966596\tvalid_1's binary_logloss: 0.195666\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.814391\n",
      "[10200]\ttraining's binary_logloss: 0.110053\ttraining's amex_metric gini 0.988 recall 0.946: 0.967055\tvalid_1's binary_logloss: 0.195645\tvalid_1's amex_metric gini 0.932 recall 0.698: 0.815258\n",
      "[10300]\ttraining's binary_logloss: 0.109334\ttraining's amex_metric gini 0.989 recall 0.947: 0.967817\tvalid_1's binary_logloss: 0.195633\tvalid_1's amex_metric gini 0.932 recall 0.699: 0.815412\n",
      "[10400]\ttraining's binary_logloss: 0.108713\ttraining's amex_metric gini 0.989 recall 0.948: 0.968546\tvalid_1's binary_logloss: 0.195641\tvalid_1's amex_metric gini 0.932 recall 0.696: 0.814194\n",
      "[10500]\ttraining's binary_logloss: 0.107965\ttraining's amex_metric gini 0.989 recall 0.950: 0.969371\tvalid_1's binary_logloss: 0.195627\tvalid_1's amex_metric gini 0.932 recall 0.697: 0.814504\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "lgb_valid = lgb.Dataset(X_test.reshape(X_test.shape[0], -1), y_test,)\n",
    "model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/nimamanaf/Desktop/kaggle/pd/data/out/lgbm13.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "joblib.dump(model, OUTDIR+f'lgbm13.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train, test=None, n_folds=5, seed=42):\n",
    "    cat_features = [f\"{cf}_last\" for cf in CATCOLS]\n",
    "    for cat_col in cat_features:\n",
    "        encoder = LabelEncoder()\n",
    "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
    "        if test is not None:\n",
    "            test[cat_col] = encoder.transform(test[cat_col])\n",
    "    # Round last float features to 2 decimal place\n",
    "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
    "    num_cols = [col for col in num_cols if 'last' in col]\n",
    "    for col in num_cols:\n",
    "        train[col + '_round2'] = train[col].round(2)\n",
    "        if test is not None:\n",
    "            test[col + '_round2'] = test[col].round(2)\n",
    "    # Get feature list\n",
    "    features = [col for col in train.columns if col not in ['customer_ID', \"S_2\", \"target\"]]\n",
    "    \n",
    "    # Create a numpy array to store test predictions\n",
    "    test_predictions = np.zeros(len(test))\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    oof_predictions = np.zeros(len(train))\n",
    "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[\"target\"])):\n",
    "        print(' ')\n",
    "        print('-'*50)\n",
    "        print(f'Training fold {fold} with {len(features)} features...')\n",
    "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
    "        y_train, y_val = train[\"target\"].iloc[trn_ind], train[\"target\"].iloc[val_ind]\n",
    "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
    "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
    "        model = lgb.train(\n",
    "            params = params,\n",
    "            train_set = lgb_train,\n",
    "            num_boost_round = 10500,\n",
    "            valid_sets = [lgb_train, lgb_valid],\n",
    "            early_stopping_rounds = 100,\n",
    "            verbose_eval = 100,\n",
    "            feval = lgb_amex_metric\n",
    "            )\n",
    "        # Save best model\n",
    "        joblib.dump(model, OUTDIR+f'Models/lgbm_fold{fold}_seed{seed}.pkl')\n",
    "        val_pred = model.predict(x_val) # Predict validation\n",
    "        oof_predictions[val_ind] = val_pred  # Add to out of folds array\n",
    "        if test is not None:\n",
    "            test_pred = model.predict(test[features]) # Predict the test set\n",
    "            test_predictions += test_pred/n_folds\n",
    "        # Compute fold metric\n",
    "        score = amex_metric(y_val, val_pred)\n",
    "        print(f'Our fold {fold} CV score is {score}')\n",
    "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "        gc.collect()\n",
    "    score = amex_metric(train[\"target\"], oof_predictions)  # Compute out of folds metric\n",
    "    print(f'Our out of folds CV score is {score}')\n",
    "    # Create a dataframe to store out of folds predictions\n",
    "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[\"target\"], 'prediction': oof_predictions})\n",
    "    oof_df.to_csv(OUTDIR+f'oof_lgbm_baseline_{n_folds}fold_seed{seed}.csv', index=False)\n",
    "    # Create a dataframe to store test prediction\n",
    "    if test is not None:\n",
    "        test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
    "        test_df.to_csv(f'/content/drive/MyDrive/Amex/Predictions/test_lgbm_baseline_{n_folds}fold_seed{seed}.csv', index = False)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('q')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05e50049e3eb32775174019135b7208a0d3852fb22829b3658213f387a3fdcbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
