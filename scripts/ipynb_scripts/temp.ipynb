{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pd.nn.conv import Conv\n",
    "\n",
    "from pd.metric import amex_metric\n",
    "from pd.data.loader import CustomerData, DataLoader\n",
    "from pd.params import *\n",
    "from pd.pred import pred_test_npy\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet(DATADIR+\"train_data.parquet\")\n",
    "train_labels = pd.read_csv(DATADIR+\"train_labels.csv\")\n",
    "train_labels.set_index(\"customer_ID\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_customers = train_data.customer_ID\n",
    "train_count =  train_customers.value_counts()\n",
    "train_c13 = train_count[train_count==13].index\n",
    "train_data = train_data[train_data.customer_ID.isin(train_c13)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cat_agg = train_data.groupby(\"customer_ID\")[CATCOLS].agg(['count', 'last', 'nunique'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458913, 33)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cat_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D_114\n",
      "D_116\n",
      "D_120\n",
      "D_63\n",
      "D_64\n",
      "D_66\n",
      "D_68\n"
     ]
    }
   ],
   "source": [
    "for c in CATCOLS:\n",
    "    if c in betterTransFeatsK79:\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B_30',\n",
       " 'B_38',\n",
       " 'D_114',\n",
       " 'D_116',\n",
       " 'D_117',\n",
       " 'D_120',\n",
       " 'D_126',\n",
       " 'D_63',\n",
       " 'D_64',\n",
       " 'D_66',\n",
       " 'D_68']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATCOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t2/_nykd9tn43dfx142nv5r8l900000gn/T/ipykernel_26043/888282133.py:5: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope = 2.944/(col_info13[c][\"q95\"] - mid_point)\n"
     ]
    }
   ],
   "source": [
    "c = \"R_13\"\n",
    "d = train_data[\"R_13\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ((d - col_info13[\"R_13\"][f\"mean_{lb}_{ub}\"])/col_info13[\"R_13\"][f\"std_{lb}_{ub}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "_ = plt.hist(norm.cdf(a), bins=100)\n",
    "_ = plt.hist(1/(1 + np.exp(slope*(mid_point - d))), bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c13' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/temp.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/temp.ipynb#ch0000003?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m train_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/temp.ipynb#ch0000003?line=3'>4</a>\u001b[0m     train_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(TRAINDATA)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/temp.ipynb#ch0000003?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m c13:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/temp.ipynb#ch0000003?line=5'>6</a>\u001b[0m     train_customers \u001b[39m=\u001b[39m train_data\u001b[39m.\u001b[39mcustomer_ID\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nimamanaf/Desktop/kaggle/pd/pd/scripts/ipynb_scripts/temp.ipynb#ch0000003?line=6'>7</a>\u001b[0m     train_count \u001b[39m=\u001b[39m  train_customers\u001b[39m.\u001b[39mvalue_counts()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'c13' is not defined"
     ]
    }
   ],
   "source": [
    "    cols = featureCols\n",
    "\n",
    "    if train_data is None:\n",
    "        train_data = pd.read_parquet(TRAINDATA)\n",
    "    if c13:\n",
    "        train_customers = train_data.customer_ID\n",
    "        train_count =  train_customers.value_counts()\n",
    "        train_c13 = train_count[train_count==13].index\n",
    "        train_data = train_data[train_data.customer_ID.isin(train_c13)]\n",
    "        col_info_name = f\"{col_info_name}13\"\n",
    "\n",
    "    for cat_col in CATCOLS:\n",
    "        encoder = LabelEncoder()\n",
    "        train_data[cat_col] = encoder.fit_transform(train_data[cat_col])\n",
    "    \n",
    "    col_info = {}\n",
    "    for c in cols:\n",
    "        col_info[c] = {}\n",
    "        d = train_data[c]\n",
    "        q2 = d.quantile(0.02)\n",
    "        q98 = d.quantile(0.98)\n",
    "        q5 = d.quantile(0.05)\n",
    "        q95 = d.quantile(0.95)\n",
    "        \n",
    "        col_min_val = d.min()\n",
    "        col_max_val = d.max()\n",
    "        hist = np.histogram(d, range=[q2, q98], density=True, bins=100)\n",
    "        \n",
    "        col_info[c][\"num_nan\"] = 1 - d.dropna().shape[0]/d.shape[0]\n",
    "        col_info[c][\"q2\"] = q2\n",
    "        col_info[c][\"q98\"] = q98\n",
    "        col_info[c][\"q1\"] = d.quantile(0.01)\n",
    "        col_info[c][\"q99\"] = d.quantile(0.99)\n",
    "        \n",
    "        col_info[c][\"min\"] = col_min_val\n",
    "        col_info[c][\"max\"] = col_max_val\n",
    "        col_info[c][\"mean\"] = d.mean()\n",
    "        col_info[c][\"median\"] = d.quantile(0.5)\n",
    "        col_info[c][\"hist\"] = hist\n",
    "        col_info[c][\"max_prob_mass\"] = hist[0].max()\n",
    "        col_info[c][\"num_nonzero_bins\"] = np.count_nonzero(hist[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nzs = []\n",
    "dist_col_27 = []\n",
    "for c in col_info.keys():\n",
    "    nz = np.count_nonzero(col_info[c][\"hist\"][0])\n",
    "    nzs.append(nz)\n",
    "    if nz < 27:\n",
    "        dist_col_27.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Conv Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with aggregation over the features\n",
    "\n",
    "### Mean aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bes.nn.es_module import ESModule\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(ESModule):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=128,):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.nf1 = nn.LayerNorm([hidden_dim])\n",
    "        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.nf2 = nn.LayerNorm([hidden_dim])\n",
    "        self.fc3 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.nf3 = nn.LayerNorm([hidden_dim])\n",
    "        self.fc4 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.nf4 = nn.LayerNorm([hidden_dim])\n",
    "        self.fc5 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.nf5 = nn.LayerNorm([hidden_dim])\n",
    "        \n",
    "        self.fcout = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "    \n",
    "    def forward(self, h, return_featues=False):\n",
    "        h = F.selu(self.fc1(h))\n",
    "        r = self.nf1(h)\n",
    "        h = F.selu(self.fc2(r))\n",
    "        h = self.nf2(h)\n",
    "        h = F.selu(self.fc3(h))\n",
    "        r = self.nf3(h+r)\n",
    "        h = F.selu(self.fc4(r))\n",
    "        h = self.nf4(h)\n",
    "        h = F.selu(self.fc5(h))\n",
    "        h = self.nf5(h+r)\n",
    "        if return_featues:\n",
    "            return torch.sigmoid(self.fcout(h)), h\n",
    "        \n",
    "        return torch.sigmoid(self.fcout(h))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the sum of the of the data and normalize by the max vals \n",
    "X = train_data.sum(axis=1) \n",
    "for idx, c in enumerate(dist_col_27):\n",
    "    X[:, idx] = X[:, idx]/col_info[c][\"max\"]\n",
    "\n",
    "X = X/X.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, train_labels, test_size=1/9, random_state=0, shuffle=True)\n",
    "validation_data = (X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomerData(X_train, train_labels=y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, BCE loss: 0.660, amex train: -0.043, val 0.000\n",
      "0, BCE loss: 0.618, amex train: 0.281, val 0.000\n",
      "0, BCE loss: 0.586, amex train: 0.452, val 0.000\n",
      "0, BCE loss: 0.493, amex train: 0.490, val 0.000\n",
      "0, BCE loss: 0.449, amex train: 0.490, val 0.000\n",
      "0, BCE loss: 0.468, amex train: 0.505, val 0.000\n",
      "0, BCE loss: 0.453, amex train: 0.525, val 0.000\n",
      "0, BCE loss: 0.414, amex train: 0.529, val 0.000\n",
      "0, BCE loss: 0.385, amex train: 0.536, val 0.000\n",
      "0, BCE loss: 0.383, amex train: 0.567, val 0.000\n",
      "0, BCE loss: 0.388, amex train: 0.553, val 0.000\n",
      "0, BCE loss: 0.377, amex train: 0.570, val 0.000\n",
      "0, BCE loss: 0.348, amex train: 0.588, val 0.000\n",
      "0, BCE loss: 0.355, amex train: 0.580, val 0.000\n",
      "0, BCE loss: 0.361, amex train: 0.585, val 0.000\n",
      "0, BCE loss: 0.373, amex train: 0.590, val 0.000\n",
      "0, BCE loss: 0.348, amex train: 0.607, val 0.000\n",
      "0, BCE loss: 0.345, amex train: 0.608, val 0.000\n",
      "0, BCE loss: 0.343, amex train: 0.612, val 0.000\n",
      "0, BCE loss: 0.347, amex train: 0.605, val 0.000\n",
      "0, BCE loss: 0.346, amex train: 0.615, val 0.000\n",
      "1, BCE loss: 0.336, amex train: 0.621, val 0.000\n",
      "1, BCE loss: 0.322, amex train: 0.641, val 0.000\n",
      "1, BCE loss: 0.328, amex train: 0.637, val 0.000\n",
      "1, BCE loss: 0.327, amex train: 0.639, val 0.000\n",
      "1, BCE loss: 0.327, amex train: 0.634, val 0.000\n",
      "1, BCE loss: 0.325, amex train: 0.626, val 0.000\n",
      "1, BCE loss: 0.319, amex train: 0.642, val 0.000\n",
      "1, BCE loss: 0.321, amex train: 0.640, val 0.000\n",
      "1, BCE loss: 0.320, amex train: 0.636, val 0.000\n",
      "1, BCE loss: 0.324, amex train: 0.645, val 0.000\n",
      "1, BCE loss: 0.323, amex train: 0.634, val 0.000\n",
      "1, BCE loss: 0.318, amex train: 0.642, val 0.000\n",
      "1, BCE loss: 0.312, amex train: 0.650, val 0.000\n",
      "1, BCE loss: 0.320, amex train: 0.643, val 0.000\n",
      "1, BCE loss: 0.315, amex train: 0.644, val 0.000\n",
      "1, BCE loss: 0.323, amex train: 0.639, val 0.000\n",
      "1, BCE loss: 0.309, amex train: 0.658, val 0.000\n",
      "1, BCE loss: 0.317, amex train: 0.655, val 0.000\n",
      "1, BCE loss: 0.312, amex train: 0.655, val 0.000\n",
      "1, BCE loss: 0.315, amex train: 0.641, val 0.000\n",
      "1, BCE loss: 0.314, amex train: 0.651, val 0.000\n",
      "2, BCE loss: 0.314, amex train: 0.644, val 0.000\n",
      "2, BCE loss: 0.301, amex train: 0.672, val 0.000\n",
      "2, BCE loss: 0.306, amex train: 0.662, val 0.000\n",
      "2, BCE loss: 0.302, amex train: 0.662, val 0.000\n",
      "2, BCE loss: 0.303, amex train: 0.655, val 0.000\n",
      "2, BCE loss: 0.305, amex train: 0.650, val 0.000\n",
      "2, BCE loss: 0.301, amex train: 0.664, val 0.000\n",
      "2, BCE loss: 0.301, amex train: 0.665, val 0.000\n",
      "2, BCE loss: 0.300, amex train: 0.657, val 0.000\n",
      "2, BCE loss: 0.303, amex train: 0.665, val 0.000\n",
      "2, BCE loss: 0.307, amex train: 0.651, val 0.000\n",
      "2, BCE loss: 0.302, amex train: 0.654, val 0.000\n",
      "2, BCE loss: 0.294, amex train: 0.665, val 0.000\n",
      "2, BCE loss: 0.303, amex train: 0.658, val 0.000\n",
      "2, BCE loss: 0.301, amex train: 0.660, val 0.000\n",
      "2, BCE loss: 0.308, amex train: 0.656, val 0.000\n",
      "2, BCE loss: 0.294, amex train: 0.676, val 0.000\n",
      "2, BCE loss: 0.297, amex train: 0.670, val 0.000\n",
      "2, BCE loss: 0.297, amex train: 0.671, val 0.000\n",
      "2, BCE loss: 0.301, amex train: 0.653, val 0.000\n",
      "2, BCE loss: 0.298, amex train: 0.674, val 0.000\n",
      "3, BCE loss: 0.300, amex train: 0.662, val 0.000\n",
      "3, BCE loss: 0.290, amex train: 0.678, val 0.000\n",
      "3, BCE loss: 0.295, amex train: 0.671, val 0.000\n",
      "3, BCE loss: 0.289, amex train: 0.676, val 0.000\n",
      "3, BCE loss: 0.291, amex train: 0.670, val 0.000\n",
      "3, BCE loss: 0.294, amex train: 0.665, val 0.000\n",
      "3, BCE loss: 0.291, amex train: 0.678, val 0.000\n",
      "3, BCE loss: 0.290, amex train: 0.679, val 0.000\n",
      "3, BCE loss: 0.289, amex train: 0.670, val 0.000\n",
      "3, BCE loss: 0.291, amex train: 0.672, val 0.000\n",
      "3, BCE loss: 0.297, amex train: 0.663, val 0.000\n",
      "3, BCE loss: 0.292, amex train: 0.664, val 0.000\n",
      "3, BCE loss: 0.286, amex train: 0.676, val 0.000\n",
      "3, BCE loss: 0.295, amex train: 0.666, val 0.000\n",
      "3, BCE loss: 0.292, amex train: 0.674, val 0.000\n",
      "3, BCE loss: 0.298, amex train: 0.668, val 0.000\n",
      "3, BCE loss: 0.286, amex train: 0.683, val 0.000\n",
      "3, BCE loss: 0.287, amex train: 0.678, val 0.000\n",
      "3, BCE loss: 0.288, amex train: 0.683, val 0.000\n",
      "3, BCE loss: 0.293, amex train: 0.667, val 0.000\n",
      "3, BCE loss: 0.288, amex train: 0.685, val 0.000\n",
      "4, BCE loss: 0.292, amex train: 0.672, val 0.000\n",
      "4, BCE loss: 0.283, amex train: 0.688, val 0.000\n",
      "4, BCE loss: 0.287, amex train: 0.687, val 0.000\n",
      "4, BCE loss: 0.281, amex train: 0.686, val 0.000\n",
      "4, BCE loss: 0.283, amex train: 0.683, val 0.000\n",
      "4, BCE loss: 0.285, amex train: 0.676, val 0.000\n",
      "4, BCE loss: 0.285, amex train: 0.688, val 0.000\n",
      "4, BCE loss: 0.284, amex train: 0.687, val 0.000\n",
      "4, BCE loss: 0.283, amex train: 0.682, val 0.000\n",
      "4, BCE loss: 0.287, amex train: 0.687, val 0.000\n",
      "4, BCE loss: 0.289, amex train: 0.674, val 0.000\n",
      "4, BCE loss: 0.288, amex train: 0.674, val 0.000\n",
      "4, BCE loss: 0.280, amex train: 0.684, val 0.000\n",
      "4, BCE loss: 0.291, amex train: 0.677, val 0.000\n",
      "4, BCE loss: 0.285, amex train: 0.683, val 0.000\n",
      "4, BCE loss: 0.295, amex train: 0.678, val 0.000\n",
      "4, BCE loss: 0.279, amex train: 0.691, val 0.000\n",
      "4, BCE loss: 0.287, amex train: 0.688, val 0.000\n",
      "4, BCE loss: 0.284, amex train: 0.692, val 0.000\n",
      "4, BCE loss: 0.292, amex train: 0.678, val 0.000\n",
      "4, BCE loss: 0.283, amex train: 0.682, val 0.000\n",
      "5, BCE loss: 0.290, amex train: 0.678, val 0.000\n",
      "5, BCE loss: 0.278, amex train: 0.694, val 0.000\n",
      "5, BCE loss: 0.284, amex train: 0.698, val 0.000\n",
      "5, BCE loss: 0.276, amex train: 0.694, val 0.000\n",
      "5, BCE loss: 0.281, amex train: 0.690, val 0.000\n",
      "5, BCE loss: 0.280, amex train: 0.686, val 0.000\n",
      "5, BCE loss: 0.282, amex train: 0.693, val 0.000\n",
      "5, BCE loss: 0.280, amex train: 0.693, val 0.000\n",
      "5, BCE loss: 0.280, amex train: 0.686, val 0.000\n",
      "5, BCE loss: 0.282, amex train: 0.692, val 0.000\n",
      "5, BCE loss: 0.286, amex train: 0.682, val 0.000\n",
      "5, BCE loss: 0.284, amex train: 0.677, val 0.000\n",
      "5, BCE loss: 0.276, amex train: 0.690, val 0.000\n",
      "5, BCE loss: 0.287, amex train: 0.681, val 0.000\n",
      "5, BCE loss: 0.281, amex train: 0.688, val 0.000\n",
      "5, BCE loss: 0.290, amex train: 0.687, val 0.000\n",
      "5, BCE loss: 0.275, amex train: 0.697, val 0.000\n",
      "5, BCE loss: 0.283, amex train: 0.695, val 0.000\n",
      "5, BCE loss: 0.280, amex train: 0.699, val 0.000\n",
      "5, BCE loss: 0.288, amex train: 0.685, val 0.000\n",
      "5, BCE loss: 0.280, amex train: 0.688, val 0.000\n",
      "6, BCE loss: 0.287, amex train: 0.681, val 0.000\n",
      "6, BCE loss: 0.276, amex train: 0.698, val 0.000\n",
      "6, BCE loss: 0.280, amex train: 0.703, val 0.000\n",
      "6, BCE loss: 0.274, amex train: 0.699, val 0.000\n",
      "6, BCE loss: 0.277, amex train: 0.695, val 0.000\n",
      "6, BCE loss: 0.277, amex train: 0.691, val 0.000\n",
      "6, BCE loss: 0.279, amex train: 0.696, val 0.000\n",
      "6, BCE loss: 0.278, amex train: 0.694, val 0.000\n",
      "6, BCE loss: 0.277, amex train: 0.691, val 0.000\n",
      "6, BCE loss: 0.279, amex train: 0.695, val 0.000\n",
      "6, BCE loss: 0.283, amex train: 0.687, val 0.000\n",
      "6, BCE loss: 0.280, amex train: 0.684, val 0.000\n",
      "6, BCE loss: 0.274, amex train: 0.695, val 0.000\n",
      "6, BCE loss: 0.283, amex train: 0.686, val 0.000\n",
      "6, BCE loss: 0.279, amex train: 0.695, val 0.000\n",
      "6, BCE loss: 0.285, amex train: 0.690, val 0.000\n",
      "6, BCE loss: 0.273, amex train: 0.700, val 0.000\n",
      "6, BCE loss: 0.278, amex train: 0.697, val 0.000\n",
      "6, BCE loss: 0.280, amex train: 0.702, val 0.000\n",
      "6, BCE loss: 0.282, amex train: 0.686, val 0.000\n",
      "6, BCE loss: 0.279, amex train: 0.693, val 0.000\n",
      "7, BCE loss: 0.283, amex train: 0.681, val 0.000\n",
      "7, BCE loss: 0.276, amex train: 0.703, val 0.000\n",
      "7, BCE loss: 0.276, amex train: 0.707, val 0.000\n",
      "7, BCE loss: 0.272, amex train: 0.702, val 0.000\n",
      "7, BCE loss: 0.273, amex train: 0.697, val 0.000\n",
      "7, BCE loss: 0.276, amex train: 0.695, val 0.000\n",
      "7, BCE loss: 0.276, amex train: 0.702, val 0.000\n",
      "7, BCE loss: 0.276, amex train: 0.697, val 0.000\n",
      "7, BCE loss: 0.276, amex train: 0.696, val 0.000\n",
      "7, BCE loss: 0.277, amex train: 0.699, val 0.000\n",
      "7, BCE loss: 0.280, amex train: 0.692, val 0.000\n",
      "7, BCE loss: 0.279, amex train: 0.688, val 0.000\n",
      "7, BCE loss: 0.272, amex train: 0.699, val 0.000\n",
      "7, BCE loss: 0.282, amex train: 0.689, val 0.000\n",
      "7, BCE loss: 0.277, amex train: 0.697, val 0.000\n",
      "7, BCE loss: 0.286, amex train: 0.693, val 0.000\n",
      "7, BCE loss: 0.271, amex train: 0.701, val 0.000\n",
      "7, BCE loss: 0.278, amex train: 0.701, val 0.000\n",
      "7, BCE loss: 0.277, amex train: 0.703, val 0.000\n",
      "7, BCE loss: 0.281, amex train: 0.689, val 0.000\n",
      "7, BCE loss: 0.276, amex train: 0.703, val 0.000\n",
      "8, BCE loss: 0.281, amex train: 0.682, val 0.000\n",
      "8, BCE loss: 0.275, amex train: 0.704, val 0.000\n",
      "8, BCE loss: 0.274, amex train: 0.710, val 0.000\n",
      "8, BCE loss: 0.271, amex train: 0.707, val 0.000\n",
      "8, BCE loss: 0.272, amex train: 0.700, val 0.000\n",
      "8, BCE loss: 0.274, amex train: 0.697, val 0.000\n",
      "8, BCE loss: 0.275, amex train: 0.704, val 0.000\n",
      "8, BCE loss: 0.275, amex train: 0.696, val 0.000\n",
      "8, BCE loss: 0.274, amex train: 0.697, val 0.000\n",
      "8, BCE loss: 0.275, amex train: 0.702, val 0.000\n",
      "8, BCE loss: 0.279, amex train: 0.695, val 0.000\n",
      "8, BCE loss: 0.278, amex train: 0.688, val 0.000\n",
      "8, BCE loss: 0.271, amex train: 0.702, val 0.000\n",
      "8, BCE loss: 0.280, amex train: 0.691, val 0.000\n",
      "8, BCE loss: 0.276, amex train: 0.696, val 0.000\n",
      "8, BCE loss: 0.284, amex train: 0.694, val 0.000\n",
      "8, BCE loss: 0.270, amex train: 0.703, val 0.000\n",
      "8, BCE loss: 0.276, amex train: 0.702, val 0.000\n",
      "8, BCE loss: 0.276, amex train: 0.704, val 0.000\n",
      "8, BCE loss: 0.280, amex train: 0.692, val 0.000\n",
      "8, BCE loss: 0.273, amex train: 0.704, val 0.000\n",
      "9, BCE loss: 0.280, amex train: 0.684, val 0.000\n",
      "9, BCE loss: 0.274, amex train: 0.706, val 0.000\n",
      "9, BCE loss: 0.272, amex train: 0.709, val 0.000\n",
      "9, BCE loss: 0.269, amex train: 0.710, val 0.000\n",
      "9, BCE loss: 0.271, amex train: 0.701, val 0.000\n",
      "9, BCE loss: 0.273, amex train: 0.700, val 0.000\n",
      "9, BCE loss: 0.274, amex train: 0.706, val 0.000\n",
      "9, BCE loss: 0.273, amex train: 0.699, val 0.000\n",
      "9, BCE loss: 0.273, amex train: 0.696, val 0.000\n",
      "9, BCE loss: 0.274, amex train: 0.705, val 0.000\n",
      "9, BCE loss: 0.277, amex train: 0.698, val 0.000\n",
      "9, BCE loss: 0.277, amex train: 0.693, val 0.000\n",
      "9, BCE loss: 0.269, amex train: 0.704, val 0.000\n",
      "9, BCE loss: 0.279, amex train: 0.692, val 0.000\n",
      "9, BCE loss: 0.275, amex train: 0.695, val 0.000\n",
      "9, BCE loss: 0.284, amex train: 0.695, val 0.000\n",
      "9, BCE loss: 0.269, amex train: 0.704, val 0.000\n",
      "9, BCE loss: 0.276, amex train: 0.703, val 0.000\n",
      "9, BCE loss: 0.275, amex train: 0.705, val 0.000\n",
      "9, BCE loss: 0.279, amex train: 0.693, val 0.000\n",
      "9, BCE loss: 0.272, amex train: 0.709, val 0.000\n",
      "10, BCE loss: 0.280, amex train: 0.685, val 0.000\n",
      "10, BCE loss: 0.273, amex train: 0.709, val 0.000\n",
      "10, BCE loss: 0.271, amex train: 0.711, val 0.000\n",
      "10, BCE loss: 0.268, amex train: 0.712, val 0.000\n",
      "10, BCE loss: 0.270, amex train: 0.703, val 0.000\n",
      "10, BCE loss: 0.272, amex train: 0.703, val 0.000\n",
      "10, BCE loss: 0.273, amex train: 0.709, val 0.000\n",
      "10, BCE loss: 0.272, amex train: 0.700, val 0.000\n",
      "10, BCE loss: 0.272, amex train: 0.697, val 0.000\n",
      "10, BCE loss: 0.273, amex train: 0.707, val 0.000\n",
      "10, BCE loss: 0.276, amex train: 0.700, val 0.000\n",
      "10, BCE loss: 0.276, amex train: 0.694, val 0.000\n",
      "10, BCE loss: 0.269, amex train: 0.707, val 0.000\n",
      "10, BCE loss: 0.278, amex train: 0.692, val 0.000\n",
      "10, BCE loss: 0.274, amex train: 0.696, val 0.000\n",
      "10, BCE loss: 0.282, amex train: 0.698, val 0.000\n",
      "10, BCE loss: 0.268, amex train: 0.707, val 0.000\n",
      "10, BCE loss: 0.274, amex train: 0.705, val 0.000\n",
      "10, BCE loss: 0.273, amex train: 0.708, val 0.000\n",
      "10, BCE loss: 0.278, amex train: 0.695, val 0.000\n",
      "10, BCE loss: 0.269, amex train: 0.713, val 0.000\n",
      "11, BCE loss: 0.279, amex train: 0.688, val 0.000\n",
      "11, BCE loss: 0.271, amex train: 0.707, val 0.000\n",
      "11, BCE loss: 0.270, amex train: 0.714, val 0.000\n",
      "11, BCE loss: 0.267, amex train: 0.713, val 0.000\n",
      "11, BCE loss: 0.269, amex train: 0.705, val 0.000\n",
      "11, BCE loss: 0.271, amex train: 0.706, val 0.000\n",
      "11, BCE loss: 0.272, amex train: 0.710, val 0.000\n",
      "11, BCE loss: 0.271, amex train: 0.702, val 0.000\n",
      "11, BCE loss: 0.272, amex train: 0.699, val 0.000\n",
      "11, BCE loss: 0.272, amex train: 0.709, val 0.000\n",
      "11, BCE loss: 0.275, amex train: 0.700, val 0.000\n",
      "11, BCE loss: 0.275, amex train: 0.697, val 0.000\n",
      "11, BCE loss: 0.267, amex train: 0.708, val 0.000\n",
      "11, BCE loss: 0.278, amex train: 0.694, val 0.000\n",
      "11, BCE loss: 0.273, amex train: 0.697, val 0.000\n",
      "11, BCE loss: 0.282, amex train: 0.696, val 0.000\n",
      "11, BCE loss: 0.267, amex train: 0.709, val 0.000\n",
      "11, BCE loss: 0.273, amex train: 0.706, val 0.000\n",
      "11, BCE loss: 0.273, amex train: 0.710, val 0.000\n",
      "11, BCE loss: 0.277, amex train: 0.696, val 0.000\n",
      "11, BCE loss: 0.269, amex train: 0.712, val 0.000\n",
      "12, BCE loss: 0.278, amex train: 0.689, val 0.000\n",
      "12, BCE loss: 0.271, amex train: 0.708, val 0.000\n",
      "12, BCE loss: 0.269, amex train: 0.715, val 0.000\n",
      "12, BCE loss: 0.267, amex train: 0.713, val 0.000\n",
      "12, BCE loss: 0.268, amex train: 0.706, val 0.000\n",
      "12, BCE loss: 0.270, amex train: 0.706, val 0.000\n",
      "12, BCE loss: 0.271, amex train: 0.714, val 0.000\n",
      "12, BCE loss: 0.270, amex train: 0.704, val 0.000\n",
      "12, BCE loss: 0.271, amex train: 0.701, val 0.000\n",
      "12, BCE loss: 0.271, amex train: 0.711, val 0.000\n",
      "12, BCE loss: 0.275, amex train: 0.701, val 0.000\n",
      "12, BCE loss: 0.274, amex train: 0.699, val 0.000\n",
      "12, BCE loss: 0.267, amex train: 0.708, val 0.000\n",
      "12, BCE loss: 0.276, amex train: 0.696, val 0.000\n",
      "12, BCE loss: 0.273, amex train: 0.699, val 0.000\n",
      "12, BCE loss: 0.280, amex train: 0.698, val 0.000\n",
      "12, BCE loss: 0.266, amex train: 0.709, val 0.000\n",
      "12, BCE loss: 0.272, amex train: 0.707, val 0.000\n",
      "12, BCE loss: 0.271, amex train: 0.710, val 0.000\n",
      "12, BCE loss: 0.276, amex train: 0.698, val 0.000\n",
      "12, BCE loss: 0.266, amex train: 0.717, val 0.000\n",
      "13, BCE loss: 0.277, amex train: 0.691, val 0.000\n",
      "13, BCE loss: 0.269, amex train: 0.710, val 0.000\n",
      "13, BCE loss: 0.268, amex train: 0.717, val 0.000\n",
      "13, BCE loss: 0.265, amex train: 0.713, val 0.000\n",
      "13, BCE loss: 0.267, amex train: 0.707, val 0.000\n",
      "13, BCE loss: 0.269, amex train: 0.708, val 0.000\n",
      "13, BCE loss: 0.270, amex train: 0.716, val 0.000\n",
      "13, BCE loss: 0.269, amex train: 0.706, val 0.000\n",
      "13, BCE loss: 0.270, amex train: 0.702, val 0.000\n",
      "13, BCE loss: 0.271, amex train: 0.712, val 0.000\n",
      "13, BCE loss: 0.274, amex train: 0.702, val 0.000\n",
      "13, BCE loss: 0.273, amex train: 0.700, val 0.000\n",
      "13, BCE loss: 0.266, amex train: 0.710, val 0.000\n",
      "13, BCE loss: 0.276, amex train: 0.695, val 0.000\n",
      "13, BCE loss: 0.271, amex train: 0.699, val 0.000\n",
      "13, BCE loss: 0.280, amex train: 0.699, val 0.000\n",
      "13, BCE loss: 0.266, amex train: 0.711, val 0.000\n",
      "13, BCE loss: 0.271, amex train: 0.709, val 0.000\n",
      "13, BCE loss: 0.273, amex train: 0.711, val 0.000\n",
      "13, BCE loss: 0.275, amex train: 0.699, val 0.000\n",
      "13, BCE loss: 0.267, amex train: 0.718, val 0.000\n",
      "14, BCE loss: 0.277, amex train: 0.693, val 0.000\n",
      "14, BCE loss: 0.269, amex train: 0.710, val 0.000\n",
      "14, BCE loss: 0.267, amex train: 0.717, val 0.000\n",
      "14, BCE loss: 0.265, amex train: 0.714, val 0.000\n",
      "14, BCE loss: 0.267, amex train: 0.708, val 0.000\n",
      "14, BCE loss: 0.268, amex train: 0.709, val 0.000\n",
      "14, BCE loss: 0.269, amex train: 0.716, val 0.000\n",
      "14, BCE loss: 0.269, amex train: 0.707, val 0.000\n",
      "14, BCE loss: 0.270, amex train: 0.701, val 0.000\n",
      "14, BCE loss: 0.270, amex train: 0.713, val 0.000\n",
      "14, BCE loss: 0.273, amex train: 0.702, val 0.000\n",
      "14, BCE loss: 0.272, amex train: 0.702, val 0.000\n",
      "14, BCE loss: 0.266, amex train: 0.711, val 0.000\n",
      "14, BCE loss: 0.275, amex train: 0.695, val 0.000\n",
      "14, BCE loss: 0.271, amex train: 0.701, val 0.000\n",
      "14, BCE loss: 0.279, amex train: 0.700, val 0.000\n",
      "14, BCE loss: 0.265, amex train: 0.711, val 0.000\n",
      "14, BCE loss: 0.271, amex train: 0.710, val 0.000\n",
      "14, BCE loss: 0.270, amex train: 0.712, val 0.000\n",
      "14, BCE loss: 0.275, amex train: 0.699, val 0.000\n",
      "14, BCE loss: 0.264, amex train: 0.721, val 0.000\n",
      "15, BCE loss: 0.277, amex train: 0.692, val 0.000\n",
      "15, BCE loss: 0.268, amex train: 0.711, val 0.000\n",
      "15, BCE loss: 0.266, amex train: 0.719, val 0.000\n",
      "15, BCE loss: 0.265, amex train: 0.714, val 0.000\n",
      "15, BCE loss: 0.266, amex train: 0.708, val 0.000\n",
      "15, BCE loss: 0.268, amex train: 0.711, val 0.000\n",
      "15, BCE loss: 0.269, amex train: 0.718, val 0.000\n",
      "15, BCE loss: 0.268, amex train: 0.708, val 0.000\n",
      "15, BCE loss: 0.269, amex train: 0.702, val 0.000\n",
      "15, BCE loss: 0.269, amex train: 0.714, val 0.000\n",
      "15, BCE loss: 0.272, amex train: 0.702, val 0.000\n",
      "15, BCE loss: 0.272, amex train: 0.703, val 0.000\n",
      "15, BCE loss: 0.265, amex train: 0.712, val 0.000\n",
      "15, BCE loss: 0.275, amex train: 0.697, val 0.000\n",
      "15, BCE loss: 0.270, amex train: 0.703, val 0.000\n",
      "15, BCE loss: 0.278, amex train: 0.700, val 0.000\n",
      "15, BCE loss: 0.265, amex train: 0.712, val 0.000\n",
      "15, BCE loss: 0.270, amex train: 0.711, val 0.000\n",
      "15, BCE loss: 0.271, amex train: 0.713, val 0.000\n",
      "15, BCE loss: 0.274, amex train: 0.699, val 0.000\n",
      "15, BCE loss: 0.264, amex train: 0.723, val 0.000\n",
      "16, BCE loss: 0.276, amex train: 0.693, val 0.000\n",
      "16, BCE loss: 0.267, amex train: 0.714, val 0.000\n",
      "16, BCE loss: 0.266, amex train: 0.719, val 0.000\n",
      "16, BCE loss: 0.264, amex train: 0.715, val 0.000\n",
      "16, BCE loss: 0.266, amex train: 0.708, val 0.000\n",
      "16, BCE loss: 0.267, amex train: 0.709, val 0.000\n",
      "16, BCE loss: 0.268, amex train: 0.718, val 0.000\n",
      "16, BCE loss: 0.267, amex train: 0.709, val 0.000\n",
      "16, BCE loss: 0.269, amex train: 0.704, val 0.000\n",
      "16, BCE loss: 0.268, amex train: 0.714, val 0.000\n",
      "16, BCE loss: 0.271, amex train: 0.703, val 0.000\n",
      "16, BCE loss: 0.271, amex train: 0.703, val 0.000\n",
      "16, BCE loss: 0.264, amex train: 0.711, val 0.000\n",
      "16, BCE loss: 0.275, amex train: 0.697, val 0.000\n",
      "16, BCE loss: 0.269, amex train: 0.705, val 0.000\n",
      "16, BCE loss: 0.278, amex train: 0.702, val 0.000\n",
      "16, BCE loss: 0.264, amex train: 0.712, val 0.000\n",
      "16, BCE loss: 0.270, amex train: 0.712, val 0.000\n",
      "16, BCE loss: 0.270, amex train: 0.715, val 0.000\n",
      "16, BCE loss: 0.274, amex train: 0.701, val 0.000\n",
      "16, BCE loss: 0.263, amex train: 0.727, val 0.000\n",
      "17, BCE loss: 0.276, amex train: 0.693, val 0.000\n",
      "17, BCE loss: 0.267, amex train: 0.715, val 0.000\n",
      "17, BCE loss: 0.265, amex train: 0.721, val 0.000\n",
      "17, BCE loss: 0.264, amex train: 0.715, val 0.000\n",
      "17, BCE loss: 0.265, amex train: 0.709, val 0.000\n",
      "17, BCE loss: 0.267, amex train: 0.710, val 0.000\n",
      "17, BCE loss: 0.268, amex train: 0.719, val 0.000\n",
      "17, BCE loss: 0.267, amex train: 0.709, val 0.000\n",
      "17, BCE loss: 0.268, amex train: 0.706, val 0.000\n",
      "17, BCE loss: 0.268, amex train: 0.716, val 0.000\n",
      "17, BCE loss: 0.271, amex train: 0.706, val 0.000\n",
      "17, BCE loss: 0.271, amex train: 0.703, val 0.000\n",
      "17, BCE loss: 0.264, amex train: 0.710, val 0.000\n",
      "17, BCE loss: 0.273, amex train: 0.698, val 0.000\n",
      "17, BCE loss: 0.269, amex train: 0.708, val 0.000\n",
      "17, BCE loss: 0.277, amex train: 0.704, val 0.000\n",
      "17, BCE loss: 0.263, amex train: 0.714, val 0.000\n",
      "17, BCE loss: 0.268, amex train: 0.713, val 0.000\n",
      "17, BCE loss: 0.269, amex train: 0.715, val 0.000\n",
      "17, BCE loss: 0.274, amex train: 0.703, val 0.000\n",
      "17, BCE loss: 0.262, amex train: 0.729, val 0.000\n",
      "18, BCE loss: 0.275, amex train: 0.695, val 0.000\n",
      "18, BCE loss: 0.266, amex train: 0.715, val 0.000\n",
      "18, BCE loss: 0.265, amex train: 0.723, val 0.000\n",
      "18, BCE loss: 0.263, amex train: 0.717, val 0.000\n",
      "18, BCE loss: 0.265, amex train: 0.709, val 0.000\n",
      "18, BCE loss: 0.266, amex train: 0.712, val 0.000\n",
      "18, BCE loss: 0.267, amex train: 0.719, val 0.000\n",
      "18, BCE loss: 0.267, amex train: 0.709, val 0.000\n",
      "18, BCE loss: 0.268, amex train: 0.707, val 0.000\n",
      "18, BCE loss: 0.267, amex train: 0.716, val 0.000\n",
      "18, BCE loss: 0.270, amex train: 0.708, val 0.000\n",
      "18, BCE loss: 0.270, amex train: 0.703, val 0.000\n",
      "18, BCE loss: 0.263, amex train: 0.711, val 0.000\n",
      "18, BCE loss: 0.273, amex train: 0.700, val 0.000\n",
      "18, BCE loss: 0.268, amex train: 0.710, val 0.000\n",
      "18, BCE loss: 0.277, amex train: 0.707, val 0.000\n",
      "18, BCE loss: 0.263, amex train: 0.715, val 0.000\n",
      "18, BCE loss: 0.268, amex train: 0.714, val 0.000\n",
      "18, BCE loss: 0.270, amex train: 0.716, val 0.000\n",
      "18, BCE loss: 0.273, amex train: 0.704, val 0.000\n",
      "18, BCE loss: 0.262, amex train: 0.730, val 0.000\n",
      "19, BCE loss: 0.275, amex train: 0.696, val 0.000\n",
      "19, BCE loss: 0.266, amex train: 0.717, val 0.000\n",
      "19, BCE loss: 0.265, amex train: 0.724, val 0.000\n",
      "19, BCE loss: 0.263, amex train: 0.718, val 0.000\n",
      "19, BCE loss: 0.264, amex train: 0.709, val 0.000\n",
      "19, BCE loss: 0.265, amex train: 0.712, val 0.000\n",
      "19, BCE loss: 0.267, amex train: 0.721, val 0.000\n",
      "19, BCE loss: 0.266, amex train: 0.709, val 0.000\n",
      "19, BCE loss: 0.267, amex train: 0.708, val 0.000\n",
      "19, BCE loss: 0.267, amex train: 0.717, val 0.000\n",
      "19, BCE loss: 0.270, amex train: 0.707, val 0.000\n",
      "19, BCE loss: 0.270, amex train: 0.705, val 0.000\n",
      "19, BCE loss: 0.263, amex train: 0.712, val 0.000\n",
      "19, BCE loss: 0.273, amex train: 0.700, val 0.000\n",
      "19, BCE loss: 0.268, amex train: 0.712, val 0.000\n",
      "19, BCE loss: 0.276, amex train: 0.708, val 0.000\n",
      "19, BCE loss: 0.263, amex train: 0.715, val 0.000\n",
      "19, BCE loss: 0.268, amex train: 0.715, val 0.000\n",
      "19, BCE loss: 0.268, amex train: 0.716, val 0.000\n",
      "19, BCE loss: 0.273, amex train: 0.704, val 0.000\n",
      "19, BCE loss: 0.260, amex train: 0.730, val 0.000\n",
      "20, BCE loss: 0.275, amex train: 0.696, val 0.000\n",
      "20, BCE loss: 0.265, amex train: 0.717, val 0.000\n",
      "20, BCE loss: 0.264, amex train: 0.725, val 0.000\n",
      "20, BCE loss: 0.263, amex train: 0.719, val 0.000\n",
      "20, BCE loss: 0.264, amex train: 0.710, val 0.000\n",
      "20, BCE loss: 0.265, amex train: 0.712, val 0.000\n",
      "20, BCE loss: 0.266, amex train: 0.720, val 0.000\n",
      "20, BCE loss: 0.266, amex train: 0.710, val 0.000\n",
      "20, BCE loss: 0.267, amex train: 0.709, val 0.000\n",
      "20, BCE loss: 0.266, amex train: 0.717, val 0.000\n",
      "20, BCE loss: 0.269, amex train: 0.706, val 0.000\n",
      "20, BCE loss: 0.269, amex train: 0.707, val 0.000\n",
      "20, BCE loss: 0.262, amex train: 0.713, val 0.000\n",
      "20, BCE loss: 0.272, amex train: 0.701, val 0.000\n",
      "20, BCE loss: 0.267, amex train: 0.712, val 0.000\n",
      "20, BCE loss: 0.275, amex train: 0.709, val 0.000\n",
      "20, BCE loss: 0.262, amex train: 0.716, val 0.000\n",
      "20, BCE loss: 0.267, amex train: 0.716, val 0.000\n",
      "20, BCE loss: 0.268, amex train: 0.717, val 0.000\n",
      "20, BCE loss: 0.273, amex train: 0.704, val 0.000\n",
      "20, BCE loss: 0.260, amex train: 0.730, val 0.000\n",
      "21, BCE loss: 0.274, amex train: 0.696, val 0.000\n",
      "21, BCE loss: 0.265, amex train: 0.717, val 0.000\n",
      "21, BCE loss: 0.264, amex train: 0.725, val 0.000\n",
      "21, BCE loss: 0.262, amex train: 0.720, val 0.000\n",
      "21, BCE loss: 0.263, amex train: 0.711, val 0.000\n",
      "21, BCE loss: 0.264, amex train: 0.713, val 0.000\n",
      "21, BCE loss: 0.266, amex train: 0.721, val 0.000\n",
      "21, BCE loss: 0.265, amex train: 0.711, val 0.000\n",
      "21, BCE loss: 0.266, amex train: 0.709, val 0.000\n",
      "21, BCE loss: 0.266, amex train: 0.717, val 0.000\n",
      "21, BCE loss: 0.269, amex train: 0.707, val 0.000\n",
      "21, BCE loss: 0.269, amex train: 0.707, val 0.000\n",
      "21, BCE loss: 0.262, amex train: 0.715, val 0.000\n",
      "21, BCE loss: 0.272, amex train: 0.702, val 0.000\n",
      "21, BCE loss: 0.267, amex train: 0.713, val 0.000\n",
      "21, BCE loss: 0.276, amex train: 0.710, val 0.000\n",
      "21, BCE loss: 0.262, amex train: 0.715, val 0.000\n",
      "21, BCE loss: 0.267, amex train: 0.716, val 0.000\n",
      "21, BCE loss: 0.268, amex train: 0.717, val 0.000\n",
      "21, BCE loss: 0.272, amex train: 0.704, val 0.000\n",
      "21, BCE loss: 0.260, amex train: 0.731, val 0.000\n",
      "22, BCE loss: 0.274, amex train: 0.697, val 0.000\n",
      "22, BCE loss: 0.265, amex train: 0.717, val 0.000\n",
      "22, BCE loss: 0.263, amex train: 0.725, val 0.000\n",
      "22, BCE loss: 0.262, amex train: 0.721, val 0.000\n",
      "22, BCE loss: 0.263, amex train: 0.712, val 0.000\n",
      "22, BCE loss: 0.264, amex train: 0.715, val 0.000\n",
      "22, BCE loss: 0.265, amex train: 0.720, val 0.000\n",
      "22, BCE loss: 0.265, amex train: 0.712, val 0.000\n",
      "22, BCE loss: 0.266, amex train: 0.710, val 0.000\n",
      "22, BCE loss: 0.265, amex train: 0.719, val 0.000\n",
      "22, BCE loss: 0.269, amex train: 0.708, val 0.000\n",
      "22, BCE loss: 0.268, amex train: 0.708, val 0.000\n",
      "22, BCE loss: 0.262, amex train: 0.716, val 0.000\n",
      "22, BCE loss: 0.271, amex train: 0.703, val 0.000\n",
      "22, BCE loss: 0.267, amex train: 0.715, val 0.000\n",
      "22, BCE loss: 0.275, amex train: 0.709, val 0.000\n",
      "22, BCE loss: 0.262, amex train: 0.716, val 0.000\n",
      "22, BCE loss: 0.266, amex train: 0.716, val 0.000\n",
      "22, BCE loss: 0.267, amex train: 0.716, val 0.000\n",
      "22, BCE loss: 0.272, amex train: 0.704, val 0.000\n",
      "22, BCE loss: 0.259, amex train: 0.734, val 0.000\n",
      "23, BCE loss: 0.273, amex train: 0.698, val 0.000\n",
      "23, BCE loss: 0.264, amex train: 0.718, val 0.000\n",
      "23, BCE loss: 0.263, amex train: 0.727, val 0.000\n",
      "23, BCE loss: 0.262, amex train: 0.721, val 0.000\n",
      "23, BCE loss: 0.263, amex train: 0.712, val 0.000\n",
      "23, BCE loss: 0.264, amex train: 0.716, val 0.000\n",
      "23, BCE loss: 0.265, amex train: 0.719, val 0.000\n",
      "23, BCE loss: 0.265, amex train: 0.712, val 0.000\n",
      "23, BCE loss: 0.266, amex train: 0.710, val 0.000\n",
      "23, BCE loss: 0.265, amex train: 0.719, val 0.000\n",
      "23, BCE loss: 0.268, amex train: 0.708, val 0.000\n",
      "23, BCE loss: 0.268, amex train: 0.709, val 0.000\n",
      "23, BCE loss: 0.261, amex train: 0.717, val 0.000\n",
      "23, BCE loss: 0.271, amex train: 0.703, val 0.000\n",
      "23, BCE loss: 0.266, amex train: 0.715, val 0.000\n",
      "23, BCE loss: 0.275, amex train: 0.709, val 0.000\n",
      "23, BCE loss: 0.262, amex train: 0.717, val 0.000\n",
      "23, BCE loss: 0.266, amex train: 0.718, val 0.000\n",
      "23, BCE loss: 0.267, amex train: 0.718, val 0.000\n",
      "23, BCE loss: 0.272, amex train: 0.704, val 0.000\n",
      "23, BCE loss: 0.258, amex train: 0.735, val 0.000\n",
      "24, BCE loss: 0.273, amex train: 0.698, val 0.000\n",
      "24, BCE loss: 0.264, amex train: 0.719, val 0.000\n",
      "24, BCE loss: 0.263, amex train: 0.727, val 0.000\n",
      "24, BCE loss: 0.261, amex train: 0.722, val 0.000\n",
      "24, BCE loss: 0.262, amex train: 0.713, val 0.000\n",
      "24, BCE loss: 0.264, amex train: 0.717, val 0.000\n",
      "24, BCE loss: 0.265, amex train: 0.719, val 0.000\n",
      "24, BCE loss: 0.264, amex train: 0.712, val 0.000\n",
      "24, BCE loss: 0.265, amex train: 0.711, val 0.000\n",
      "24, BCE loss: 0.264, amex train: 0.719, val 0.000\n",
      "24, BCE loss: 0.268, amex train: 0.709, val 0.000\n",
      "24, BCE loss: 0.268, amex train: 0.709, val 0.000\n",
      "24, BCE loss: 0.261, amex train: 0.718, val 0.000\n",
      "24, BCE loss: 0.271, amex train: 0.704, val 0.000\n",
      "24, BCE loss: 0.266, amex train: 0.716, val 0.000\n",
      "24, BCE loss: 0.275, amex train: 0.710, val 0.000\n",
      "24, BCE loss: 0.261, amex train: 0.717, val 0.000\n",
      "24, BCE loss: 0.266, amex train: 0.718, val 0.000\n",
      "24, BCE loss: 0.267, amex train: 0.719, val 0.000\n",
      "24, BCE loss: 0.272, amex train: 0.705, val 0.000\n",
      "24, BCE loss: 0.258, amex train: 0.735, val 0.000\n",
      "25, BCE loss: 0.273, amex train: 0.699, val 0.000\n",
      "25, BCE loss: 0.264, amex train: 0.720, val 0.000\n",
      "25, BCE loss: 0.262, amex train: 0.727, val 0.000\n",
      "25, BCE loss: 0.261, amex train: 0.722, val 0.000\n",
      "25, BCE loss: 0.262, amex train: 0.713, val 0.000\n",
      "25, BCE loss: 0.264, amex train: 0.718, val 0.000\n",
      "25, BCE loss: 0.264, amex train: 0.719, val 0.000\n",
      "25, BCE loss: 0.264, amex train: 0.713, val 0.000\n",
      "25, BCE loss: 0.265, amex train: 0.711, val 0.000\n",
      "25, BCE loss: 0.264, amex train: 0.720, val 0.000\n",
      "25, BCE loss: 0.267, amex train: 0.711, val 0.000\n",
      "25, BCE loss: 0.267, amex train: 0.711, val 0.000\n",
      "25, BCE loss: 0.261, amex train: 0.718, val 0.000\n",
      "25, BCE loss: 0.270, amex train: 0.705, val 0.000\n",
      "25, BCE loss: 0.265, amex train: 0.716, val 0.000\n",
      "25, BCE loss: 0.274, amex train: 0.711, val 0.000\n",
      "25, BCE loss: 0.261, amex train: 0.717, val 0.000\n",
      "25, BCE loss: 0.265, amex train: 0.720, val 0.000\n",
      "25, BCE loss: 0.266, amex train: 0.720, val 0.000\n",
      "25, BCE loss: 0.271, amex train: 0.705, val 0.000\n",
      "25, BCE loss: 0.257, amex train: 0.736, val 0.000\n",
      "26, BCE loss: 0.272, amex train: 0.699, val 0.000\n",
      "26, BCE loss: 0.263, amex train: 0.720, val 0.000\n",
      "26, BCE loss: 0.262, amex train: 0.728, val 0.000\n",
      "26, BCE loss: 0.261, amex train: 0.723, val 0.000\n",
      "26, BCE loss: 0.262, amex train: 0.714, val 0.000\n",
      "26, BCE loss: 0.263, amex train: 0.717, val 0.000\n",
      "26, BCE loss: 0.264, amex train: 0.720, val 0.000\n",
      "26, BCE loss: 0.263, amex train: 0.714, val 0.000\n",
      "26, BCE loss: 0.265, amex train: 0.711, val 0.000\n",
      "26, BCE loss: 0.264, amex train: 0.721, val 0.000\n",
      "26, BCE loss: 0.267, amex train: 0.710, val 0.000\n",
      "26, BCE loss: 0.267, amex train: 0.710, val 0.000\n",
      "26, BCE loss: 0.260, amex train: 0.718, val 0.000\n",
      "26, BCE loss: 0.270, amex train: 0.706, val 0.000\n",
      "26, BCE loss: 0.265, amex train: 0.717, val 0.000\n",
      "26, BCE loss: 0.274, amex train: 0.710, val 0.000\n",
      "26, BCE loss: 0.261, amex train: 0.717, val 0.000\n",
      "26, BCE loss: 0.265, amex train: 0.720, val 0.000\n",
      "26, BCE loss: 0.266, amex train: 0.720, val 0.000\n",
      "26, BCE loss: 0.271, amex train: 0.705, val 0.000\n",
      "26, BCE loss: 0.257, amex train: 0.737, val 0.000\n",
      "27, BCE loss: 0.272, amex train: 0.699, val 0.000\n",
      "27, BCE loss: 0.263, amex train: 0.722, val 0.000\n",
      "27, BCE loss: 0.262, amex train: 0.728, val 0.000\n",
      "27, BCE loss: 0.261, amex train: 0.723, val 0.000\n",
      "27, BCE loss: 0.261, amex train: 0.715, val 0.000\n",
      "27, BCE loss: 0.263, amex train: 0.717, val 0.000\n",
      "27, BCE loss: 0.264, amex train: 0.720, val 0.000\n",
      "27, BCE loss: 0.263, amex train: 0.714, val 0.000\n",
      "27, BCE loss: 0.264, amex train: 0.712, val 0.000\n",
      "27, BCE loss: 0.263, amex train: 0.721, val 0.000\n",
      "27, BCE loss: 0.267, amex train: 0.711, val 0.000\n",
      "27, BCE loss: 0.267, amex train: 0.710, val 0.000\n",
      "27, BCE loss: 0.260, amex train: 0.719, val 0.000\n",
      "27, BCE loss: 0.270, amex train: 0.706, val 0.000\n",
      "27, BCE loss: 0.265, amex train: 0.716, val 0.000\n",
      "27, BCE loss: 0.273, amex train: 0.710, val 0.000\n",
      "27, BCE loss: 0.261, amex train: 0.718, val 0.000\n",
      "27, BCE loss: 0.265, amex train: 0.722, val 0.000\n",
      "27, BCE loss: 0.266, amex train: 0.722, val 0.000\n",
      "27, BCE loss: 0.271, amex train: 0.706, val 0.000\n",
      "27, BCE loss: 0.256, amex train: 0.738, val 0.000\n",
      "28, BCE loss: 0.272, amex train: 0.700, val 0.000\n",
      "28, BCE loss: 0.263, amex train: 0.723, val 0.000\n",
      "28, BCE loss: 0.261, amex train: 0.728, val 0.000\n",
      "28, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "28, BCE loss: 0.261, amex train: 0.714, val 0.000\n",
      "28, BCE loss: 0.263, amex train: 0.718, val 0.000\n",
      "28, BCE loss: 0.263, amex train: 0.721, val 0.000\n",
      "28, BCE loss: 0.263, amex train: 0.715, val 0.000\n",
      "28, BCE loss: 0.264, amex train: 0.713, val 0.000\n",
      "28, BCE loss: 0.263, amex train: 0.722, val 0.000\n",
      "28, BCE loss: 0.266, amex train: 0.711, val 0.000\n",
      "28, BCE loss: 0.266, amex train: 0.711, val 0.000\n",
      "28, BCE loss: 0.260, amex train: 0.720, val 0.000\n",
      "28, BCE loss: 0.270, amex train: 0.706, val 0.000\n",
      "28, BCE loss: 0.264, amex train: 0.716, val 0.000\n",
      "28, BCE loss: 0.273, amex train: 0.711, val 0.000\n",
      "28, BCE loss: 0.260, amex train: 0.718, val 0.000\n",
      "28, BCE loss: 0.264, amex train: 0.721, val 0.000\n",
      "28, BCE loss: 0.265, amex train: 0.722, val 0.000\n",
      "28, BCE loss: 0.271, amex train: 0.706, val 0.000\n",
      "28, BCE loss: 0.255, amex train: 0.739, val 0.000\n",
      "29, BCE loss: 0.271, amex train: 0.700, val 0.000\n",
      "29, BCE loss: 0.262, amex train: 0.723, val 0.000\n",
      "29, BCE loss: 0.261, amex train: 0.729, val 0.000\n",
      "29, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "29, BCE loss: 0.261, amex train: 0.714, val 0.000\n",
      "29, BCE loss: 0.262, amex train: 0.719, val 0.000\n",
      "29, BCE loss: 0.263, amex train: 0.722, val 0.000\n",
      "29, BCE loss: 0.262, amex train: 0.715, val 0.000\n",
      "29, BCE loss: 0.264, amex train: 0.713, val 0.000\n",
      "29, BCE loss: 0.263, amex train: 0.722, val 0.000\n",
      "29, BCE loss: 0.266, amex train: 0.712, val 0.000\n",
      "29, BCE loss: 0.266, amex train: 0.712, val 0.000\n",
      "29, BCE loss: 0.259, amex train: 0.720, val 0.000\n",
      "29, BCE loss: 0.269, amex train: 0.707, val 0.000\n",
      "29, BCE loss: 0.264, amex train: 0.717, val 0.000\n",
      "29, BCE loss: 0.273, amex train: 0.711, val 0.000\n",
      "29, BCE loss: 0.260, amex train: 0.719, val 0.000\n",
      "29, BCE loss: 0.264, amex train: 0.722, val 0.000\n",
      "29, BCE loss: 0.265, amex train: 0.721, val 0.000\n",
      "29, BCE loss: 0.270, amex train: 0.707, val 0.000\n",
      "29, BCE loss: 0.255, amex train: 0.739, val 0.000\n",
      "30, BCE loss: 0.271, amex train: 0.701, val 0.000\n",
      "30, BCE loss: 0.262, amex train: 0.724, val 0.000\n",
      "30, BCE loss: 0.261, amex train: 0.730, val 0.000\n",
      "30, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "30, BCE loss: 0.261, amex train: 0.715, val 0.000\n",
      "30, BCE loss: 0.262, amex train: 0.720, val 0.000\n",
      "30, BCE loss: 0.263, amex train: 0.723, val 0.000\n",
      "30, BCE loss: 0.262, amex train: 0.715, val 0.000\n",
      "30, BCE loss: 0.263, amex train: 0.715, val 0.000\n",
      "30, BCE loss: 0.262, amex train: 0.722, val 0.000\n",
      "30, BCE loss: 0.266, amex train: 0.713, val 0.000\n",
      "30, BCE loss: 0.266, amex train: 0.712, val 0.000\n",
      "30, BCE loss: 0.259, amex train: 0.720, val 0.000\n",
      "30, BCE loss: 0.269, amex train: 0.709, val 0.000\n",
      "30, BCE loss: 0.264, amex train: 0.717, val 0.000\n",
      "30, BCE loss: 0.273, amex train: 0.712, val 0.000\n",
      "30, BCE loss: 0.260, amex train: 0.720, val 0.000\n",
      "30, BCE loss: 0.264, amex train: 0.721, val 0.000\n",
      "30, BCE loss: 0.265, amex train: 0.722, val 0.000\n",
      "30, BCE loss: 0.270, amex train: 0.707, val 0.000\n",
      "30, BCE loss: 0.255, amex train: 0.739, val 0.000\n",
      "31, BCE loss: 0.271, amex train: 0.701, val 0.000\n",
      "31, BCE loss: 0.262, amex train: 0.724, val 0.000\n",
      "31, BCE loss: 0.261, amex train: 0.730, val 0.000\n",
      "31, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "31, BCE loss: 0.260, amex train: 0.715, val 0.000\n",
      "31, BCE loss: 0.262, amex train: 0.720, val 0.000\n",
      "31, BCE loss: 0.262, amex train: 0.723, val 0.000\n",
      "31, BCE loss: 0.262, amex train: 0.715, val 0.000\n",
      "31, BCE loss: 0.263, amex train: 0.715, val 0.000\n",
      "31, BCE loss: 0.262, amex train: 0.723, val 0.000\n",
      "31, BCE loss: 0.265, amex train: 0.712, val 0.000\n",
      "31, BCE loss: 0.265, amex train: 0.713, val 0.000\n",
      "31, BCE loss: 0.259, amex train: 0.720, val 0.000\n",
      "31, BCE loss: 0.269, amex train: 0.710, val 0.000\n",
      "31, BCE loss: 0.264, amex train: 0.717, val 0.000\n",
      "31, BCE loss: 0.272, amex train: 0.710, val 0.000\n",
      "31, BCE loss: 0.260, amex train: 0.721, val 0.000\n",
      "31, BCE loss: 0.263, amex train: 0.722, val 0.000\n",
      "31, BCE loss: 0.264, amex train: 0.722, val 0.000\n",
      "31, BCE loss: 0.270, amex train: 0.707, val 0.000\n",
      "31, BCE loss: 0.254, amex train: 0.741, val 0.000\n",
      "32, BCE loss: 0.271, amex train: 0.702, val 0.000\n",
      "32, BCE loss: 0.261, amex train: 0.724, val 0.000\n",
      "32, BCE loss: 0.260, amex train: 0.731, val 0.000\n",
      "32, BCE loss: 0.259, amex train: 0.724, val 0.000\n",
      "32, BCE loss: 0.260, amex train: 0.715, val 0.000\n",
      "32, BCE loss: 0.262, amex train: 0.720, val 0.000\n",
      "32, BCE loss: 0.262, amex train: 0.724, val 0.000\n",
      "32, BCE loss: 0.261, amex train: 0.716, val 0.000\n",
      "32, BCE loss: 0.263, amex train: 0.716, val 0.000\n",
      "32, BCE loss: 0.262, amex train: 0.723, val 0.000\n",
      "32, BCE loss: 0.265, amex train: 0.712, val 0.000\n",
      "32, BCE loss: 0.265, amex train: 0.713, val 0.000\n",
      "32, BCE loss: 0.258, amex train: 0.721, val 0.000\n",
      "32, BCE loss: 0.269, amex train: 0.710, val 0.000\n",
      "32, BCE loss: 0.263, amex train: 0.718, val 0.000\n",
      "32, BCE loss: 0.272, amex train: 0.710, val 0.000\n",
      "32, BCE loss: 0.260, amex train: 0.721, val 0.000\n",
      "32, BCE loss: 0.263, amex train: 0.724, val 0.000\n",
      "32, BCE loss: 0.264, amex train: 0.722, val 0.000\n",
      "32, BCE loss: 0.269, amex train: 0.708, val 0.000\n",
      "32, BCE loss: 0.254, amex train: 0.742, val 0.000\n",
      "33, BCE loss: 0.270, amex train: 0.702, val 0.000\n",
      "33, BCE loss: 0.261, amex train: 0.725, val 0.000\n",
      "33, BCE loss: 0.260, amex train: 0.730, val 0.000\n",
      "33, BCE loss: 0.259, amex train: 0.724, val 0.000\n",
      "33, BCE loss: 0.260, amex train: 0.716, val 0.000\n",
      "33, BCE loss: 0.262, amex train: 0.720, val 0.000\n",
      "33, BCE loss: 0.262, amex train: 0.724, val 0.000\n",
      "33, BCE loss: 0.261, amex train: 0.718, val 0.000\n",
      "33, BCE loss: 0.263, amex train: 0.717, val 0.000\n",
      "33, BCE loss: 0.262, amex train: 0.723, val 0.000\n",
      "33, BCE loss: 0.265, amex train: 0.713, val 0.000\n",
      "33, BCE loss: 0.265, amex train: 0.714, val 0.000\n",
      "33, BCE loss: 0.258, amex train: 0.721, val 0.000\n",
      "33, BCE loss: 0.268, amex train: 0.711, val 0.000\n",
      "33, BCE loss: 0.263, amex train: 0.719, val 0.000\n",
      "33, BCE loss: 0.272, amex train: 0.710, val 0.000\n",
      "33, BCE loss: 0.259, amex train: 0.720, val 0.000\n",
      "33, BCE loss: 0.263, amex train: 0.724, val 0.000\n",
      "33, BCE loss: 0.264, amex train: 0.723, val 0.000\n",
      "33, BCE loss: 0.269, amex train: 0.708, val 0.000\n",
      "33, BCE loss: 0.253, amex train: 0.742, val 0.000\n",
      "34, BCE loss: 0.270, amex train: 0.702, val 0.000\n",
      "34, BCE loss: 0.261, amex train: 0.725, val 0.000\n",
      "34, BCE loss: 0.260, amex train: 0.730, val 0.000\n",
      "34, BCE loss: 0.259, amex train: 0.725, val 0.000\n",
      "34, BCE loss: 0.260, amex train: 0.716, val 0.000\n",
      "34, BCE loss: 0.261, amex train: 0.720, val 0.000\n",
      "34, BCE loss: 0.262, amex train: 0.725, val 0.000\n",
      "34, BCE loss: 0.261, amex train: 0.718, val 0.000\n",
      "34, BCE loss: 0.262, amex train: 0.716, val 0.000\n",
      "34, BCE loss: 0.261, amex train: 0.723, val 0.000\n",
      "34, BCE loss: 0.265, amex train: 0.713, val 0.000\n",
      "34, BCE loss: 0.264, amex train: 0.714, val 0.000\n",
      "34, BCE loss: 0.258, amex train: 0.722, val 0.000\n",
      "34, BCE loss: 0.268, amex train: 0.711, val 0.000\n",
      "34, BCE loss: 0.263, amex train: 0.719, val 0.000\n",
      "34, BCE loss: 0.271, amex train: 0.711, val 0.000\n",
      "34, BCE loss: 0.259, amex train: 0.720, val 0.000\n",
      "34, BCE loss: 0.263, amex train: 0.725, val 0.000\n",
      "34, BCE loss: 0.264, amex train: 0.723, val 0.000\n",
      "34, BCE loss: 0.269, amex train: 0.708, val 0.000\n",
      "34, BCE loss: 0.253, amex train: 0.743, val 0.000\n",
      "35, BCE loss: 0.270, amex train: 0.704, val 0.000\n",
      "35, BCE loss: 0.261, amex train: 0.725, val 0.000\n",
      "35, BCE loss: 0.260, amex train: 0.730, val 0.000\n",
      "35, BCE loss: 0.259, amex train: 0.724, val 0.000\n",
      "35, BCE loss: 0.259, amex train: 0.717, val 0.000\n",
      "35, BCE loss: 0.261, amex train: 0.720, val 0.000\n",
      "35, BCE loss: 0.261, amex train: 0.724, val 0.000\n",
      "35, BCE loss: 0.261, amex train: 0.718, val 0.000\n",
      "35, BCE loss: 0.262, amex train: 0.717, val 0.000\n",
      "35, BCE loss: 0.261, amex train: 0.723, val 0.000\n",
      "35, BCE loss: 0.264, amex train: 0.713, val 0.000\n",
      "35, BCE loss: 0.264, amex train: 0.715, val 0.000\n",
      "35, BCE loss: 0.258, amex train: 0.721, val 0.000\n",
      "35, BCE loss: 0.268, amex train: 0.711, val 0.000\n",
      "35, BCE loss: 0.263, amex train: 0.719, val 0.000\n",
      "35, BCE loss: 0.271, amex train: 0.711, val 0.000\n",
      "35, BCE loss: 0.259, amex train: 0.721, val 0.000\n",
      "35, BCE loss: 0.262, amex train: 0.725, val 0.000\n",
      "35, BCE loss: 0.263, amex train: 0.725, val 0.000\n",
      "35, BCE loss: 0.269, amex train: 0.708, val 0.000\n",
      "35, BCE loss: 0.253, amex train: 0.743, val 0.000\n",
      "36, BCE loss: 0.270, amex train: 0.704, val 0.000\n",
      "36, BCE loss: 0.261, amex train: 0.725, val 0.000\n",
      "36, BCE loss: 0.259, amex train: 0.730, val 0.000\n",
      "36, BCE loss: 0.259, amex train: 0.724, val 0.000\n",
      "36, BCE loss: 0.259, amex train: 0.719, val 0.000\n",
      "36, BCE loss: 0.261, amex train: 0.720, val 0.000\n",
      "36, BCE loss: 0.261, amex train: 0.725, val 0.000\n",
      "36, BCE loss: 0.260, amex train: 0.718, val 0.000\n",
      "36, BCE loss: 0.262, amex train: 0.718, val 0.000\n",
      "36, BCE loss: 0.261, amex train: 0.723, val 0.000\n",
      "36, BCE loss: 0.264, amex train: 0.714, val 0.000\n",
      "36, BCE loss: 0.264, amex train: 0.716, val 0.000\n",
      "36, BCE loss: 0.257, amex train: 0.722, val 0.000\n",
      "36, BCE loss: 0.268, amex train: 0.712, val 0.000\n",
      "36, BCE loss: 0.262, amex train: 0.720, val 0.000\n",
      "36, BCE loss: 0.271, amex train: 0.711, val 0.000\n",
      "36, BCE loss: 0.259, amex train: 0.722, val 0.000\n",
      "36, BCE loss: 0.262, amex train: 0.725, val 0.000\n",
      "36, BCE loss: 0.263, amex train: 0.725, val 0.000\n",
      "36, BCE loss: 0.269, amex train: 0.709, val 0.000\n",
      "36, BCE loss: 0.252, amex train: 0.744, val 0.000\n",
      "37, BCE loss: 0.269, amex train: 0.705, val 0.000\n",
      "37, BCE loss: 0.260, amex train: 0.725, val 0.000\n",
      "37, BCE loss: 0.259, amex train: 0.730, val 0.000\n",
      "37, BCE loss: 0.259, amex train: 0.725, val 0.000\n",
      "37, BCE loss: 0.259, amex train: 0.719, val 0.000\n",
      "37, BCE loss: 0.261, amex train: 0.720, val 0.000\n",
      "37, BCE loss: 0.261, amex train: 0.725, val 0.000\n",
      "37, BCE loss: 0.260, amex train: 0.718, val 0.000\n",
      "37, BCE loss: 0.262, amex train: 0.718, val 0.000\n",
      "37, BCE loss: 0.261, amex train: 0.723, val 0.000\n",
      "37, BCE loss: 0.264, amex train: 0.715, val 0.000\n",
      "37, BCE loss: 0.263, amex train: 0.716, val 0.000\n",
      "37, BCE loss: 0.257, amex train: 0.722, val 0.000\n",
      "37, BCE loss: 0.267, amex train: 0.713, val 0.000\n",
      "37, BCE loss: 0.262, amex train: 0.720, val 0.000\n",
      "37, BCE loss: 0.271, amex train: 0.711, val 0.000\n",
      "37, BCE loss: 0.259, amex train: 0.723, val 0.000\n",
      "37, BCE loss: 0.262, amex train: 0.725, val 0.000\n",
      "37, BCE loss: 0.263, amex train: 0.725, val 0.000\n",
      "37, BCE loss: 0.269, amex train: 0.708, val 0.000\n",
      "37, BCE loss: 0.252, amex train: 0.746, val 0.000\n",
      "38, BCE loss: 0.269, amex train: 0.706, val 0.000\n",
      "38, BCE loss: 0.260, amex train: 0.726, val 0.000\n",
      "38, BCE loss: 0.259, amex train: 0.730, val 0.000\n",
      "38, BCE loss: 0.258, amex train: 0.725, val 0.000\n",
      "38, BCE loss: 0.259, amex train: 0.720, val 0.000\n",
      "38, BCE loss: 0.261, amex train: 0.720, val 0.000\n",
      "38, BCE loss: 0.261, amex train: 0.725, val 0.000\n",
      "38, BCE loss: 0.260, amex train: 0.719, val 0.000\n",
      "38, BCE loss: 0.261, amex train: 0.719, val 0.000\n",
      "38, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "38, BCE loss: 0.264, amex train: 0.715, val 0.000\n",
      "38, BCE loss: 0.263, amex train: 0.716, val 0.000\n",
      "38, BCE loss: 0.257, amex train: 0.722, val 0.000\n",
      "38, BCE loss: 0.267, amex train: 0.714, val 0.000\n",
      "38, BCE loss: 0.262, amex train: 0.720, val 0.000\n",
      "38, BCE loss: 0.270, amex train: 0.711, val 0.000\n",
      "38, BCE loss: 0.258, amex train: 0.723, val 0.000\n",
      "38, BCE loss: 0.262, amex train: 0.725, val 0.000\n",
      "38, BCE loss: 0.263, amex train: 0.725, val 0.000\n",
      "38, BCE loss: 0.268, amex train: 0.710, val 0.000\n",
      "38, BCE loss: 0.251, amex train: 0.745, val 0.000\n",
      "39, BCE loss: 0.269, amex train: 0.706, val 0.000\n",
      "39, BCE loss: 0.260, amex train: 0.726, val 0.000\n",
      "39, BCE loss: 0.259, amex train: 0.730, val 0.000\n",
      "39, BCE loss: 0.258, amex train: 0.726, val 0.000\n",
      "39, BCE loss: 0.259, amex train: 0.720, val 0.000\n",
      "39, BCE loss: 0.260, amex train: 0.721, val 0.000\n",
      "39, BCE loss: 0.261, amex train: 0.726, val 0.000\n",
      "39, BCE loss: 0.260, amex train: 0.719, val 0.000\n",
      "39, BCE loss: 0.261, amex train: 0.719, val 0.000\n",
      "39, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "39, BCE loss: 0.263, amex train: 0.716, val 0.000\n",
      "39, BCE loss: 0.263, amex train: 0.717, val 0.000\n",
      "39, BCE loss: 0.257, amex train: 0.722, val 0.000\n",
      "39, BCE loss: 0.267, amex train: 0.714, val 0.000\n",
      "39, BCE loss: 0.262, amex train: 0.721, val 0.000\n",
      "39, BCE loss: 0.270, amex train: 0.711, val 0.000\n",
      "39, BCE loss: 0.258, amex train: 0.724, val 0.000\n",
      "39, BCE loss: 0.262, amex train: 0.725, val 0.000\n",
      "39, BCE loss: 0.263, amex train: 0.725, val 0.000\n",
      "39, BCE loss: 0.268, amex train: 0.710, val 0.000\n",
      "39, BCE loss: 0.251, amex train: 0.746, val 0.000\n",
      "40, BCE loss: 0.269, amex train: 0.706, val 0.000\n",
      "40, BCE loss: 0.260, amex train: 0.727, val 0.000\n",
      "40, BCE loss: 0.259, amex train: 0.730, val 0.000\n",
      "40, BCE loss: 0.258, amex train: 0.727, val 0.000\n",
      "40, BCE loss: 0.258, amex train: 0.719, val 0.000\n",
      "40, BCE loss: 0.260, amex train: 0.721, val 0.000\n",
      "40, BCE loss: 0.260, amex train: 0.726, val 0.000\n",
      "40, BCE loss: 0.260, amex train: 0.719, val 0.000\n",
      "40, BCE loss: 0.261, amex train: 0.720, val 0.000\n",
      "40, BCE loss: 0.260, amex train: 0.722, val 0.000\n",
      "40, BCE loss: 0.263, amex train: 0.715, val 0.000\n",
      "40, BCE loss: 0.263, amex train: 0.717, val 0.000\n",
      "40, BCE loss: 0.257, amex train: 0.723, val 0.000\n",
      "40, BCE loss: 0.267, amex train: 0.714, val 0.000\n",
      "40, BCE loss: 0.262, amex train: 0.722, val 0.000\n",
      "40, BCE loss: 0.270, amex train: 0.710, val 0.000\n",
      "40, BCE loss: 0.258, amex train: 0.724, val 0.000\n",
      "40, BCE loss: 0.261, amex train: 0.726, val 0.000\n",
      "40, BCE loss: 0.262, amex train: 0.725, val 0.000\n",
      "40, BCE loss: 0.268, amex train: 0.712, val 0.000\n",
      "40, BCE loss: 0.250, amex train: 0.745, val 0.000\n",
      "41, BCE loss: 0.268, amex train: 0.706, val 0.000\n",
      "41, BCE loss: 0.259, amex train: 0.728, val 0.000\n",
      "41, BCE loss: 0.258, amex train: 0.732, val 0.000\n",
      "41, BCE loss: 0.258, amex train: 0.727, val 0.000\n",
      "41, BCE loss: 0.258, amex train: 0.719, val 0.000\n",
      "41, BCE loss: 0.260, amex train: 0.720, val 0.000\n",
      "41, BCE loss: 0.260, amex train: 0.727, val 0.000\n",
      "41, BCE loss: 0.259, amex train: 0.720, val 0.000\n",
      "41, BCE loss: 0.261, amex train: 0.720, val 0.000\n",
      "41, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "41, BCE loss: 0.263, amex train: 0.715, val 0.000\n",
      "41, BCE loss: 0.263, amex train: 0.718, val 0.000\n",
      "41, BCE loss: 0.256, amex train: 0.723, val 0.000\n",
      "41, BCE loss: 0.267, amex train: 0.715, val 0.000\n",
      "41, BCE loss: 0.261, amex train: 0.723, val 0.000\n",
      "41, BCE loss: 0.270, amex train: 0.710, val 0.000\n",
      "41, BCE loss: 0.258, amex train: 0.724, val 0.000\n",
      "41, BCE loss: 0.261, amex train: 0.726, val 0.000\n",
      "41, BCE loss: 0.262, amex train: 0.726, val 0.000\n",
      "41, BCE loss: 0.268, amex train: 0.711, val 0.000\n",
      "41, BCE loss: 0.250, amex train: 0.745, val 0.000\n",
      "42, BCE loss: 0.268, amex train: 0.706, val 0.000\n",
      "42, BCE loss: 0.259, amex train: 0.729, val 0.000\n",
      "42, BCE loss: 0.258, amex train: 0.733, val 0.000\n",
      "42, BCE loss: 0.258, amex train: 0.727, val 0.000\n",
      "42, BCE loss: 0.258, amex train: 0.720, val 0.000\n",
      "42, BCE loss: 0.260, amex train: 0.720, val 0.000\n",
      "42, BCE loss: 0.260, amex train: 0.727, val 0.000\n",
      "42, BCE loss: 0.259, amex train: 0.720, val 0.000\n",
      "42, BCE loss: 0.260, amex train: 0.721, val 0.000\n",
      "42, BCE loss: 0.260, amex train: 0.722, val 0.000\n",
      "42, BCE loss: 0.263, amex train: 0.716, val 0.000\n",
      "42, BCE loss: 0.262, amex train: 0.718, val 0.000\n",
      "42, BCE loss: 0.256, amex train: 0.724, val 0.000\n",
      "42, BCE loss: 0.266, amex train: 0.715, val 0.000\n",
      "42, BCE loss: 0.261, amex train: 0.724, val 0.000\n",
      "42, BCE loss: 0.270, amex train: 0.710, val 0.000\n",
      "42, BCE loss: 0.258, amex train: 0.724, val 0.000\n",
      "42, BCE loss: 0.261, amex train: 0.727, val 0.000\n",
      "42, BCE loss: 0.262, amex train: 0.725, val 0.000\n",
      "42, BCE loss: 0.267, amex train: 0.712, val 0.000\n",
      "42, BCE loss: 0.250, amex train: 0.744, val 0.000\n",
      "43, BCE loss: 0.268, amex train: 0.706, val 0.000\n",
      "43, BCE loss: 0.259, amex train: 0.729, val 0.000\n",
      "43, BCE loss: 0.258, amex train: 0.734, val 0.000\n",
      "43, BCE loss: 0.258, amex train: 0.727, val 0.000\n",
      "43, BCE loss: 0.258, amex train: 0.720, val 0.000\n",
      "43, BCE loss: 0.260, amex train: 0.719, val 0.000\n",
      "43, BCE loss: 0.260, amex train: 0.727, val 0.000\n",
      "43, BCE loss: 0.259, amex train: 0.721, val 0.000\n",
      "43, BCE loss: 0.260, amex train: 0.721, val 0.000\n",
      "43, BCE loss: 0.259, amex train: 0.724, val 0.000\n",
      "43, BCE loss: 0.262, amex train: 0.716, val 0.000\n",
      "43, BCE loss: 0.262, amex train: 0.718, val 0.000\n",
      "43, BCE loss: 0.256, amex train: 0.724, val 0.000\n",
      "43, BCE loss: 0.266, amex train: 0.715, val 0.000\n",
      "43, BCE loss: 0.261, amex train: 0.724, val 0.000\n",
      "43, BCE loss: 0.270, amex train: 0.711, val 0.000\n",
      "43, BCE loss: 0.258, amex train: 0.725, val 0.000\n",
      "43, BCE loss: 0.261, amex train: 0.727, val 0.000\n",
      "43, BCE loss: 0.262, amex train: 0.725, val 0.000\n",
      "43, BCE loss: 0.267, amex train: 0.712, val 0.000\n",
      "43, BCE loss: 0.249, amex train: 0.743, val 0.000\n",
      "44, BCE loss: 0.268, amex train: 0.707, val 0.000\n",
      "44, BCE loss: 0.259, amex train: 0.729, val 0.000\n",
      "44, BCE loss: 0.258, amex train: 0.734, val 0.000\n",
      "44, BCE loss: 0.257, amex train: 0.727, val 0.000\n",
      "44, BCE loss: 0.258, amex train: 0.720, val 0.000\n",
      "44, BCE loss: 0.260, amex train: 0.719, val 0.000\n",
      "44, BCE loss: 0.260, amex train: 0.727, val 0.000\n",
      "44, BCE loss: 0.259, amex train: 0.721, val 0.000\n",
      "44, BCE loss: 0.260, amex train: 0.721, val 0.000\n",
      "44, BCE loss: 0.259, amex train: 0.725, val 0.000\n",
      "44, BCE loss: 0.262, amex train: 0.715, val 0.000\n",
      "44, BCE loss: 0.262, amex train: 0.718, val 0.000\n",
      "44, BCE loss: 0.256, amex train: 0.725, val 0.000\n",
      "44, BCE loss: 0.266, amex train: 0.715, val 0.000\n",
      "44, BCE loss: 0.261, amex train: 0.724, val 0.000\n",
      "44, BCE loss: 0.269, amex train: 0.712, val 0.000\n",
      "44, BCE loss: 0.257, amex train: 0.725, val 0.000\n",
      "44, BCE loss: 0.261, amex train: 0.727, val 0.000\n",
      "44, BCE loss: 0.262, amex train: 0.725, val 0.000\n",
      "44, BCE loss: 0.267, amex train: 0.712, val 0.000\n",
      "44, BCE loss: 0.249, amex train: 0.744, val 0.000\n",
      "45, BCE loss: 0.267, amex train: 0.707, val 0.000\n",
      "45, BCE loss: 0.259, amex train: 0.730, val 0.000\n",
      "45, BCE loss: 0.258, amex train: 0.736, val 0.000\n",
      "45, BCE loss: 0.257, amex train: 0.727, val 0.000\n",
      "45, BCE loss: 0.257, amex train: 0.721, val 0.000\n",
      "45, BCE loss: 0.259, amex train: 0.719, val 0.000\n",
      "45, BCE loss: 0.260, amex train: 0.727, val 0.000\n",
      "45, BCE loss: 0.259, amex train: 0.722, val 0.000\n",
      "45, BCE loss: 0.260, amex train: 0.721, val 0.000\n",
      "45, BCE loss: 0.259, amex train: 0.725, val 0.000\n",
      "45, BCE loss: 0.262, amex train: 0.716, val 0.000\n",
      "45, BCE loss: 0.262, amex train: 0.719, val 0.000\n",
      "45, BCE loss: 0.256, amex train: 0.724, val 0.000\n",
      "45, BCE loss: 0.266, amex train: 0.715, val 0.000\n",
      "45, BCE loss: 0.261, amex train: 0.724, val 0.000\n",
      "45, BCE loss: 0.269, amex train: 0.712, val 0.000\n",
      "45, BCE loss: 0.257, amex train: 0.726, val 0.000\n",
      "45, BCE loss: 0.260, amex train: 0.728, val 0.000\n",
      "45, BCE loss: 0.261, amex train: 0.725, val 0.000\n",
      "45, BCE loss: 0.267, amex train: 0.713, val 0.000\n",
      "45, BCE loss: 0.249, amex train: 0.745, val 0.000\n",
      "46, BCE loss: 0.267, amex train: 0.707, val 0.000\n",
      "46, BCE loss: 0.259, amex train: 0.730, val 0.000\n",
      "46, BCE loss: 0.257, amex train: 0.736, val 0.000\n",
      "46, BCE loss: 0.257, amex train: 0.728, val 0.000\n",
      "46, BCE loss: 0.257, amex train: 0.721, val 0.000\n",
      "46, BCE loss: 0.259, amex train: 0.720, val 0.000\n",
      "46, BCE loss: 0.259, amex train: 0.729, val 0.000\n",
      "46, BCE loss: 0.258, amex train: 0.722, val 0.000\n",
      "46, BCE loss: 0.260, amex train: 0.721, val 0.000\n",
      "46, BCE loss: 0.259, amex train: 0.726, val 0.000\n",
      "46, BCE loss: 0.262, amex train: 0.716, val 0.000\n",
      "46, BCE loss: 0.261, amex train: 0.720, val 0.000\n",
      "46, BCE loss: 0.255, amex train: 0.725, val 0.000\n",
      "46, BCE loss: 0.266, amex train: 0.715, val 0.000\n",
      "46, BCE loss: 0.260, amex train: 0.726, val 0.000\n",
      "46, BCE loss: 0.269, amex train: 0.712, val 0.000\n",
      "46, BCE loss: 0.257, amex train: 0.727, val 0.000\n",
      "46, BCE loss: 0.260, amex train: 0.728, val 0.000\n",
      "46, BCE loss: 0.261, amex train: 0.726, val 0.000\n",
      "46, BCE loss: 0.267, amex train: 0.714, val 0.000\n",
      "46, BCE loss: 0.248, amex train: 0.746, val 0.000\n",
      "47, BCE loss: 0.267, amex train: 0.708, val 0.000\n",
      "47, BCE loss: 0.258, amex train: 0.730, val 0.000\n",
      "47, BCE loss: 0.257, amex train: 0.735, val 0.000\n",
      "47, BCE loss: 0.257, amex train: 0.729, val 0.000\n",
      "47, BCE loss: 0.257, amex train: 0.721, val 0.000\n",
      "47, BCE loss: 0.259, amex train: 0.721, val 0.000\n",
      "47, BCE loss: 0.259, amex train: 0.729, val 0.000\n",
      "47, BCE loss: 0.258, amex train: 0.722, val 0.000\n",
      "47, BCE loss: 0.260, amex train: 0.721, val 0.000\n",
      "47, BCE loss: 0.259, amex train: 0.727, val 0.000\n",
      "47, BCE loss: 0.262, amex train: 0.716, val 0.000\n",
      "47, BCE loss: 0.261, amex train: 0.720, val 0.000\n",
      "47, BCE loss: 0.255, amex train: 0.726, val 0.000\n",
      "47, BCE loss: 0.265, amex train: 0.716, val 0.000\n",
      "47, BCE loss: 0.260, amex train: 0.726, val 0.000\n",
      "47, BCE loss: 0.269, amex train: 0.712, val 0.000\n",
      "47, BCE loss: 0.257, amex train: 0.727, val 0.000\n",
      "47, BCE loss: 0.260, amex train: 0.728, val 0.000\n",
      "47, BCE loss: 0.261, amex train: 0.725, val 0.000\n",
      "47, BCE loss: 0.266, amex train: 0.715, val 0.000\n",
      "47, BCE loss: 0.248, amex train: 0.747, val 0.000\n",
      "48, BCE loss: 0.267, amex train: 0.708, val 0.000\n",
      "48, BCE loss: 0.258, amex train: 0.732, val 0.000\n",
      "48, BCE loss: 0.257, amex train: 0.736, val 0.000\n",
      "48, BCE loss: 0.257, amex train: 0.730, val 0.000\n",
      "48, BCE loss: 0.257, amex train: 0.721, val 0.000\n",
      "48, BCE loss: 0.259, amex train: 0.721, val 0.000\n",
      "48, BCE loss: 0.259, amex train: 0.729, val 0.000\n",
      "48, BCE loss: 0.258, amex train: 0.723, val 0.000\n",
      "48, BCE loss: 0.259, amex train: 0.721, val 0.000\n",
      "48, BCE loss: 0.258, amex train: 0.727, val 0.000\n",
      "48, BCE loss: 0.261, amex train: 0.717, val 0.000\n",
      "48, BCE loss: 0.261, amex train: 0.720, val 0.000\n",
      "48, BCE loss: 0.255, amex train: 0.726, val 0.000\n",
      "48, BCE loss: 0.265, amex train: 0.717, val 0.000\n",
      "48, BCE loss: 0.260, amex train: 0.725, val 0.000\n",
      "48, BCE loss: 0.269, amex train: 0.713, val 0.000\n",
      "48, BCE loss: 0.257, amex train: 0.727, val 0.000\n",
      "48, BCE loss: 0.260, amex train: 0.729, val 0.000\n",
      "48, BCE loss: 0.261, amex train: 0.726, val 0.000\n",
      "48, BCE loss: 0.266, amex train: 0.715, val 0.000\n",
      "48, BCE loss: 0.248, amex train: 0.747, val 0.000\n",
      "49, BCE loss: 0.267, amex train: 0.708, val 0.000\n",
      "49, BCE loss: 0.258, amex train: 0.732, val 0.000\n",
      "49, BCE loss: 0.257, amex train: 0.736, val 0.000\n",
      "49, BCE loss: 0.256, amex train: 0.729, val 0.000\n",
      "49, BCE loss: 0.257, amex train: 0.720, val 0.000\n",
      "49, BCE loss: 0.259, amex train: 0.722, val 0.000\n",
      "49, BCE loss: 0.259, amex train: 0.728, val 0.000\n",
      "49, BCE loss: 0.258, amex train: 0.723, val 0.000\n",
      "49, BCE loss: 0.259, amex train: 0.721, val 0.000\n",
      "49, BCE loss: 0.258, amex train: 0.727, val 0.000\n",
      "49, BCE loss: 0.261, amex train: 0.718, val 0.000\n",
      "49, BCE loss: 0.261, amex train: 0.720, val 0.000\n",
      "49, BCE loss: 0.255, amex train: 0.726, val 0.000\n",
      "49, BCE loss: 0.265, amex train: 0.717, val 0.000\n",
      "49, BCE loss: 0.260, amex train: 0.725, val 0.000\n",
      "49, BCE loss: 0.268, amex train: 0.713, val 0.000\n",
      "49, BCE loss: 0.257, amex train: 0.727, val 0.000\n",
      "49, BCE loss: 0.260, amex train: 0.728, val 0.000\n",
      "49, BCE loss: 0.261, amex train: 0.727, val 0.000\n",
      "49, BCE loss: 0.266, amex train: 0.715, val 0.000\n",
      "49, BCE loss: 0.247, amex train: 0.748, val 0.000\n",
      "50, BCE loss: 0.267, amex train: 0.708, val 0.000\n",
      "50, BCE loss: 0.258, amex train: 0.733, val 0.000\n",
      "50, BCE loss: 0.257, amex train: 0.736, val 0.000\n",
      "50, BCE loss: 0.256, amex train: 0.731, val 0.000\n",
      "50, BCE loss: 0.256, amex train: 0.721, val 0.000\n",
      "50, BCE loss: 0.258, amex train: 0.722, val 0.000\n",
      "50, BCE loss: 0.259, amex train: 0.729, val 0.000\n",
      "50, BCE loss: 0.258, amex train: 0.723, val 0.000\n",
      "50, BCE loss: 0.259, amex train: 0.723, val 0.000\n",
      "50, BCE loss: 0.258, amex train: 0.728, val 0.000\n",
      "50, BCE loss: 0.261, amex train: 0.719, val 0.000\n",
      "50, BCE loss: 0.261, amex train: 0.720, val 0.000\n",
      "50, BCE loss: 0.255, amex train: 0.726, val 0.000\n",
      "50, BCE loss: 0.265, amex train: 0.718, val 0.000\n",
      "50, BCE loss: 0.260, amex train: 0.725, val 0.000\n",
      "50, BCE loss: 0.268, amex train: 0.714, val 0.000\n",
      "50, BCE loss: 0.257, amex train: 0.727, val 0.000\n",
      "50, BCE loss: 0.259, amex train: 0.728, val 0.000\n",
      "50, BCE loss: 0.261, amex train: 0.727, val 0.000\n",
      "50, BCE loss: 0.265, amex train: 0.716, val 0.000\n",
      "50, BCE loss: 0.247, amex train: 0.749, val 0.000\n",
      "51, BCE loss: 0.266, amex train: 0.708, val 0.000\n",
      "51, BCE loss: 0.257, amex train: 0.733, val 0.000\n",
      "51, BCE loss: 0.257, amex train: 0.736, val 0.000\n",
      "51, BCE loss: 0.256, amex train: 0.731, val 0.000\n",
      "51, BCE loss: 0.256, amex train: 0.722, val 0.000\n",
      "51, BCE loss: 0.258, amex train: 0.722, val 0.000\n",
      "51, BCE loss: 0.259, amex train: 0.729, val 0.000\n",
      "51, BCE loss: 0.257, amex train: 0.723, val 0.000\n",
      "51, BCE loss: 0.259, amex train: 0.723, val 0.000\n",
      "51, BCE loss: 0.258, amex train: 0.728, val 0.000\n",
      "51, BCE loss: 0.261, amex train: 0.719, val 0.000\n",
      "51, BCE loss: 0.261, amex train: 0.721, val 0.000\n",
      "51, BCE loss: 0.254, amex train: 0.727, val 0.000\n",
      "51, BCE loss: 0.265, amex train: 0.718, val 0.000\n",
      "51, BCE loss: 0.259, amex train: 0.724, val 0.000\n",
      "51, BCE loss: 0.268, amex train: 0.714, val 0.000\n",
      "51, BCE loss: 0.256, amex train: 0.727, val 0.000\n",
      "51, BCE loss: 0.259, amex train: 0.728, val 0.000\n",
      "51, BCE loss: 0.261, amex train: 0.727, val 0.000\n",
      "51, BCE loss: 0.265, amex train: 0.716, val 0.000\n",
      "51, BCE loss: 0.247, amex train: 0.749, val 0.000\n",
      "52, BCE loss: 0.266, amex train: 0.708, val 0.000\n",
      "52, BCE loss: 0.257, amex train: 0.733, val 0.000\n",
      "52, BCE loss: 0.256, amex train: 0.735, val 0.000\n",
      "52, BCE loss: 0.256, amex train: 0.731, val 0.000\n",
      "52, BCE loss: 0.256, amex train: 0.723, val 0.000\n",
      "52, BCE loss: 0.258, amex train: 0.722, val 0.000\n",
      "52, BCE loss: 0.259, amex train: 0.731, val 0.000\n",
      "52, BCE loss: 0.257, amex train: 0.724, val 0.000\n",
      "52, BCE loss: 0.259, amex train: 0.724, val 0.000\n",
      "52, BCE loss: 0.258, amex train: 0.727, val 0.000\n",
      "52, BCE loss: 0.260, amex train: 0.720, val 0.000\n",
      "52, BCE loss: 0.260, amex train: 0.721, val 0.000\n",
      "52, BCE loss: 0.254, amex train: 0.727, val 0.000\n",
      "52, BCE loss: 0.265, amex train: 0.718, val 0.000\n",
      "52, BCE loss: 0.259, amex train: 0.725, val 0.000\n",
      "52, BCE loss: 0.268, amex train: 0.715, val 0.000\n",
      "52, BCE loss: 0.256, amex train: 0.727, val 0.000\n",
      "52, BCE loss: 0.259, amex train: 0.728, val 0.000\n",
      "52, BCE loss: 0.260, amex train: 0.728, val 0.000\n",
      "52, BCE loss: 0.265, amex train: 0.716, val 0.000\n",
      "52, BCE loss: 0.247, amex train: 0.749, val 0.000\n",
      "53, BCE loss: 0.266, amex train: 0.708, val 0.000\n",
      "53, BCE loss: 0.257, amex train: 0.734, val 0.000\n",
      "53, BCE loss: 0.256, amex train: 0.736, val 0.000\n",
      "53, BCE loss: 0.256, amex train: 0.731, val 0.000\n",
      "53, BCE loss: 0.256, amex train: 0.723, val 0.000\n",
      "53, BCE loss: 0.258, amex train: 0.722, val 0.000\n",
      "53, BCE loss: 0.258, amex train: 0.731, val 0.000\n",
      "53, BCE loss: 0.257, amex train: 0.724, val 0.000\n",
      "53, BCE loss: 0.259, amex train: 0.723, val 0.000\n",
      "53, BCE loss: 0.258, amex train: 0.726, val 0.000\n",
      "53, BCE loss: 0.260, amex train: 0.720, val 0.000\n",
      "53, BCE loss: 0.260, amex train: 0.722, val 0.000\n",
      "53, BCE loss: 0.254, amex train: 0.727, val 0.000\n",
      "53, BCE loss: 0.264, amex train: 0.718, val 0.000\n",
      "53, BCE loss: 0.259, amex train: 0.725, val 0.000\n",
      "53, BCE loss: 0.268, amex train: 0.715, val 0.000\n",
      "53, BCE loss: 0.256, amex train: 0.729, val 0.000\n",
      "53, BCE loss: 0.259, amex train: 0.728, val 0.000\n",
      "53, BCE loss: 0.260, amex train: 0.728, val 0.000\n",
      "53, BCE loss: 0.265, amex train: 0.717, val 0.000\n",
      "53, BCE loss: 0.246, amex train: 0.748, val 0.000\n",
      "54, BCE loss: 0.266, amex train: 0.708, val 0.000\n",
      "54, BCE loss: 0.257, amex train: 0.734, val 0.000\n",
      "54, BCE loss: 0.256, amex train: 0.735, val 0.000\n",
      "54, BCE loss: 0.256, amex train: 0.731, val 0.000\n",
      "54, BCE loss: 0.256, amex train: 0.723, val 0.000\n",
      "54, BCE loss: 0.258, amex train: 0.723, val 0.000\n",
      "54, BCE loss: 0.258, amex train: 0.732, val 0.000\n",
      "54, BCE loss: 0.257, amex train: 0.724, val 0.000\n",
      "54, BCE loss: 0.258, amex train: 0.724, val 0.000\n",
      "54, BCE loss: 0.257, amex train: 0.727, val 0.000\n",
      "54, BCE loss: 0.260, amex train: 0.722, val 0.000\n",
      "54, BCE loss: 0.260, amex train: 0.722, val 0.000\n",
      "54, BCE loss: 0.254, amex train: 0.728, val 0.000\n",
      "54, BCE loss: 0.264, amex train: 0.718, val 0.000\n",
      "54, BCE loss: 0.259, amex train: 0.724, val 0.000\n",
      "54, BCE loss: 0.267, amex train: 0.716, val 0.000\n",
      "54, BCE loss: 0.256, amex train: 0.728, val 0.000\n",
      "54, BCE loss: 0.259, amex train: 0.729, val 0.000\n",
      "54, BCE loss: 0.260, amex train: 0.728, val 0.000\n",
      "54, BCE loss: 0.265, amex train: 0.718, val 0.000\n",
      "54, BCE loss: 0.246, amex train: 0.750, val 0.000\n",
      "55, BCE loss: 0.266, amex train: 0.708, val 0.000\n",
      "55, BCE loss: 0.257, amex train: 0.734, val 0.000\n",
      "55, BCE loss: 0.256, amex train: 0.736, val 0.000\n",
      "55, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "55, BCE loss: 0.256, amex train: 0.723, val 0.000\n",
      "55, BCE loss: 0.258, amex train: 0.723, val 0.000\n",
      "55, BCE loss: 0.258, amex train: 0.732, val 0.000\n",
      "55, BCE loss: 0.257, amex train: 0.725, val 0.000\n",
      "55, BCE loss: 0.258, amex train: 0.724, val 0.000\n",
      "55, BCE loss: 0.257, amex train: 0.728, val 0.000\n",
      "55, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "55, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "55, BCE loss: 0.254, amex train: 0.728, val 0.000\n",
      "55, BCE loss: 0.264, amex train: 0.718, val 0.000\n",
      "55, BCE loss: 0.259, amex train: 0.725, val 0.000\n",
      "55, BCE loss: 0.267, amex train: 0.717, val 0.000\n",
      "55, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "55, BCE loss: 0.258, amex train: 0.729, val 0.000\n",
      "55, BCE loss: 0.260, amex train: 0.728, val 0.000\n",
      "55, BCE loss: 0.264, amex train: 0.718, val 0.000\n",
      "55, BCE loss: 0.246, amex train: 0.750, val 0.000\n",
      "56, BCE loss: 0.265, amex train: 0.709, val 0.000\n",
      "56, BCE loss: 0.256, amex train: 0.735, val 0.000\n",
      "56, BCE loss: 0.256, amex train: 0.736, val 0.000\n",
      "56, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "56, BCE loss: 0.255, amex train: 0.724, val 0.000\n",
      "56, BCE loss: 0.257, amex train: 0.723, val 0.000\n",
      "56, BCE loss: 0.258, amex train: 0.732, val 0.000\n",
      "56, BCE loss: 0.257, amex train: 0.724, val 0.000\n",
      "56, BCE loss: 0.258, amex train: 0.725, val 0.000\n",
      "56, BCE loss: 0.257, amex train: 0.728, val 0.000\n",
      "56, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "56, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "56, BCE loss: 0.253, amex train: 0.729, val 0.000\n",
      "56, BCE loss: 0.264, amex train: 0.719, val 0.000\n",
      "56, BCE loss: 0.258, amex train: 0.726, val 0.000\n",
      "56, BCE loss: 0.267, amex train: 0.718, val 0.000\n",
      "56, BCE loss: 0.256, amex train: 0.729, val 0.000\n",
      "56, BCE loss: 0.258, amex train: 0.729, val 0.000\n",
      "56, BCE loss: 0.260, amex train: 0.727, val 0.000\n",
      "56, BCE loss: 0.264, amex train: 0.719, val 0.000\n",
      "56, BCE loss: 0.246, amex train: 0.749, val 0.000\n",
      "57, BCE loss: 0.265, amex train: 0.709, val 0.000\n",
      "57, BCE loss: 0.256, amex train: 0.735, val 0.000\n",
      "57, BCE loss: 0.256, amex train: 0.736, val 0.000\n",
      "57, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "57, BCE loss: 0.255, amex train: 0.724, val 0.000\n",
      "57, BCE loss: 0.257, amex train: 0.725, val 0.000\n",
      "57, BCE loss: 0.258, amex train: 0.732, val 0.000\n",
      "57, BCE loss: 0.256, amex train: 0.724, val 0.000\n",
      "57, BCE loss: 0.258, amex train: 0.725, val 0.000\n",
      "57, BCE loss: 0.257, amex train: 0.728, val 0.000\n",
      "57, BCE loss: 0.259, amex train: 0.723, val 0.000\n",
      "57, BCE loss: 0.260, amex train: 0.722, val 0.000\n",
      "57, BCE loss: 0.253, amex train: 0.729, val 0.000\n",
      "57, BCE loss: 0.264, amex train: 0.719, val 0.000\n",
      "57, BCE loss: 0.258, amex train: 0.725, val 0.000\n",
      "57, BCE loss: 0.267, amex train: 0.719, val 0.000\n",
      "57, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "57, BCE loss: 0.258, amex train: 0.729, val 0.000\n",
      "57, BCE loss: 0.260, amex train: 0.727, val 0.000\n",
      "57, BCE loss: 0.264, amex train: 0.720, val 0.000\n",
      "57, BCE loss: 0.245, amex train: 0.750, val 0.000\n",
      "58, BCE loss: 0.265, amex train: 0.709, val 0.000\n",
      "58, BCE loss: 0.256, amex train: 0.735, val 0.000\n",
      "58, BCE loss: 0.255, amex train: 0.736, val 0.000\n",
      "58, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "58, BCE loss: 0.255, amex train: 0.724, val 0.000\n",
      "58, BCE loss: 0.257, amex train: 0.725, val 0.000\n",
      "58, BCE loss: 0.258, amex train: 0.732, val 0.000\n",
      "58, BCE loss: 0.256, amex train: 0.725, val 0.000\n",
      "58, BCE loss: 0.258, amex train: 0.725, val 0.000\n",
      "58, BCE loss: 0.257, amex train: 0.729, val 0.000\n",
      "58, BCE loss: 0.259, amex train: 0.723, val 0.000\n",
      "58, BCE loss: 0.259, amex train: 0.723, val 0.000\n",
      "58, BCE loss: 0.253, amex train: 0.729, val 0.000\n",
      "58, BCE loss: 0.264, amex train: 0.719, val 0.000\n",
      "58, BCE loss: 0.258, amex train: 0.726, val 0.000\n",
      "58, BCE loss: 0.267, amex train: 0.720, val 0.000\n",
      "58, BCE loss: 0.256, amex train: 0.731, val 0.000\n",
      "58, BCE loss: 0.258, amex train: 0.730, val 0.000\n",
      "58, BCE loss: 0.259, amex train: 0.726, val 0.000\n",
      "58, BCE loss: 0.264, amex train: 0.719, val 0.000\n",
      "58, BCE loss: 0.245, amex train: 0.751, val 0.000\n",
      "59, BCE loss: 0.265, amex train: 0.709, val 0.000\n",
      "59, BCE loss: 0.256, amex train: 0.735, val 0.000\n",
      "59, BCE loss: 0.255, amex train: 0.736, val 0.000\n",
      "59, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "59, BCE loss: 0.255, amex train: 0.723, val 0.000\n",
      "59, BCE loss: 0.257, amex train: 0.724, val 0.000\n",
      "59, BCE loss: 0.258, amex train: 0.732, val 0.000\n",
      "59, BCE loss: 0.256, amex train: 0.725, val 0.000\n",
      "59, BCE loss: 0.258, amex train: 0.726, val 0.000\n",
      "59, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "59, BCE loss: 0.259, amex train: 0.722, val 0.000\n",
      "59, BCE loss: 0.259, amex train: 0.723, val 0.000\n",
      "59, BCE loss: 0.253, amex train: 0.729, val 0.000\n",
      "59, BCE loss: 0.263, amex train: 0.719, val 0.000\n",
      "59, BCE loss: 0.258, amex train: 0.726, val 0.000\n",
      "59, BCE loss: 0.267, amex train: 0.719, val 0.000\n",
      "59, BCE loss: 0.255, amex train: 0.731, val 0.000\n",
      "59, BCE loss: 0.258, amex train: 0.730, val 0.000\n",
      "59, BCE loss: 0.259, amex train: 0.727, val 0.000\n",
      "59, BCE loss: 0.264, amex train: 0.718, val 0.000\n",
      "59, BCE loss: 0.244, amex train: 0.751, val 0.000\n",
      "60, BCE loss: 0.265, amex train: 0.709, val 0.000\n",
      "60, BCE loss: 0.256, amex train: 0.734, val 0.000\n",
      "60, BCE loss: 0.255, amex train: 0.736, val 0.000\n",
      "60, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "60, BCE loss: 0.255, amex train: 0.724, val 0.000\n",
      "60, BCE loss: 0.257, amex train: 0.724, val 0.000\n",
      "60, BCE loss: 0.258, amex train: 0.733, val 0.000\n",
      "60, BCE loss: 0.256, amex train: 0.725, val 0.000\n",
      "60, BCE loss: 0.258, amex train: 0.726, val 0.000\n",
      "60, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "60, BCE loss: 0.259, amex train: 0.722, val 0.000\n",
      "60, BCE loss: 0.259, amex train: 0.724, val 0.000\n",
      "60, BCE loss: 0.253, amex train: 0.728, val 0.000\n",
      "60, BCE loss: 0.263, amex train: 0.720, val 0.000\n",
      "60, BCE loss: 0.258, amex train: 0.727, val 0.000\n",
      "60, BCE loss: 0.267, amex train: 0.719, val 0.000\n",
      "60, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "60, BCE loss: 0.258, amex train: 0.731, val 0.000\n",
      "60, BCE loss: 0.259, amex train: 0.728, val 0.000\n",
      "60, BCE loss: 0.264, amex train: 0.719, val 0.000\n",
      "60, BCE loss: 0.244, amex train: 0.752, val 0.000\n",
      "61, BCE loss: 0.264, amex train: 0.710, val 0.000\n",
      "61, BCE loss: 0.256, amex train: 0.735, val 0.000\n",
      "61, BCE loss: 0.255, amex train: 0.737, val 0.000\n",
      "61, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "61, BCE loss: 0.255, amex train: 0.724, val 0.000\n",
      "61, BCE loss: 0.256, amex train: 0.725, val 0.000\n",
      "61, BCE loss: 0.258, amex train: 0.733, val 0.000\n",
      "61, BCE loss: 0.256, amex train: 0.725, val 0.000\n",
      "61, BCE loss: 0.257, amex train: 0.726, val 0.000\n",
      "61, BCE loss: 0.256, amex train: 0.732, val 0.000\n",
      "61, BCE loss: 0.259, amex train: 0.721, val 0.000\n",
      "61, BCE loss: 0.259, amex train: 0.724, val 0.000\n",
      "61, BCE loss: 0.253, amex train: 0.728, val 0.000\n",
      "61, BCE loss: 0.263, amex train: 0.720, val 0.000\n",
      "61, BCE loss: 0.258, amex train: 0.728, val 0.000\n",
      "61, BCE loss: 0.266, amex train: 0.719, val 0.000\n",
      "61, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "61, BCE loss: 0.258, amex train: 0.731, val 0.000\n",
      "61, BCE loss: 0.259, amex train: 0.728, val 0.000\n",
      "61, BCE loss: 0.263, amex train: 0.720, val 0.000\n",
      "61, BCE loss: 0.244, amex train: 0.752, val 0.000\n",
      "62, BCE loss: 0.264, amex train: 0.710, val 0.000\n",
      "62, BCE loss: 0.256, amex train: 0.735, val 0.000\n",
      "62, BCE loss: 0.255, amex train: 0.737, val 0.000\n",
      "62, BCE loss: 0.254, amex train: 0.733, val 0.000\n",
      "62, BCE loss: 0.255, amex train: 0.725, val 0.000\n",
      "62, BCE loss: 0.256, amex train: 0.725, val 0.000\n",
      "62, BCE loss: 0.257, amex train: 0.733, val 0.000\n",
      "62, BCE loss: 0.256, amex train: 0.726, val 0.000\n",
      "62, BCE loss: 0.257, amex train: 0.726, val 0.000\n",
      "62, BCE loss: 0.256, amex train: 0.732, val 0.000\n",
      "62, BCE loss: 0.259, amex train: 0.720, val 0.000\n",
      "62, BCE loss: 0.259, amex train: 0.724, val 0.000\n",
      "62, BCE loss: 0.253, amex train: 0.727, val 0.000\n",
      "62, BCE loss: 0.263, amex train: 0.720, val 0.000\n",
      "62, BCE loss: 0.258, amex train: 0.728, val 0.000\n",
      "62, BCE loss: 0.266, amex train: 0.719, val 0.000\n",
      "62, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "62, BCE loss: 0.258, amex train: 0.731, val 0.000\n",
      "62, BCE loss: 0.258, amex train: 0.728, val 0.000\n",
      "62, BCE loss: 0.263, amex train: 0.722, val 0.000\n",
      "62, BCE loss: 0.243, amex train: 0.753, val 0.000\n",
      "63, BCE loss: 0.264, amex train: 0.711, val 0.000\n",
      "63, BCE loss: 0.256, amex train: 0.735, val 0.000\n",
      "63, BCE loss: 0.254, amex train: 0.738, val 0.000\n",
      "63, BCE loss: 0.254, amex train: 0.733, val 0.000\n",
      "63, BCE loss: 0.255, amex train: 0.725, val 0.000\n",
      "63, BCE loss: 0.256, amex train: 0.726, val 0.000\n",
      "63, BCE loss: 0.257, amex train: 0.734, val 0.000\n",
      "63, BCE loss: 0.256, amex train: 0.727, val 0.000\n",
      "63, BCE loss: 0.257, amex train: 0.727, val 0.000\n",
      "63, BCE loss: 0.256, amex train: 0.732, val 0.000\n",
      "63, BCE loss: 0.258, amex train: 0.720, val 0.000\n",
      "63, BCE loss: 0.258, amex train: 0.725, val 0.000\n",
      "63, BCE loss: 0.253, amex train: 0.728, val 0.000\n",
      "63, BCE loss: 0.263, amex train: 0.720, val 0.000\n",
      "63, BCE loss: 0.258, amex train: 0.729, val 0.000\n",
      "63, BCE loss: 0.266, amex train: 0.719, val 0.000\n",
      "63, BCE loss: 0.254, amex train: 0.732, val 0.000\n",
      "63, BCE loss: 0.258, amex train: 0.731, val 0.000\n",
      "63, BCE loss: 0.258, amex train: 0.729, val 0.000\n",
      "63, BCE loss: 0.263, amex train: 0.723, val 0.000\n",
      "63, BCE loss: 0.243, amex train: 0.753, val 0.000\n",
      "64, BCE loss: 0.264, amex train: 0.712, val 0.000\n",
      "64, BCE loss: 0.256, amex train: 0.734, val 0.000\n",
      "64, BCE loss: 0.254, amex train: 0.738, val 0.000\n",
      "64, BCE loss: 0.254, amex train: 0.734, val 0.000\n",
      "64, BCE loss: 0.255, amex train: 0.725, val 0.000\n",
      "64, BCE loss: 0.256, amex train: 0.726, val 0.000\n",
      "64, BCE loss: 0.257, amex train: 0.735, val 0.000\n",
      "64, BCE loss: 0.256, amex train: 0.727, val 0.000\n",
      "64, BCE loss: 0.257, amex train: 0.727, val 0.000\n",
      "64, BCE loss: 0.256, amex train: 0.732, val 0.000\n",
      "64, BCE loss: 0.258, amex train: 0.720, val 0.000\n",
      "64, BCE loss: 0.258, amex train: 0.725, val 0.000\n",
      "64, BCE loss: 0.253, amex train: 0.727, val 0.000\n",
      "64, BCE loss: 0.262, amex train: 0.720, val 0.000\n",
      "64, BCE loss: 0.257, amex train: 0.730, val 0.000\n",
      "64, BCE loss: 0.266, amex train: 0.719, val 0.000\n",
      "64, BCE loss: 0.254, amex train: 0.732, val 0.000\n",
      "64, BCE loss: 0.258, amex train: 0.732, val 0.000\n",
      "64, BCE loss: 0.258, amex train: 0.728, val 0.000\n",
      "64, BCE loss: 0.263, amex train: 0.723, val 0.000\n",
      "64, BCE loss: 0.243, amex train: 0.753, val 0.000\n",
      "65, BCE loss: 0.264, amex train: 0.712, val 0.000\n",
      "65, BCE loss: 0.255, amex train: 0.734, val 0.000\n",
      "65, BCE loss: 0.254, amex train: 0.738, val 0.000\n",
      "65, BCE loss: 0.254, amex train: 0.734, val 0.000\n",
      "65, BCE loss: 0.254, amex train: 0.725, val 0.000\n",
      "65, BCE loss: 0.256, amex train: 0.727, val 0.000\n",
      "65, BCE loss: 0.257, amex train: 0.736, val 0.000\n",
      "65, BCE loss: 0.256, amex train: 0.728, val 0.000\n",
      "65, BCE loss: 0.256, amex train: 0.728, val 0.000\n",
      "65, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "65, BCE loss: 0.258, amex train: 0.721, val 0.000\n",
      "65, BCE loss: 0.258, amex train: 0.726, val 0.000\n",
      "65, BCE loss: 0.252, amex train: 0.727, val 0.000\n",
      "65, BCE loss: 0.262, amex train: 0.720, val 0.000\n",
      "65, BCE loss: 0.257, amex train: 0.730, val 0.000\n",
      "65, BCE loss: 0.266, amex train: 0.720, val 0.000\n",
      "65, BCE loss: 0.254, amex train: 0.732, val 0.000\n",
      "65, BCE loss: 0.257, amex train: 0.732, val 0.000\n",
      "65, BCE loss: 0.258, amex train: 0.729, val 0.000\n",
      "65, BCE loss: 0.262, amex train: 0.722, val 0.000\n",
      "65, BCE loss: 0.242, amex train: 0.753, val 0.000\n",
      "66, BCE loss: 0.263, amex train: 0.713, val 0.000\n",
      "66, BCE loss: 0.255, amex train: 0.734, val 0.000\n",
      "66, BCE loss: 0.254, amex train: 0.739, val 0.000\n",
      "66, BCE loss: 0.253, amex train: 0.735, val 0.000\n",
      "66, BCE loss: 0.254, amex train: 0.725, val 0.000\n",
      "66, BCE loss: 0.255, amex train: 0.727, val 0.000\n",
      "66, BCE loss: 0.257, amex train: 0.736, val 0.000\n",
      "66, BCE loss: 0.255, amex train: 0.727, val 0.000\n",
      "66, BCE loss: 0.256, amex train: 0.728, val 0.000\n",
      "66, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "66, BCE loss: 0.258, amex train: 0.721, val 0.000\n",
      "66, BCE loss: 0.258, amex train: 0.726, val 0.000\n",
      "66, BCE loss: 0.252, amex train: 0.729, val 0.000\n",
      "66, BCE loss: 0.262, amex train: 0.720, val 0.000\n",
      "66, BCE loss: 0.257, amex train: 0.729, val 0.000\n",
      "66, BCE loss: 0.266, amex train: 0.721, val 0.000\n",
      "66, BCE loss: 0.254, amex train: 0.732, val 0.000\n",
      "66, BCE loss: 0.257, amex train: 0.733, val 0.000\n",
      "66, BCE loss: 0.258, amex train: 0.729, val 0.000\n",
      "66, BCE loss: 0.262, amex train: 0.723, val 0.000\n",
      "66, BCE loss: 0.242, amex train: 0.755, val 0.000\n",
      "67, BCE loss: 0.263, amex train: 0.713, val 0.000\n",
      "67, BCE loss: 0.255, amex train: 0.734, val 0.000\n",
      "67, BCE loss: 0.254, amex train: 0.740, val 0.000\n",
      "67, BCE loss: 0.253, amex train: 0.735, val 0.000\n",
      "67, BCE loss: 0.254, amex train: 0.725, val 0.000\n",
      "67, BCE loss: 0.255, amex train: 0.727, val 0.000\n",
      "67, BCE loss: 0.256, amex train: 0.736, val 0.000\n",
      "67, BCE loss: 0.255, amex train: 0.728, val 0.000\n",
      "67, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "67, BCE loss: 0.255, amex train: 0.735, val 0.000\n",
      "67, BCE loss: 0.258, amex train: 0.722, val 0.000\n",
      "67, BCE loss: 0.258, amex train: 0.727, val 0.000\n",
      "67, BCE loss: 0.252, amex train: 0.730, val 0.000\n",
      "67, BCE loss: 0.262, amex train: 0.720, val 0.000\n",
      "67, BCE loss: 0.257, amex train: 0.730, val 0.000\n",
      "67, BCE loss: 0.266, amex train: 0.720, val 0.000\n",
      "67, BCE loss: 0.254, amex train: 0.733, val 0.000\n",
      "67, BCE loss: 0.257, amex train: 0.734, val 0.000\n",
      "67, BCE loss: 0.258, amex train: 0.729, val 0.000\n",
      "67, BCE loss: 0.262, amex train: 0.723, val 0.000\n",
      "67, BCE loss: 0.242, amex train: 0.755, val 0.000\n",
      "68, BCE loss: 0.263, amex train: 0.714, val 0.000\n",
      "68, BCE loss: 0.255, amex train: 0.736, val 0.000\n",
      "68, BCE loss: 0.253, amex train: 0.740, val 0.000\n",
      "68, BCE loss: 0.253, amex train: 0.736, val 0.000\n",
      "68, BCE loss: 0.254, amex train: 0.726, val 0.000\n",
      "68, BCE loss: 0.255, amex train: 0.727, val 0.000\n",
      "68, BCE loss: 0.256, amex train: 0.737, val 0.000\n",
      "68, BCE loss: 0.255, amex train: 0.728, val 0.000\n",
      "68, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "68, BCE loss: 0.255, amex train: 0.735, val 0.000\n",
      "68, BCE loss: 0.257, amex train: 0.722, val 0.000\n",
      "68, BCE loss: 0.257, amex train: 0.727, val 0.000\n",
      "68, BCE loss: 0.252, amex train: 0.730, val 0.000\n",
      "68, BCE loss: 0.262, amex train: 0.720, val 0.000\n",
      "68, BCE loss: 0.256, amex train: 0.729, val 0.000\n",
      "68, BCE loss: 0.265, amex train: 0.721, val 0.000\n",
      "68, BCE loss: 0.254, amex train: 0.733, val 0.000\n",
      "68, BCE loss: 0.257, amex train: 0.734, val 0.000\n",
      "68, BCE loss: 0.258, amex train: 0.729, val 0.000\n",
      "68, BCE loss: 0.262, amex train: 0.722, val 0.000\n",
      "68, BCE loss: 0.242, amex train: 0.754, val 0.000\n",
      "69, BCE loss: 0.263, amex train: 0.715, val 0.000\n",
      "69, BCE loss: 0.254, amex train: 0.737, val 0.000\n",
      "69, BCE loss: 0.253, amex train: 0.741, val 0.000\n",
      "69, BCE loss: 0.253, amex train: 0.734, val 0.000\n",
      "69, BCE loss: 0.253, amex train: 0.726, val 0.000\n",
      "69, BCE loss: 0.255, amex train: 0.727, val 0.000\n",
      "69, BCE loss: 0.256, amex train: 0.737, val 0.000\n",
      "69, BCE loss: 0.255, amex train: 0.728, val 0.000\n",
      "69, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "69, BCE loss: 0.255, amex train: 0.735, val 0.000\n",
      "69, BCE loss: 0.257, amex train: 0.723, val 0.000\n",
      "69, BCE loss: 0.257, amex train: 0.728, val 0.000\n",
      "69, BCE loss: 0.251, amex train: 0.731, val 0.000\n",
      "69, BCE loss: 0.261, amex train: 0.721, val 0.000\n",
      "69, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "69, BCE loss: 0.265, amex train: 0.722, val 0.000\n",
      "69, BCE loss: 0.253, amex train: 0.734, val 0.000\n",
      "69, BCE loss: 0.257, amex train: 0.734, val 0.000\n",
      "69, BCE loss: 0.258, amex train: 0.729, val 0.000\n",
      "69, BCE loss: 0.261, amex train: 0.723, val 0.000\n",
      "69, BCE loss: 0.242, amex train: 0.753, val 0.000\n",
      "70, BCE loss: 0.263, amex train: 0.714, val 0.000\n",
      "70, BCE loss: 0.254, amex train: 0.737, val 0.000\n",
      "70, BCE loss: 0.253, amex train: 0.741, val 0.000\n",
      "70, BCE loss: 0.252, amex train: 0.735, val 0.000\n",
      "70, BCE loss: 0.253, amex train: 0.727, val 0.000\n",
      "70, BCE loss: 0.255, amex train: 0.727, val 0.000\n",
      "70, BCE loss: 0.256, amex train: 0.737, val 0.000\n",
      "70, BCE loss: 0.254, amex train: 0.729, val 0.000\n",
      "70, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "70, BCE loss: 0.255, amex train: 0.736, val 0.000\n",
      "70, BCE loss: 0.257, amex train: 0.723, val 0.000\n",
      "70, BCE loss: 0.257, amex train: 0.728, val 0.000\n",
      "70, BCE loss: 0.251, amex train: 0.729, val 0.000\n",
      "70, BCE loss: 0.261, amex train: 0.722, val 0.000\n",
      "70, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "70, BCE loss: 0.265, amex train: 0.722, val 0.000\n",
      "70, BCE loss: 0.253, amex train: 0.734, val 0.000\n",
      "70, BCE loss: 0.256, amex train: 0.735, val 0.000\n",
      "70, BCE loss: 0.258, amex train: 0.730, val 0.000\n",
      "70, BCE loss: 0.261, amex train: 0.723, val 0.000\n",
      "70, BCE loss: 0.241, amex train: 0.755, val 0.000\n",
      "71, BCE loss: 0.262, amex train: 0.714, val 0.000\n",
      "71, BCE loss: 0.254, amex train: 0.739, val 0.000\n",
      "71, BCE loss: 0.253, amex train: 0.742, val 0.000\n",
      "71, BCE loss: 0.252, amex train: 0.735, val 0.000\n",
      "71, BCE loss: 0.253, amex train: 0.729, val 0.000\n",
      "71, BCE loss: 0.254, amex train: 0.727, val 0.000\n",
      "71, BCE loss: 0.256, amex train: 0.737, val 0.000\n",
      "71, BCE loss: 0.254, amex train: 0.729, val 0.000\n",
      "71, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "71, BCE loss: 0.255, amex train: 0.736, val 0.000\n",
      "71, BCE loss: 0.257, amex train: 0.724, val 0.000\n",
      "71, BCE loss: 0.257, amex train: 0.729, val 0.000\n",
      "71, BCE loss: 0.251, amex train: 0.730, val 0.000\n",
      "71, BCE loss: 0.261, amex train: 0.723, val 0.000\n",
      "71, BCE loss: 0.256, amex train: 0.731, val 0.000\n",
      "71, BCE loss: 0.265, amex train: 0.723, val 0.000\n",
      "71, BCE loss: 0.253, amex train: 0.734, val 0.000\n",
      "71, BCE loss: 0.256, amex train: 0.736, val 0.000\n",
      "71, BCE loss: 0.258, amex train: 0.729, val 0.000\n",
      "71, BCE loss: 0.261, amex train: 0.724, val 0.000\n",
      "71, BCE loss: 0.241, amex train: 0.757, val 0.000\n",
      "72, BCE loss: 0.262, amex train: 0.714, val 0.000\n",
      "72, BCE loss: 0.253, amex train: 0.739, val 0.000\n",
      "72, BCE loss: 0.253, amex train: 0.742, val 0.000\n",
      "72, BCE loss: 0.252, amex train: 0.736, val 0.000\n",
      "72, BCE loss: 0.253, amex train: 0.729, val 0.000\n",
      "72, BCE loss: 0.254, amex train: 0.727, val 0.000\n",
      "72, BCE loss: 0.256, amex train: 0.737, val 0.000\n",
      "72, BCE loss: 0.254, amex train: 0.730, val 0.000\n",
      "72, BCE loss: 0.256, amex train: 0.731, val 0.000\n",
      "72, BCE loss: 0.254, amex train: 0.736, val 0.000\n",
      "72, BCE loss: 0.257, amex train: 0.724, val 0.000\n",
      "72, BCE loss: 0.257, amex train: 0.729, val 0.000\n",
      "72, BCE loss: 0.251, amex train: 0.730, val 0.000\n",
      "72, BCE loss: 0.261, amex train: 0.723, val 0.000\n",
      "72, BCE loss: 0.256, amex train: 0.732, val 0.000\n",
      "72, BCE loss: 0.265, amex train: 0.724, val 0.000\n",
      "72, BCE loss: 0.253, amex train: 0.734, val 0.000\n",
      "72, BCE loss: 0.256, amex train: 0.736, val 0.000\n",
      "72, BCE loss: 0.257, amex train: 0.728, val 0.000\n",
      "72, BCE loss: 0.260, amex train: 0.724, val 0.000\n",
      "72, BCE loss: 0.241, amex train: 0.757, val 0.000\n",
      "73, BCE loss: 0.262, amex train: 0.714, val 0.000\n",
      "73, BCE loss: 0.253, amex train: 0.740, val 0.000\n",
      "73, BCE loss: 0.253, amex train: 0.743, val 0.000\n",
      "73, BCE loss: 0.252, amex train: 0.736, val 0.000\n",
      "73, BCE loss: 0.253, amex train: 0.731, val 0.000\n",
      "73, BCE loss: 0.254, amex train: 0.728, val 0.000\n",
      "73, BCE loss: 0.255, amex train: 0.738, val 0.000\n",
      "73, BCE loss: 0.254, amex train: 0.730, val 0.000\n",
      "73, BCE loss: 0.256, amex train: 0.731, val 0.000\n",
      "73, BCE loss: 0.254, amex train: 0.737, val 0.000\n",
      "73, BCE loss: 0.256, amex train: 0.725, val 0.000\n",
      "73, BCE loss: 0.257, amex train: 0.729, val 0.000\n",
      "73, BCE loss: 0.251, amex train: 0.730, val 0.000\n",
      "73, BCE loss: 0.261, amex train: 0.723, val 0.000\n",
      "73, BCE loss: 0.256, amex train: 0.731, val 0.000\n",
      "73, BCE loss: 0.265, amex train: 0.724, val 0.000\n",
      "73, BCE loss: 0.253, amex train: 0.734, val 0.000\n",
      "73, BCE loss: 0.256, amex train: 0.737, val 0.000\n",
      "73, BCE loss: 0.257, amex train: 0.728, val 0.000\n",
      "73, BCE loss: 0.260, amex train: 0.724, val 0.000\n",
      "73, BCE loss: 0.241, amex train: 0.757, val 0.000\n",
      "74, BCE loss: 0.262, amex train: 0.715, val 0.000\n",
      "74, BCE loss: 0.253, amex train: 0.739, val 0.000\n",
      "74, BCE loss: 0.253, amex train: 0.743, val 0.000\n",
      "74, BCE loss: 0.252, amex train: 0.737, val 0.000\n",
      "74, BCE loss: 0.252, amex train: 0.731, val 0.000\n",
      "74, BCE loss: 0.254, amex train: 0.728, val 0.000\n",
      "74, BCE loss: 0.255, amex train: 0.738, val 0.000\n",
      "74, BCE loss: 0.254, amex train: 0.730, val 0.000\n",
      "74, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "74, BCE loss: 0.254, amex train: 0.737, val 0.000\n",
      "74, BCE loss: 0.256, amex train: 0.725, val 0.000\n",
      "74, BCE loss: 0.256, amex train: 0.729, val 0.000\n",
      "74, BCE loss: 0.251, amex train: 0.730, val 0.000\n",
      "74, BCE loss: 0.261, amex train: 0.723, val 0.000\n",
      "74, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "74, BCE loss: 0.264, amex train: 0.724, val 0.000\n",
      "74, BCE loss: 0.253, amex train: 0.735, val 0.000\n",
      "74, BCE loss: 0.256, amex train: 0.737, val 0.000\n",
      "74, BCE loss: 0.257, amex train: 0.729, val 0.000\n",
      "74, BCE loss: 0.260, amex train: 0.724, val 0.000\n",
      "74, BCE loss: 0.241, amex train: 0.758, val 0.000\n",
      "75, BCE loss: 0.262, amex train: 0.716, val 0.000\n",
      "75, BCE loss: 0.253, amex train: 0.741, val 0.000\n",
      "75, BCE loss: 0.253, amex train: 0.744, val 0.000\n",
      "75, BCE loss: 0.251, amex train: 0.737, val 0.000\n",
      "75, BCE loss: 0.252, amex train: 0.730, val 0.000\n",
      "75, BCE loss: 0.254, amex train: 0.729, val 0.000\n",
      "75, BCE loss: 0.255, amex train: 0.738, val 0.000\n",
      "75, BCE loss: 0.254, amex train: 0.731, val 0.000\n",
      "75, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "75, BCE loss: 0.254, amex train: 0.737, val 0.000\n",
      "75, BCE loss: 0.256, amex train: 0.726, val 0.000\n",
      "75, BCE loss: 0.256, amex train: 0.729, val 0.000\n",
      "75, BCE loss: 0.250, amex train: 0.731, val 0.000\n",
      "75, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "75, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "75, BCE loss: 0.264, amex train: 0.724, val 0.000\n",
      "75, BCE loss: 0.252, amex train: 0.735, val 0.000\n",
      "75, BCE loss: 0.255, amex train: 0.737, val 0.000\n",
      "75, BCE loss: 0.257, amex train: 0.728, val 0.000\n",
      "75, BCE loss: 0.260, amex train: 0.725, val 0.000\n",
      "75, BCE loss: 0.240, amex train: 0.759, val 0.000\n",
      "76, BCE loss: 0.262, amex train: 0.716, val 0.000\n",
      "76, BCE loss: 0.252, amex train: 0.741, val 0.000\n",
      "76, BCE loss: 0.253, amex train: 0.743, val 0.000\n",
      "76, BCE loss: 0.251, amex train: 0.737, val 0.000\n",
      "76, BCE loss: 0.252, amex train: 0.730, val 0.000\n",
      "76, BCE loss: 0.254, amex train: 0.729, val 0.000\n",
      "76, BCE loss: 0.255, amex train: 0.738, val 0.000\n",
      "76, BCE loss: 0.254, amex train: 0.731, val 0.000\n",
      "76, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "76, BCE loss: 0.254, amex train: 0.736, val 0.000\n",
      "76, BCE loss: 0.256, amex train: 0.725, val 0.000\n",
      "76, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "76, BCE loss: 0.250, amex train: 0.732, val 0.000\n",
      "76, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "76, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "76, BCE loss: 0.264, amex train: 0.725, val 0.000\n",
      "76, BCE loss: 0.252, amex train: 0.736, val 0.000\n",
      "76, BCE loss: 0.255, amex train: 0.737, val 0.000\n",
      "76, BCE loss: 0.257, amex train: 0.730, val 0.000\n",
      "76, BCE loss: 0.259, amex train: 0.725, val 0.000\n",
      "76, BCE loss: 0.240, amex train: 0.759, val 0.000\n",
      "77, BCE loss: 0.262, amex train: 0.716, val 0.000\n",
      "77, BCE loss: 0.252, amex train: 0.742, val 0.000\n",
      "77, BCE loss: 0.252, amex train: 0.744, val 0.000\n",
      "77, BCE loss: 0.251, amex train: 0.737, val 0.000\n",
      "77, BCE loss: 0.252, amex train: 0.729, val 0.000\n",
      "77, BCE loss: 0.254, amex train: 0.729, val 0.000\n",
      "77, BCE loss: 0.255, amex train: 0.739, val 0.000\n",
      "77, BCE loss: 0.254, amex train: 0.730, val 0.000\n",
      "77, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "77, BCE loss: 0.254, amex train: 0.737, val 0.000\n",
      "77, BCE loss: 0.256, amex train: 0.725, val 0.000\n",
      "77, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "77, BCE loss: 0.250, amex train: 0.731, val 0.000\n",
      "77, BCE loss: 0.260, amex train: 0.723, val 0.000\n",
      "77, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "77, BCE loss: 0.264, amex train: 0.725, val 0.000\n",
      "77, BCE loss: 0.252, amex train: 0.736, val 0.000\n",
      "77, BCE loss: 0.255, amex train: 0.737, val 0.000\n",
      "77, BCE loss: 0.257, amex train: 0.730, val 0.000\n",
      "77, BCE loss: 0.259, amex train: 0.726, val 0.000\n",
      "77, BCE loss: 0.240, amex train: 0.759, val 0.000\n",
      "78, BCE loss: 0.261, amex train: 0.716, val 0.000\n",
      "78, BCE loss: 0.252, amex train: 0.743, val 0.000\n",
      "78, BCE loss: 0.252, amex train: 0.744, val 0.000\n",
      "78, BCE loss: 0.250, amex train: 0.737, val 0.000\n",
      "78, BCE loss: 0.252, amex train: 0.729, val 0.000\n",
      "78, BCE loss: 0.253, amex train: 0.729, val 0.000\n",
      "78, BCE loss: 0.255, amex train: 0.740, val 0.000\n",
      "78, BCE loss: 0.254, amex train: 0.731, val 0.000\n",
      "78, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "78, BCE loss: 0.254, amex train: 0.736, val 0.000\n",
      "78, BCE loss: 0.255, amex train: 0.725, val 0.000\n",
      "78, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "78, BCE loss: 0.250, amex train: 0.732, val 0.000\n",
      "78, BCE loss: 0.260, amex train: 0.724, val 0.000\n",
      "78, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "78, BCE loss: 0.264, amex train: 0.725, val 0.000\n",
      "78, BCE loss: 0.252, amex train: 0.736, val 0.000\n",
      "78, BCE loss: 0.255, amex train: 0.737, val 0.000\n",
      "78, BCE loss: 0.258, amex train: 0.731, val 0.000\n",
      "78, BCE loss: 0.259, amex train: 0.727, val 0.000\n",
      "78, BCE loss: 0.240, amex train: 0.761, val 0.000\n",
      "79, BCE loss: 0.261, amex train: 0.717, val 0.000\n",
      "79, BCE loss: 0.252, amex train: 0.743, val 0.000\n",
      "79, BCE loss: 0.252, amex train: 0.744, val 0.000\n",
      "79, BCE loss: 0.250, amex train: 0.738, val 0.000\n",
      "79, BCE loss: 0.252, amex train: 0.729, val 0.000\n",
      "79, BCE loss: 0.253, amex train: 0.728, val 0.000\n",
      "79, BCE loss: 0.255, amex train: 0.740, val 0.000\n",
      "79, BCE loss: 0.254, amex train: 0.732, val 0.000\n",
      "79, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "79, BCE loss: 0.254, amex train: 0.736, val 0.000\n",
      "79, BCE loss: 0.255, amex train: 0.724, val 0.000\n",
      "79, BCE loss: 0.256, amex train: 0.731, val 0.000\n",
      "79, BCE loss: 0.250, amex train: 0.732, val 0.000\n",
      "79, BCE loss: 0.260, amex train: 0.724, val 0.000\n",
      "79, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "79, BCE loss: 0.264, amex train: 0.725, val 0.000\n",
      "79, BCE loss: 0.252, amex train: 0.737, val 0.000\n",
      "79, BCE loss: 0.255, amex train: 0.738, val 0.000\n",
      "79, BCE loss: 0.257, amex train: 0.730, val 0.000\n",
      "79, BCE loss: 0.259, amex train: 0.728, val 0.000\n",
      "79, BCE loss: 0.240, amex train: 0.761, val 0.000\n",
      "80, BCE loss: 0.261, amex train: 0.716, val 0.000\n",
      "80, BCE loss: 0.252, amex train: 0.744, val 0.000\n",
      "80, BCE loss: 0.252, amex train: 0.745, val 0.000\n",
      "80, BCE loss: 0.250, amex train: 0.739, val 0.000\n",
      "80, BCE loss: 0.252, amex train: 0.730, val 0.000\n",
      "80, BCE loss: 0.253, amex train: 0.729, val 0.000\n",
      "80, BCE loss: 0.254, amex train: 0.740, val 0.000\n",
      "80, BCE loss: 0.254, amex train: 0.731, val 0.000\n",
      "80, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "80, BCE loss: 0.254, amex train: 0.736, val 0.000\n",
      "80, BCE loss: 0.255, amex train: 0.724, val 0.000\n",
      "80, BCE loss: 0.255, amex train: 0.731, val 0.000\n",
      "80, BCE loss: 0.250, amex train: 0.732, val 0.000\n",
      "80, BCE loss: 0.260, amex train: 0.725, val 0.000\n",
      "80, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "80, BCE loss: 0.264, amex train: 0.725, val 0.000\n",
      "80, BCE loss: 0.252, amex train: 0.737, val 0.000\n",
      "80, BCE loss: 0.255, amex train: 0.739, val 0.000\n",
      "80, BCE loss: 0.257, amex train: 0.730, val 0.000\n",
      "80, BCE loss: 0.259, amex train: 0.728, val 0.000\n",
      "80, BCE loss: 0.239, amex train: 0.761, val 0.000\n",
      "81, BCE loss: 0.261, amex train: 0.717, val 0.000\n",
      "81, BCE loss: 0.252, amex train: 0.744, val 0.000\n",
      "81, BCE loss: 0.252, amex train: 0.744, val 0.000\n",
      "81, BCE loss: 0.251, amex train: 0.738, val 0.000\n",
      "81, BCE loss: 0.252, amex train: 0.730, val 0.000\n",
      "81, BCE loss: 0.253, amex train: 0.730, val 0.000\n",
      "81, BCE loss: 0.254, amex train: 0.740, val 0.000\n",
      "81, BCE loss: 0.254, amex train: 0.731, val 0.000\n",
      "81, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "81, BCE loss: 0.254, amex train: 0.736, val 0.000\n",
      "81, BCE loss: 0.255, amex train: 0.724, val 0.000\n",
      "81, BCE loss: 0.255, amex train: 0.731, val 0.000\n",
      "81, BCE loss: 0.250, amex train: 0.732, val 0.000\n",
      "81, BCE loss: 0.259, amex train: 0.725, val 0.000\n",
      "81, BCE loss: 0.255, amex train: 0.734, val 0.000\n",
      "81, BCE loss: 0.263, amex train: 0.726, val 0.000\n",
      "81, BCE loss: 0.251, amex train: 0.737, val 0.000\n",
      "81, BCE loss: 0.255, amex train: 0.739, val 0.000\n",
      "81, BCE loss: 0.256, amex train: 0.730, val 0.000\n",
      "81, BCE loss: 0.259, amex train: 0.729, val 0.000\n",
      "81, BCE loss: 0.239, amex train: 0.762, val 0.000\n",
      "82, BCE loss: 0.261, amex train: 0.718, val 0.000\n",
      "82, BCE loss: 0.251, amex train: 0.744, val 0.000\n",
      "82, BCE loss: 0.252, amex train: 0.744, val 0.000\n",
      "82, BCE loss: 0.250, amex train: 0.738, val 0.000\n",
      "82, BCE loss: 0.252, amex train: 0.730, val 0.000\n",
      "82, BCE loss: 0.253, amex train: 0.730, val 0.000\n",
      "82, BCE loss: 0.254, amex train: 0.740, val 0.000\n",
      "82, BCE loss: 0.254, amex train: 0.731, val 0.000\n",
      "82, BCE loss: 0.254, amex train: 0.733, val 0.000\n",
      "82, BCE loss: 0.253, amex train: 0.736, val 0.000\n",
      "82, BCE loss: 0.255, amex train: 0.725, val 0.000\n",
      "82, BCE loss: 0.255, amex train: 0.731, val 0.000\n",
      "82, BCE loss: 0.250, amex train: 0.733, val 0.000\n",
      "82, BCE loss: 0.259, amex train: 0.725, val 0.000\n",
      "82, BCE loss: 0.254, amex train: 0.735, val 0.000\n",
      "82, BCE loss: 0.263, amex train: 0.727, val 0.000\n",
      "82, BCE loss: 0.251, amex train: 0.737, val 0.000\n",
      "82, BCE loss: 0.254, amex train: 0.739, val 0.000\n",
      "82, BCE loss: 0.256, amex train: 0.731, val 0.000\n",
      "82, BCE loss: 0.258, amex train: 0.729, val 0.000\n",
      "82, BCE loss: 0.238, amex train: 0.763, val 0.000\n",
      "83, BCE loss: 0.261, amex train: 0.718, val 0.000\n",
      "83, BCE loss: 0.251, amex train: 0.745, val 0.000\n",
      "83, BCE loss: 0.252, amex train: 0.744, val 0.000\n",
      "83, BCE loss: 0.250, amex train: 0.737, val 0.000\n",
      "83, BCE loss: 0.251, amex train: 0.730, val 0.000\n",
      "83, BCE loss: 0.253, amex train: 0.730, val 0.000\n",
      "83, BCE loss: 0.254, amex train: 0.741, val 0.000\n",
      "83, BCE loss: 0.253, amex train: 0.732, val 0.000\n",
      "83, BCE loss: 0.254, amex train: 0.733, val 0.000\n",
      "83, BCE loss: 0.253, amex train: 0.735, val 0.000\n",
      "83, BCE loss: 0.255, amex train: 0.726, val 0.000\n",
      "83, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "83, BCE loss: 0.249, amex train: 0.733, val 0.000\n",
      "83, BCE loss: 0.259, amex train: 0.726, val 0.000\n",
      "83, BCE loss: 0.254, amex train: 0.735, val 0.000\n",
      "83, BCE loss: 0.263, amex train: 0.727, val 0.000\n",
      "83, BCE loss: 0.251, amex train: 0.737, val 0.000\n",
      "83, BCE loss: 0.254, amex train: 0.740, val 0.000\n",
      "83, BCE loss: 0.257, amex train: 0.731, val 0.000\n",
      "83, BCE loss: 0.258, amex train: 0.729, val 0.000\n",
      "83, BCE loss: 0.238, amex train: 0.764, val 0.000\n",
      "84, BCE loss: 0.261, amex train: 0.718, val 0.000\n",
      "84, BCE loss: 0.250, amex train: 0.745, val 0.000\n",
      "84, BCE loss: 0.252, amex train: 0.744, val 0.000\n",
      "84, BCE loss: 0.250, amex train: 0.737, val 0.000\n",
      "84, BCE loss: 0.251, amex train: 0.730, val 0.000\n",
      "84, BCE loss: 0.252, amex train: 0.730, val 0.000\n",
      "84, BCE loss: 0.254, amex train: 0.741, val 0.000\n",
      "84, BCE loss: 0.253, amex train: 0.731, val 0.000\n",
      "84, BCE loss: 0.254, amex train: 0.733, val 0.000\n",
      "84, BCE loss: 0.253, amex train: 0.736, val 0.000\n",
      "84, BCE loss: 0.255, amex train: 0.727, val 0.000\n",
      "84, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "84, BCE loss: 0.249, amex train: 0.733, val 0.000\n",
      "84, BCE loss: 0.259, amex train: 0.726, val 0.000\n",
      "84, BCE loss: 0.254, amex train: 0.736, val 0.000\n",
      "84, BCE loss: 0.263, amex train: 0.728, val 0.000\n",
      "84, BCE loss: 0.251, amex train: 0.738, val 0.000\n",
      "84, BCE loss: 0.254, amex train: 0.740, val 0.000\n",
      "84, BCE loss: 0.257, amex train: 0.732, val 0.000\n",
      "84, BCE loss: 0.257, amex train: 0.729, val 0.000\n",
      "84, BCE loss: 0.238, amex train: 0.764, val 0.000\n",
      "85, BCE loss: 0.260, amex train: 0.719, val 0.000\n",
      "85, BCE loss: 0.250, amex train: 0.744, val 0.000\n",
      "85, BCE loss: 0.251, amex train: 0.745, val 0.000\n",
      "85, BCE loss: 0.249, amex train: 0.738, val 0.000\n",
      "85, BCE loss: 0.251, amex train: 0.730, val 0.000\n",
      "85, BCE loss: 0.252, amex train: 0.730, val 0.000\n",
      "85, BCE loss: 0.254, amex train: 0.740, val 0.000\n",
      "85, BCE loss: 0.253, amex train: 0.732, val 0.000\n",
      "85, BCE loss: 0.254, amex train: 0.734, val 0.000\n",
      "85, BCE loss: 0.253, amex train: 0.737, val 0.000\n",
      "85, BCE loss: 0.254, amex train: 0.726, val 0.000\n",
      "85, BCE loss: 0.255, amex train: 0.732, val 0.000\n",
      "85, BCE loss: 0.249, amex train: 0.733, val 0.000\n",
      "85, BCE loss: 0.259, amex train: 0.726, val 0.000\n",
      "85, BCE loss: 0.254, amex train: 0.736, val 0.000\n",
      "85, BCE loss: 0.263, amex train: 0.729, val 0.000\n",
      "85, BCE loss: 0.251, amex train: 0.738, val 0.000\n",
      "85, BCE loss: 0.253, amex train: 0.740, val 0.000\n",
      "85, BCE loss: 0.257, amex train: 0.732, val 0.000\n",
      "85, BCE loss: 0.257, amex train: 0.730, val 0.000\n",
      "85, BCE loss: 0.238, amex train: 0.765, val 0.000\n",
      "86, BCE loss: 0.260, amex train: 0.719, val 0.000\n",
      "86, BCE loss: 0.250, amex train: 0.746, val 0.000\n",
      "86, BCE loss: 0.251, amex train: 0.745, val 0.000\n",
      "86, BCE loss: 0.249, amex train: 0.739, val 0.000\n",
      "86, BCE loss: 0.251, amex train: 0.729, val 0.000\n",
      "86, BCE loss: 0.252, amex train: 0.730, val 0.000\n",
      "86, BCE loss: 0.253, amex train: 0.740, val 0.000\n",
      "86, BCE loss: 0.253, amex train: 0.732, val 0.000\n",
      "86, BCE loss: 0.254, amex train: 0.734, val 0.000\n",
      "86, BCE loss: 0.253, amex train: 0.737, val 0.000\n",
      "86, BCE loss: 0.254, amex train: 0.727, val 0.000\n",
      "86, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "86, BCE loss: 0.249, amex train: 0.732, val 0.000\n",
      "86, BCE loss: 0.259, amex train: 0.727, val 0.000\n",
      "86, BCE loss: 0.254, amex train: 0.735, val 0.000\n",
      "86, BCE loss: 0.263, amex train: 0.728, val 0.000\n",
      "86, BCE loss: 0.251, amex train: 0.739, val 0.000\n",
      "86, BCE loss: 0.253, amex train: 0.741, val 0.000\n",
      "86, BCE loss: 0.257, amex train: 0.732, val 0.000\n",
      "86, BCE loss: 0.257, amex train: 0.730, val 0.000\n",
      "86, BCE loss: 0.237, amex train: 0.765, val 0.000\n",
      "87, BCE loss: 0.260, amex train: 0.719, val 0.000\n",
      "87, BCE loss: 0.250, amex train: 0.745, val 0.000\n",
      "87, BCE loss: 0.251, amex train: 0.745, val 0.000\n",
      "87, BCE loss: 0.249, amex train: 0.739, val 0.000\n",
      "87, BCE loss: 0.251, amex train: 0.728, val 0.000\n",
      "87, BCE loss: 0.252, amex train: 0.730, val 0.000\n",
      "87, BCE loss: 0.253, amex train: 0.740, val 0.000\n",
      "87, BCE loss: 0.253, amex train: 0.731, val 0.000\n",
      "87, BCE loss: 0.254, amex train: 0.734, val 0.000\n",
      "87, BCE loss: 0.253, amex train: 0.737, val 0.000\n",
      "87, BCE loss: 0.254, amex train: 0.727, val 0.000\n",
      "87, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "87, BCE loss: 0.249, amex train: 0.732, val 0.000\n",
      "87, BCE loss: 0.258, amex train: 0.727, val 0.000\n",
      "87, BCE loss: 0.254, amex train: 0.737, val 0.000\n",
      "87, BCE loss: 0.263, amex train: 0.729, val 0.000\n",
      "87, BCE loss: 0.251, amex train: 0.739, val 0.000\n",
      "87, BCE loss: 0.253, amex train: 0.741, val 0.000\n",
      "87, BCE loss: 0.257, amex train: 0.732, val 0.000\n",
      "87, BCE loss: 0.257, amex train: 0.730, val 0.000\n",
      "87, BCE loss: 0.237, amex train: 0.765, val 0.000\n",
      "88, BCE loss: 0.260, amex train: 0.719, val 0.000\n",
      "88, BCE loss: 0.250, amex train: 0.745, val 0.000\n",
      "88, BCE loss: 0.251, amex train: 0.746, val 0.000\n",
      "88, BCE loss: 0.249, amex train: 0.740, val 0.000\n",
      "88, BCE loss: 0.251, amex train: 0.728, val 0.000\n",
      "88, BCE loss: 0.252, amex train: 0.730, val 0.000\n",
      "88, BCE loss: 0.253, amex train: 0.740, val 0.000\n",
      "88, BCE loss: 0.253, amex train: 0.730, val 0.000\n",
      "88, BCE loss: 0.254, amex train: 0.734, val 0.000\n",
      "88, BCE loss: 0.253, amex train: 0.736, val 0.000\n",
      "88, BCE loss: 0.254, amex train: 0.727, val 0.000\n",
      "88, BCE loss: 0.254, amex train: 0.734, val 0.000\n",
      "88, BCE loss: 0.249, amex train: 0.732, val 0.000\n",
      "88, BCE loss: 0.258, amex train: 0.726, val 0.000\n",
      "88, BCE loss: 0.253, amex train: 0.736, val 0.000\n",
      "88, BCE loss: 0.263, amex train: 0.729, val 0.000\n",
      "88, BCE loss: 0.251, amex train: 0.739, val 0.000\n",
      "88, BCE loss: 0.253, amex train: 0.741, val 0.000\n",
      "88, BCE loss: 0.257, amex train: 0.732, val 0.000\n",
      "88, BCE loss: 0.258, amex train: 0.730, val 0.000\n",
      "88, BCE loss: 0.237, amex train: 0.765, val 0.000\n",
      "89, BCE loss: 0.260, amex train: 0.719, val 0.000\n",
      "89, BCE loss: 0.250, amex train: 0.746, val 0.000\n",
      "89, BCE loss: 0.251, amex train: 0.746, val 0.000\n",
      "89, BCE loss: 0.249, amex train: 0.739, val 0.000\n",
      "89, BCE loss: 0.251, amex train: 0.728, val 0.000\n",
      "89, BCE loss: 0.252, amex train: 0.730, val 0.000\n",
      "89, BCE loss: 0.253, amex train: 0.740, val 0.000\n",
      "89, BCE loss: 0.253, amex train: 0.731, val 0.000\n",
      "89, BCE loss: 0.254, amex train: 0.734, val 0.000\n",
      "89, BCE loss: 0.253, amex train: 0.738, val 0.000\n",
      "89, BCE loss: 0.254, amex train: 0.727, val 0.000\n",
      "89, BCE loss: 0.254, amex train: 0.733, val 0.000\n",
      "89, BCE loss: 0.249, amex train: 0.732, val 0.000\n",
      "89, BCE loss: 0.258, amex train: 0.727, val 0.000\n",
      "89, BCE loss: 0.253, amex train: 0.736, val 0.000\n",
      "89, BCE loss: 0.262, amex train: 0.729, val 0.000\n",
      "89, BCE loss: 0.250, amex train: 0.739, val 0.000\n",
      "89, BCE loss: 0.253, amex train: 0.741, val 0.000\n",
      "89, BCE loss: 0.256, amex train: 0.732, val 0.000\n",
      "89, BCE loss: 0.258, amex train: 0.729, val 0.000\n",
      "89, BCE loss: 0.236, amex train: 0.765, val 0.000\n",
      "90, BCE loss: 0.260, amex train: 0.719, val 0.000\n",
      "90, BCE loss: 0.250, amex train: 0.745, val 0.000\n",
      "90, BCE loss: 0.251, amex train: 0.745, val 0.000\n",
      "90, BCE loss: 0.250, amex train: 0.738, val 0.000\n",
      "90, BCE loss: 0.251, amex train: 0.729, val 0.000\n",
      "90, BCE loss: 0.253, amex train: 0.731, val 0.000\n",
      "90, BCE loss: 0.253, amex train: 0.741, val 0.000\n",
      "90, BCE loss: 0.253, amex train: 0.731, val 0.000\n",
      "90, BCE loss: 0.254, amex train: 0.735, val 0.000\n",
      "90, BCE loss: 0.252, amex train: 0.739, val 0.000\n",
      "90, BCE loss: 0.254, amex train: 0.727, val 0.000\n",
      "90, BCE loss: 0.254, amex train: 0.732, val 0.000\n",
      "90, BCE loss: 0.249, amex train: 0.732, val 0.000\n",
      "90, BCE loss: 0.258, amex train: 0.725, val 0.000\n",
      "90, BCE loss: 0.253, amex train: 0.736, val 0.000\n",
      "90, BCE loss: 0.262, amex train: 0.731, val 0.000\n",
      "90, BCE loss: 0.251, amex train: 0.739, val 0.000\n",
      "90, BCE loss: 0.253, amex train: 0.741, val 0.000\n",
      "90, BCE loss: 0.256, amex train: 0.734, val 0.000\n",
      "90, BCE loss: 0.258, amex train: 0.730, val 0.000\n",
      "90, BCE loss: 0.236, amex train: 0.765, val 0.000\n",
      "91, BCE loss: 0.260, amex train: 0.718, val 0.000\n",
      "91, BCE loss: 0.250, amex train: 0.745, val 0.000\n",
      "91, BCE loss: 0.251, amex train: 0.745, val 0.000\n",
      "91, BCE loss: 0.250, amex train: 0.737, val 0.000\n",
      "91, BCE loss: 0.250, amex train: 0.731, val 0.000\n",
      "91, BCE loss: 0.253, amex train: 0.730, val 0.000\n",
      "91, BCE loss: 0.253, amex train: 0.741, val 0.000\n",
      "91, BCE loss: 0.252, amex train: 0.732, val 0.000\n",
      "91, BCE loss: 0.254, amex train: 0.733, val 0.000\n",
      "91, BCE loss: 0.252, amex train: 0.740, val 0.000\n",
      "91, BCE loss: 0.254, amex train: 0.728, val 0.000\n",
      "91, BCE loss: 0.254, amex train: 0.732, val 0.000\n",
      "91, BCE loss: 0.249, amex train: 0.731, val 0.000\n",
      "91, BCE loss: 0.258, amex train: 0.726, val 0.000\n",
      "91, BCE loss: 0.254, amex train: 0.737, val 0.000\n",
      "91, BCE loss: 0.262, amex train: 0.732, val 0.000\n",
      "91, BCE loss: 0.251, amex train: 0.739, val 0.000\n",
      "91, BCE loss: 0.253, amex train: 0.741, val 0.000\n",
      "91, BCE loss: 0.255, amex train: 0.734, val 0.000\n",
      "91, BCE loss: 0.259, amex train: 0.729, val 0.000\n",
      "91, BCE loss: 0.235, amex train: 0.765, val 0.000\n",
      "92, BCE loss: 0.260, amex train: 0.720, val 0.000\n",
      "92, BCE loss: 0.250, amex train: 0.745, val 0.000\n",
      "92, BCE loss: 0.251, amex train: 0.747, val 0.000\n",
      "92, BCE loss: 0.251, amex train: 0.735, val 0.000\n",
      "92, BCE loss: 0.250, amex train: 0.730, val 0.000\n",
      "92, BCE loss: 0.253, amex train: 0.731, val 0.000\n",
      "92, BCE loss: 0.253, amex train: 0.740, val 0.000\n",
      "92, BCE loss: 0.252, amex train: 0.735, val 0.000\n",
      "92, BCE loss: 0.254, amex train: 0.733, val 0.000\n",
      "92, BCE loss: 0.252, amex train: 0.742, val 0.000\n",
      "92, BCE loss: 0.254, amex train: 0.729, val 0.000\n",
      "92, BCE loss: 0.254, amex train: 0.731, val 0.000\n",
      "92, BCE loss: 0.249, amex train: 0.732, val 0.000\n",
      "92, BCE loss: 0.258, amex train: 0.727, val 0.000\n",
      "92, BCE loss: 0.254, amex train: 0.736, val 0.000\n",
      "92, BCE loss: 0.261, amex train: 0.731, val 0.000\n",
      "92, BCE loss: 0.251, amex train: 0.740, val 0.000\n",
      "92, BCE loss: 0.254, amex train: 0.740, val 0.000\n",
      "92, BCE loss: 0.254, amex train: 0.733, val 0.000\n",
      "92, BCE loss: 0.260, amex train: 0.729, val 0.000\n",
      "92, BCE loss: 0.235, amex train: 0.763, val 0.000\n",
      "93, BCE loss: 0.260, amex train: 0.722, val 0.000\n",
      "93, BCE loss: 0.252, amex train: 0.742, val 0.000\n",
      "93, BCE loss: 0.250, amex train: 0.747, val 0.000\n",
      "93, BCE loss: 0.251, amex train: 0.735, val 0.000\n",
      "93, BCE loss: 0.251, amex train: 0.731, val 0.000\n",
      "93, BCE loss: 0.252, amex train: 0.735, val 0.000\n",
      "93, BCE loss: 0.254, amex train: 0.739, val 0.000\n",
      "93, BCE loss: 0.253, amex train: 0.735, val 0.000\n",
      "93, BCE loss: 0.253, amex train: 0.734, val 0.000\n",
      "93, BCE loss: 0.252, amex train: 0.740, val 0.000\n",
      "93, BCE loss: 0.254, amex train: 0.726, val 0.000\n",
      "93, BCE loss: 0.253, amex train: 0.733, val 0.000\n",
      "93, BCE loss: 0.250, amex train: 0.729, val 0.000\n",
      "93, BCE loss: 0.258, amex train: 0.730, val 0.000\n",
      "93, BCE loss: 0.254, amex train: 0.736, val 0.000\n",
      "93, BCE loss: 0.261, amex train: 0.729, val 0.000\n",
      "93, BCE loss: 0.250, amex train: 0.739, val 0.000\n",
      "93, BCE loss: 0.254, amex train: 0.738, val 0.000\n",
      "93, BCE loss: 0.254, amex train: 0.733, val 0.000\n",
      "93, BCE loss: 0.259, amex train: 0.731, val 0.000\n",
      "93, BCE loss: 0.235, amex train: 0.759, val 0.000\n",
      "94, BCE loss: 0.260, amex train: 0.721, val 0.000\n",
      "94, BCE loss: 0.251, amex train: 0.743, val 0.000\n",
      "94, BCE loss: 0.250, amex train: 0.748, val 0.000\n",
      "94, BCE loss: 0.250, amex train: 0.737, val 0.000\n",
      "94, BCE loss: 0.251, amex train: 0.732, val 0.000\n",
      "94, BCE loss: 0.252, amex train: 0.735, val 0.000\n",
      "94, BCE loss: 0.253, amex train: 0.740, val 0.000\n",
      "94, BCE loss: 0.254, amex train: 0.736, val 0.000\n",
      "94, BCE loss: 0.252, amex train: 0.735, val 0.000\n",
      "94, BCE loss: 0.252, amex train: 0.740, val 0.000\n",
      "94, BCE loss: 0.256, amex train: 0.726, val 0.000\n",
      "94, BCE loss: 0.253, amex train: 0.736, val 0.000\n",
      "94, BCE loss: 0.250, amex train: 0.730, val 0.000\n",
      "94, BCE loss: 0.258, amex train: 0.730, val 0.000\n",
      "94, BCE loss: 0.253, amex train: 0.737, val 0.000\n",
      "94, BCE loss: 0.262, amex train: 0.730, val 0.000\n",
      "94, BCE loss: 0.250, amex train: 0.739, val 0.000\n",
      "94, BCE loss: 0.253, amex train: 0.738, val 0.000\n",
      "94, BCE loss: 0.255, amex train: 0.733, val 0.000\n",
      "94, BCE loss: 0.257, amex train: 0.733, val 0.000\n",
      "94, BCE loss: 0.234, amex train: 0.762, val 0.000\n",
      "95, BCE loss: 0.260, amex train: 0.720, val 0.000\n",
      "95, BCE loss: 0.249, amex train: 0.746, val 0.000\n",
      "95, BCE loss: 0.250, amex train: 0.748, val 0.000\n",
      "95, BCE loss: 0.249, amex train: 0.738, val 0.000\n",
      "95, BCE loss: 0.250, amex train: 0.732, val 0.000\n",
      "95, BCE loss: 0.251, amex train: 0.736, val 0.000\n",
      "95, BCE loss: 0.252, amex train: 0.743, val 0.000\n",
      "95, BCE loss: 0.253, amex train: 0.735, val 0.000\n",
      "95, BCE loss: 0.252, amex train: 0.735, val 0.000\n",
      "95, BCE loss: 0.251, amex train: 0.741, val 0.000\n",
      "95, BCE loss: 0.255, amex train: 0.727, val 0.000\n",
      "95, BCE loss: 0.252, amex train: 0.736, val 0.000\n",
      "95, BCE loss: 0.249, amex train: 0.734, val 0.000\n",
      "95, BCE loss: 0.257, amex train: 0.731, val 0.000\n",
      "95, BCE loss: 0.252, amex train: 0.738, val 0.000\n",
      "95, BCE loss: 0.261, amex train: 0.732, val 0.000\n",
      "95, BCE loss: 0.249, amex train: 0.739, val 0.000\n",
      "95, BCE loss: 0.253, amex train: 0.739, val 0.000\n",
      "95, BCE loss: 0.254, amex train: 0.735, val 0.000\n",
      "95, BCE loss: 0.256, amex train: 0.735, val 0.000\n",
      "95, BCE loss: 0.234, amex train: 0.766, val 0.000\n",
      "96, BCE loss: 0.259, amex train: 0.722, val 0.000\n",
      "96, BCE loss: 0.249, amex train: 0.744, val 0.000\n",
      "96, BCE loss: 0.249, amex train: 0.749, val 0.000\n",
      "96, BCE loss: 0.249, amex train: 0.739, val 0.000\n",
      "96, BCE loss: 0.249, amex train: 0.732, val 0.000\n",
      "96, BCE loss: 0.250, amex train: 0.736, val 0.000\n",
      "96, BCE loss: 0.252, amex train: 0.744, val 0.000\n",
      "96, BCE loss: 0.252, amex train: 0.736, val 0.000\n",
      "96, BCE loss: 0.252, amex train: 0.735, val 0.000\n",
      "96, BCE loss: 0.251, amex train: 0.741, val 0.000\n",
      "96, BCE loss: 0.254, amex train: 0.728, val 0.000\n",
      "96, BCE loss: 0.252, amex train: 0.736, val 0.000\n",
      "96, BCE loss: 0.249, amex train: 0.733, val 0.000\n",
      "96, BCE loss: 0.257, amex train: 0.732, val 0.000\n",
      "96, BCE loss: 0.252, amex train: 0.738, val 0.000\n",
      "96, BCE loss: 0.261, amex train: 0.733, val 0.000\n",
      "96, BCE loss: 0.249, amex train: 0.740, val 0.000\n",
      "96, BCE loss: 0.253, amex train: 0.741, val 0.000\n",
      "96, BCE loss: 0.255, amex train: 0.737, val 0.000\n",
      "96, BCE loss: 0.256, amex train: 0.733, val 0.000\n",
      "96, BCE loss: 0.234, amex train: 0.765, val 0.000\n",
      "97, BCE loss: 0.259, amex train: 0.722, val 0.000\n",
      "97, BCE loss: 0.250, amex train: 0.744, val 0.000\n",
      "97, BCE loss: 0.249, amex train: 0.751, val 0.000\n",
      "97, BCE loss: 0.249, amex train: 0.740, val 0.000\n",
      "97, BCE loss: 0.249, amex train: 0.734, val 0.000\n",
      "97, BCE loss: 0.250, amex train: 0.735, val 0.000\n",
      "97, BCE loss: 0.252, amex train: 0.742, val 0.000\n",
      "97, BCE loss: 0.251, amex train: 0.737, val 0.000\n",
      "97, BCE loss: 0.252, amex train: 0.736, val 0.000\n",
      "97, BCE loss: 0.251, amex train: 0.742, val 0.000\n",
      "97, BCE loss: 0.253, amex train: 0.728, val 0.000\n",
      "97, BCE loss: 0.252, amex train: 0.735, val 0.000\n",
      "97, BCE loss: 0.248, amex train: 0.734, val 0.000\n",
      "97, BCE loss: 0.257, amex train: 0.733, val 0.000\n",
      "97, BCE loss: 0.252, amex train: 0.740, val 0.000\n",
      "97, BCE loss: 0.261, amex train: 0.734, val 0.000\n",
      "97, BCE loss: 0.250, amex train: 0.741, val 0.000\n",
      "97, BCE loss: 0.253, amex train: 0.742, val 0.000\n",
      "97, BCE loss: 0.256, amex train: 0.737, val 0.000\n",
      "97, BCE loss: 0.256, amex train: 0.733, val 0.000\n",
      "97, BCE loss: 0.234, amex train: 0.768, val 0.000\n",
      "98, BCE loss: 0.258, amex train: 0.722, val 0.000\n",
      "98, BCE loss: 0.250, amex train: 0.745, val 0.000\n",
      "98, BCE loss: 0.249, amex train: 0.750, val 0.000\n",
      "98, BCE loss: 0.249, amex train: 0.741, val 0.000\n",
      "98, BCE loss: 0.249, amex train: 0.734, val 0.000\n",
      "98, BCE loss: 0.250, amex train: 0.735, val 0.000\n",
      "98, BCE loss: 0.252, amex train: 0.742, val 0.000\n",
      "98, BCE loss: 0.251, amex train: 0.738, val 0.000\n",
      "98, BCE loss: 0.252, amex train: 0.736, val 0.000\n",
      "98, BCE loss: 0.251, amex train: 0.742, val 0.000\n",
      "98, BCE loss: 0.253, amex train: 0.728, val 0.000\n",
      "98, BCE loss: 0.253, amex train: 0.736, val 0.000\n",
      "98, BCE loss: 0.248, amex train: 0.733, val 0.000\n",
      "98, BCE loss: 0.257, amex train: 0.733, val 0.000\n",
      "98, BCE loss: 0.252, amex train: 0.740, val 0.000\n",
      "98, BCE loss: 0.260, amex train: 0.733, val 0.000\n",
      "98, BCE loss: 0.250, amex train: 0.742, val 0.000\n",
      "98, BCE loss: 0.252, amex train: 0.743, val 0.000\n",
      "98, BCE loss: 0.256, amex train: 0.737, val 0.000\n",
      "98, BCE loss: 0.256, amex train: 0.733, val 0.000\n",
      "98, BCE loss: 0.234, amex train: 0.768, val 0.000\n",
      "99, BCE loss: 0.258, amex train: 0.721, val 0.000\n",
      "99, BCE loss: 0.250, amex train: 0.745, val 0.000\n",
      "99, BCE loss: 0.249, amex train: 0.751, val 0.000\n",
      "99, BCE loss: 0.249, amex train: 0.741, val 0.000\n",
      "99, BCE loss: 0.249, amex train: 0.734, val 0.000\n",
      "99, BCE loss: 0.250, amex train: 0.736, val 0.000\n",
      "99, BCE loss: 0.252, amex train: 0.742, val 0.000\n",
      "99, BCE loss: 0.251, amex train: 0.737, val 0.000\n",
      "99, BCE loss: 0.252, amex train: 0.736, val 0.000\n",
      "99, BCE loss: 0.251, amex train: 0.743, val 0.000\n",
      "99, BCE loss: 0.253, amex train: 0.729, val 0.000\n",
      "99, BCE loss: 0.252, amex train: 0.735, val 0.000\n",
      "99, BCE loss: 0.248, amex train: 0.733, val 0.000\n",
      "99, BCE loss: 0.256, amex train: 0.734, val 0.000\n",
      "99, BCE loss: 0.252, amex train: 0.740, val 0.000\n",
      "99, BCE loss: 0.260, amex train: 0.734, val 0.000\n",
      "99, BCE loss: 0.250, amex train: 0.742, val 0.000\n",
      "99, BCE loss: 0.253, amex train: 0.743, val 0.000\n",
      "99, BCE loss: 0.255, amex train: 0.736, val 0.000\n",
      "99, BCE loss: 0.256, amex train: 0.733, val 0.000\n",
      "99, BCE loss: 0.233, amex train: 0.769, val 0.000\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mlp_col27_sum\"\n",
    "model = MLP(input_dim=91)\n",
    "model = train_torch_model(model, train_loader, num_epochs=100, validation_data=validation_data, \n",
    "                            output_model_name=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model with only c27 (extract a feature for them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Model with pre-trained conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, conv_feat =  conv(torch.as_tensor(np.load(OUTDIR+\"train_data_all.npy\"), dtype=torch.float32), return_featues=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.99"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.40+11.70+13.10+1.69+6.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(OUTDIR+\"agg_feat.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load(OUTDIR+\"train_labels_all.npy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, train_labels, test_size=1/9, random_state=0, shuffle=True)\n",
    "validation_data = (X_test, y_test)\n",
    "\n",
    "train_dataset = CustomerData(X_train, train_labels=y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pd.nn.train_utils import train_torch_model\n",
    "from bes.nn.es_module import ESModule\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(ESModule):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=128,):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.nf1 = nn.LayerNorm([hidden_dim])\n",
    "        \n",
    "        self.fcout = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "    \n",
    "    def forward(self, h, return_featues=False):\n",
    "        h = F.selu(self.fc1(h))\n",
    "        h = self.nf1(h)\n",
    "        if return_featues:\n",
    "            return torch.sigmoid(self.fcout(h)), h\n",
    "        \n",
    "        return torch.sigmoid(self.fcout(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(ESModule):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=128,):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.nf1 = nn.LayerNorm([hidden_dim])\n",
    "        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.nf2 = nn.LayerNorm([hidden_dim])\n",
    "        self.fc3 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.nf3 = nn.LayerNorm([hidden_dim])\n",
    "        self.fc4 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.nf4 = nn.LayerNorm([hidden_dim])\n",
    "        self.fc5 = nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.nf5 = nn.LayerNorm([hidden_dim])\n",
    "        \n",
    "        self.fcout = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "    \n",
    "    def forward(self, h, return_featues=False):\n",
    "        h = F.selu(self.fc1(h))\n",
    "        r = self.nf1(h)\n",
    "        h = F.selu(self.fc2(r))\n",
    "        h = self.nf2(h)\n",
    "        h = F.selu(self.fc3(h))\n",
    "        r = self.nf3(h+r)\n",
    "        h = F.selu(self.fc4(r))\n",
    "        h = self.nf4(h)\n",
    "        h = F.selu(self.fc5(h))\n",
    "        h = self.nf5(h+r)\n",
    "        if return_featues:\n",
    "            return torch.sigmoid(self.fcout(h)), h\n",
    "        \n",
    "        return torch.sigmoid(self.fcout(h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, BCE loss: 0.887, amex train: -0.070, val 0.000\n",
      "0, BCE loss: 0.457, amex train: 0.719, val 0.000\n",
      "0, BCE loss: 0.343, amex train: 0.772, val 0.000\n",
      "0, BCE loss: 0.239, amex train: 0.791, val 0.772\n",
      "0, BCE loss: 0.262, amex train: 0.791, val 0.772\n",
      "0, BCE loss: 0.294, amex train: 0.772, val 0.000\n",
      "0, BCE loss: 0.259, amex train: 0.785, val 0.775\n",
      "0, BCE loss: 0.233, amex train: 0.783, val 0.774\n",
      "0, BCE loss: 0.225, amex train: 0.789, val 0.774\n",
      "0, BCE loss: 0.251, amex train: 0.769, val 0.000\n",
      "0, BCE loss: 0.267, amex train: 0.774, val 0.000\n",
      "0, BCE loss: 0.252, amex train: 0.781, val 0.776\n",
      "0, BCE loss: 0.227, amex train: 0.793, val 0.776\n",
      "0, BCE loss: 0.228, amex train: 0.781, val 0.777\n",
      "0, BCE loss: 0.221, amex train: 0.801, val 0.777\n",
      "0, BCE loss: 0.236, amex train: 0.784, val 0.777\n",
      "0, BCE loss: 0.237, amex train: 0.791, val 0.777\n",
      "0, BCE loss: 0.235, amex train: 0.778, val 0.000\n",
      "0, BCE loss: 0.225, amex train: 0.792, val 0.777\n",
      "0, BCE loss: 0.223, amex train: 0.786, val 0.777\n",
      "0, BCE loss: 0.224, amex train: 0.789, val 0.778\n",
      "1, BCE loss: 0.224, amex train: 0.797, val 0.777\n",
      "1, BCE loss: 0.229, amex train: 0.788, val 0.777\n",
      "1, BCE loss: 0.229, amex train: 0.787, val 0.777\n",
      "1, BCE loss: 0.217, amex train: 0.798, val 0.777\n",
      "1, BCE loss: 0.221, amex train: 0.795, val 0.777\n",
      "1, BCE loss: 0.227, amex train: 0.780, val 0.777\n",
      "1, BCE loss: 0.218, amex train: 0.789, val 0.777\n",
      "1, BCE loss: 0.225, amex train: 0.789, val 0.777\n",
      "1, BCE loss: 0.224, amex train: 0.792, val 0.777\n",
      "1, BCE loss: 0.234, amex train: 0.774, val 0.000\n",
      "1, BCE loss: 0.231, amex train: 0.777, val 0.000\n",
      "1, BCE loss: 0.224, amex train: 0.785, val 0.778\n",
      "1, BCE loss: 0.217, amex train: 0.797, val 0.779\n",
      "1, BCE loss: 0.226, amex train: 0.784, val 0.779\n",
      "1, BCE loss: 0.214, amex train: 0.801, val 0.778\n",
      "1, BCE loss: 0.222, amex train: 0.785, val 0.778\n",
      "1, BCE loss: 0.219, amex train: 0.792, val 0.777\n",
      "1, BCE loss: 0.220, amex train: 0.783, val 0.778\n",
      "1, BCE loss: 0.217, amex train: 0.792, val 0.778\n",
      "1, BCE loss: 0.221, amex train: 0.785, val 0.779\n",
      "1, BCE loss: 0.221, amex train: 0.787, val 0.779\n",
      "2, BCE loss: 0.218, amex train: 0.800, val 0.779\n",
      "2, BCE loss: 0.219, amex train: 0.792, val 0.778\n",
      "2, BCE loss: 0.221, amex train: 0.790, val 0.779\n",
      "2, BCE loss: 0.212, amex train: 0.800, val 0.779\n",
      "2, BCE loss: 0.218, amex train: 0.796, val 0.779\n",
      "2, BCE loss: 0.225, amex train: 0.781, val 0.778\n",
      "2, BCE loss: 0.215, amex train: 0.789, val 0.778\n",
      "2, BCE loss: 0.221, amex train: 0.791, val 0.778\n",
      "2, BCE loss: 0.219, amex train: 0.791, val 0.778\n",
      "2, BCE loss: 0.230, amex train: 0.775, val 0.000\n",
      "2, BCE loss: 0.230, amex train: 0.776, val 0.000\n",
      "2, BCE loss: 0.223, amex train: 0.786, val 0.778\n",
      "2, BCE loss: 0.216, amex train: 0.796, val 0.778\n",
      "2, BCE loss: 0.223, amex train: 0.785, val 0.778\n",
      "2, BCE loss: 0.212, amex train: 0.801, val 0.778\n",
      "2, BCE loss: 0.220, amex train: 0.786, val 0.778\n",
      "2, BCE loss: 0.218, amex train: 0.793, val 0.778\n",
      "2, BCE loss: 0.220, amex train: 0.784, val 0.779\n",
      "2, BCE loss: 0.216, amex train: 0.793, val 0.779\n",
      "2, BCE loss: 0.220, amex train: 0.786, val 0.778\n",
      "2, BCE loss: 0.220, amex train: 0.787, val 0.778\n",
      "3, BCE loss: 0.217, amex train: 0.800, val 0.778\n",
      "3, BCE loss: 0.219, amex train: 0.791, val 0.779\n",
      "3, BCE loss: 0.220, amex train: 0.790, val 0.778\n",
      "3, BCE loss: 0.212, amex train: 0.798, val 0.779\n",
      "3, BCE loss: 0.218, amex train: 0.796, val 0.778\n",
      "3, BCE loss: 0.226, amex train: 0.782, val 0.778\n",
      "3, BCE loss: 0.215, amex train: 0.790, val 0.778\n",
      "3, BCE loss: 0.220, amex train: 0.792, val 0.778\n",
      "3, BCE loss: 0.218, amex train: 0.791, val 0.778\n",
      "3, BCE loss: 0.230, amex train: 0.774, val 0.000\n",
      "3, BCE loss: 0.229, amex train: 0.778, val 0.000\n",
      "3, BCE loss: 0.224, amex train: 0.786, val 0.778\n",
      "3, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "3, BCE loss: 0.223, amex train: 0.785, val 0.778\n",
      "3, BCE loss: 0.212, amex train: 0.801, val 0.779\n",
      "3, BCE loss: 0.220, amex train: 0.786, val 0.779\n",
      "3, BCE loss: 0.217, amex train: 0.794, val 0.779\n",
      "3, BCE loss: 0.220, amex train: 0.784, val 0.778\n",
      "3, BCE loss: 0.216, amex train: 0.795, val 0.778\n",
      "3, BCE loss: 0.220, amex train: 0.786, val 0.778\n",
      "3, BCE loss: 0.220, amex train: 0.785, val 0.778\n",
      "4, BCE loss: 0.217, amex train: 0.800, val 0.778\n",
      "4, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "4, BCE loss: 0.220, amex train: 0.792, val 0.779\n",
      "4, BCE loss: 0.211, amex train: 0.800, val 0.779\n",
      "4, BCE loss: 0.218, amex train: 0.797, val 0.779\n",
      "4, BCE loss: 0.225, amex train: 0.781, val 0.779\n",
      "4, BCE loss: 0.215, amex train: 0.790, val 0.779\n",
      "4, BCE loss: 0.220, amex train: 0.793, val 0.779\n",
      "4, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "4, BCE loss: 0.229, amex train: 0.775, val 0.000\n",
      "4, BCE loss: 0.229, amex train: 0.779, val 0.000\n",
      "4, BCE loss: 0.223, amex train: 0.785, val 0.779\n",
      "4, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "4, BCE loss: 0.222, amex train: 0.784, val 0.779\n",
      "4, BCE loss: 0.212, amex train: 0.803, val 0.779\n",
      "4, BCE loss: 0.219, amex train: 0.786, val 0.779\n",
      "4, BCE loss: 0.217, amex train: 0.795, val 0.778\n",
      "4, BCE loss: 0.220, amex train: 0.784, val 0.779\n",
      "4, BCE loss: 0.216, amex train: 0.795, val 0.779\n",
      "4, BCE loss: 0.220, amex train: 0.786, val 0.779\n",
      "4, BCE loss: 0.220, amex train: 0.786, val 0.779\n",
      "5, BCE loss: 0.217, amex train: 0.800, val 0.779\n",
      "5, BCE loss: 0.218, amex train: 0.793, val 0.779\n",
      "5, BCE loss: 0.220, amex train: 0.794, val 0.779\n",
      "5, BCE loss: 0.212, amex train: 0.800, val 0.779\n",
      "5, BCE loss: 0.218, amex train: 0.799, val 0.779\n",
      "5, BCE loss: 0.225, amex train: 0.782, val 0.779\n",
      "5, BCE loss: 0.214, amex train: 0.790, val 0.779\n",
      "5, BCE loss: 0.219, amex train: 0.793, val 0.779\n",
      "5, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "5, BCE loss: 0.229, amex train: 0.775, val 0.000\n",
      "5, BCE loss: 0.229, amex train: 0.779, val 0.000\n",
      "5, BCE loss: 0.223, amex train: 0.786, val 0.779\n",
      "5, BCE loss: 0.216, amex train: 0.799, val 0.779\n",
      "5, BCE loss: 0.222, amex train: 0.783, val 0.780\n",
      "5, BCE loss: 0.212, amex train: 0.803, val 0.779\n",
      "5, BCE loss: 0.219, amex train: 0.786, val 0.779\n",
      "5, BCE loss: 0.216, amex train: 0.795, val 0.779\n",
      "5, BCE loss: 0.219, amex train: 0.784, val 0.779\n",
      "5, BCE loss: 0.215, amex train: 0.794, val 0.778\n",
      "5, BCE loss: 0.220, amex train: 0.786, val 0.779\n",
      "5, BCE loss: 0.220, amex train: 0.786, val 0.779\n",
      "6, BCE loss: 0.216, amex train: 0.800, val 0.779\n",
      "6, BCE loss: 0.218, amex train: 0.794, val 0.780\n",
      "6, BCE loss: 0.220, amex train: 0.794, val 0.780\n",
      "6, BCE loss: 0.212, amex train: 0.800, val 0.779\n",
      "6, BCE loss: 0.218, amex train: 0.798, val 0.780\n",
      "6, BCE loss: 0.224, amex train: 0.781, val 0.779\n",
      "6, BCE loss: 0.214, amex train: 0.790, val 0.779\n",
      "6, BCE loss: 0.220, amex train: 0.793, val 0.779\n",
      "6, BCE loss: 0.218, amex train: 0.790, val 0.779\n",
      "6, BCE loss: 0.229, amex train: 0.775, val 0.000\n",
      "6, BCE loss: 0.228, amex train: 0.778, val 0.000\n",
      "6, BCE loss: 0.222, amex train: 0.786, val 0.779\n",
      "6, BCE loss: 0.216, amex train: 0.799, val 0.779\n",
      "6, BCE loss: 0.222, amex train: 0.784, val 0.779\n",
      "6, BCE loss: 0.211, amex train: 0.803, val 0.780\n",
      "6, BCE loss: 0.219, amex train: 0.786, val 0.779\n",
      "6, BCE loss: 0.216, amex train: 0.794, val 0.779\n",
      "6, BCE loss: 0.219, amex train: 0.783, val 0.779\n",
      "6, BCE loss: 0.215, amex train: 0.794, val 0.779\n",
      "6, BCE loss: 0.220, amex train: 0.787, val 0.779\n",
      "6, BCE loss: 0.219, amex train: 0.787, val 0.779\n",
      "7, BCE loss: 0.216, amex train: 0.800, val 0.780\n",
      "7, BCE loss: 0.218, amex train: 0.794, val 0.780\n",
      "7, BCE loss: 0.220, amex train: 0.794, val 0.780\n",
      "7, BCE loss: 0.212, amex train: 0.800, val 0.780\n",
      "7, BCE loss: 0.217, amex train: 0.798, val 0.779\n",
      "7, BCE loss: 0.224, amex train: 0.782, val 0.779\n",
      "7, BCE loss: 0.214, amex train: 0.790, val 0.779\n",
      "7, BCE loss: 0.220, amex train: 0.793, val 0.779\n",
      "7, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "7, BCE loss: 0.229, amex train: 0.775, val 0.000\n",
      "7, BCE loss: 0.228, amex train: 0.779, val 0.000\n",
      "7, BCE loss: 0.222, amex train: 0.785, val 0.779\n",
      "7, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "7, BCE loss: 0.222, amex train: 0.783, val 0.779\n",
      "7, BCE loss: 0.211, amex train: 0.803, val 0.780\n",
      "7, BCE loss: 0.218, amex train: 0.787, val 0.780\n",
      "7, BCE loss: 0.216, amex train: 0.794, val 0.779\n",
      "7, BCE loss: 0.219, amex train: 0.783, val 0.778\n",
      "7, BCE loss: 0.215, amex train: 0.794, val 0.778\n",
      "7, BCE loss: 0.220, amex train: 0.787, val 0.779\n",
      "7, BCE loss: 0.219, amex train: 0.789, val 0.779\n",
      "8, BCE loss: 0.216, amex train: 0.801, val 0.779\n",
      "8, BCE loss: 0.218, amex train: 0.794, val 0.780\n",
      "8, BCE loss: 0.220, amex train: 0.793, val 0.780\n",
      "8, BCE loss: 0.211, amex train: 0.801, val 0.780\n",
      "8, BCE loss: 0.217, amex train: 0.798, val 0.779\n",
      "8, BCE loss: 0.224, amex train: 0.781, val 0.779\n",
      "8, BCE loss: 0.214, amex train: 0.790, val 0.779\n",
      "8, BCE loss: 0.219, amex train: 0.794, val 0.779\n",
      "8, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "8, BCE loss: 0.229, amex train: 0.775, val 0.000\n",
      "8, BCE loss: 0.227, amex train: 0.779, val 0.000\n",
      "8, BCE loss: 0.222, amex train: 0.786, val 0.779\n",
      "8, BCE loss: 0.215, amex train: 0.797, val 0.779\n",
      "8, BCE loss: 0.222, amex train: 0.784, val 0.780\n",
      "8, BCE loss: 0.210, amex train: 0.803, val 0.779\n",
      "8, BCE loss: 0.218, amex train: 0.787, val 0.779\n",
      "8, BCE loss: 0.215, amex train: 0.794, val 0.779\n",
      "8, BCE loss: 0.219, amex train: 0.783, val 0.778\n",
      "8, BCE loss: 0.215, amex train: 0.794, val 0.779\n",
      "8, BCE loss: 0.219, amex train: 0.787, val 0.779\n",
      "8, BCE loss: 0.218, amex train: 0.790, val 0.779\n",
      "9, BCE loss: 0.216, amex train: 0.800, val 0.779\n",
      "9, BCE loss: 0.217, amex train: 0.793, val 0.779\n",
      "9, BCE loss: 0.219, amex train: 0.792, val 0.779\n",
      "9, BCE loss: 0.211, amex train: 0.800, val 0.779\n",
      "9, BCE loss: 0.217, amex train: 0.798, val 0.779\n",
      "9, BCE loss: 0.223, amex train: 0.782, val 0.779\n",
      "9, BCE loss: 0.214, amex train: 0.789, val 0.779\n",
      "9, BCE loss: 0.219, amex train: 0.794, val 0.779\n",
      "9, BCE loss: 0.217, amex train: 0.791, val 0.779\n",
      "9, BCE loss: 0.229, amex train: 0.775, val 0.000\n",
      "9, BCE loss: 0.227, amex train: 0.779, val 0.000\n",
      "9, BCE loss: 0.222, amex train: 0.785, val 0.779\n",
      "9, BCE loss: 0.215, amex train: 0.798, val 0.779\n",
      "9, BCE loss: 0.222, amex train: 0.784, val 0.780\n",
      "9, BCE loss: 0.210, amex train: 0.802, val 0.779\n",
      "9, BCE loss: 0.218, amex train: 0.787, val 0.779\n",
      "9, BCE loss: 0.215, amex train: 0.794, val 0.779\n",
      "9, BCE loss: 0.219, amex train: 0.783, val 0.778\n",
      "9, BCE loss: 0.215, amex train: 0.794, val 0.778\n",
      "9, BCE loss: 0.219, amex train: 0.788, val 0.779\n",
      "9, BCE loss: 0.218, amex train: 0.790, val 0.779\n",
      "10, BCE loss: 0.216, amex train: 0.800, val 0.779\n",
      "10, BCE loss: 0.217, amex train: 0.794, val 0.779\n",
      "10, BCE loss: 0.219, amex train: 0.792, val 0.779\n",
      "10, BCE loss: 0.211, amex train: 0.801, val 0.779\n",
      "10, BCE loss: 0.216, amex train: 0.798, val 0.780\n",
      "10, BCE loss: 0.223, amex train: 0.782, val 0.780\n",
      "10, BCE loss: 0.214, amex train: 0.790, val 0.779\n",
      "10, BCE loss: 0.219, amex train: 0.795, val 0.779\n",
      "10, BCE loss: 0.217, amex train: 0.791, val 0.779\n",
      "10, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "10, BCE loss: 0.227, amex train: 0.780, val 0.000\n",
      "10, BCE loss: 0.222, amex train: 0.785, val 0.779\n",
      "10, BCE loss: 0.215, amex train: 0.797, val 0.779\n",
      "10, BCE loss: 0.222, amex train: 0.784, val 0.780\n",
      "10, BCE loss: 0.210, amex train: 0.802, val 0.779\n",
      "10, BCE loss: 0.218, amex train: 0.789, val 0.779\n",
      "10, BCE loss: 0.215, amex train: 0.794, val 0.779\n",
      "10, BCE loss: 0.219, amex train: 0.783, val 0.778\n",
      "10, BCE loss: 0.215, amex train: 0.795, val 0.778\n",
      "10, BCE loss: 0.219, amex train: 0.788, val 0.779\n",
      "10, BCE loss: 0.218, amex train: 0.790, val 0.779\n",
      "11, BCE loss: 0.216, amex train: 0.800, val 0.779\n",
      "11, BCE loss: 0.217, amex train: 0.793, val 0.779\n",
      "11, BCE loss: 0.219, amex train: 0.793, val 0.780\n",
      "11, BCE loss: 0.211, amex train: 0.800, val 0.780\n",
      "11, BCE loss: 0.216, amex train: 0.797, val 0.780\n",
      "11, BCE loss: 0.223, amex train: 0.781, val 0.780\n",
      "11, BCE loss: 0.214, amex train: 0.790, val 0.779\n",
      "11, BCE loss: 0.219, amex train: 0.796, val 0.779\n",
      "11, BCE loss: 0.217, amex train: 0.791, val 0.779\n",
      "11, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "11, BCE loss: 0.227, amex train: 0.780, val 0.000\n",
      "11, BCE loss: 0.222, amex train: 0.785, val 0.779\n",
      "11, BCE loss: 0.215, amex train: 0.798, val 0.779\n",
      "11, BCE loss: 0.222, amex train: 0.784, val 0.780\n",
      "11, BCE loss: 0.210, amex train: 0.803, val 0.779\n",
      "11, BCE loss: 0.218, amex train: 0.789, val 0.779\n",
      "11, BCE loss: 0.215, amex train: 0.794, val 0.779\n",
      "11, BCE loss: 0.219, amex train: 0.783, val 0.778\n",
      "11, BCE loss: 0.215, amex train: 0.795, val 0.779\n",
      "11, BCE loss: 0.219, amex train: 0.788, val 0.779\n",
      "11, BCE loss: 0.218, amex train: 0.790, val 0.779\n",
      "12, BCE loss: 0.215, amex train: 0.800, val 0.779\n",
      "12, BCE loss: 0.217, amex train: 0.794, val 0.779\n",
      "12, BCE loss: 0.219, amex train: 0.793, val 0.780\n",
      "12, BCE loss: 0.211, amex train: 0.800, val 0.780\n",
      "12, BCE loss: 0.216, amex train: 0.798, val 0.780\n",
      "12, BCE loss: 0.223, amex train: 0.781, val 0.779\n",
      "12, BCE loss: 0.214, amex train: 0.790, val 0.779\n",
      "12, BCE loss: 0.219, amex train: 0.797, val 0.780\n",
      "12, BCE loss: 0.217, amex train: 0.791, val 0.780\n",
      "12, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "12, BCE loss: 0.227, amex train: 0.780, val 0.000\n",
      "12, BCE loss: 0.222, amex train: 0.785, val 0.779\n",
      "12, BCE loss: 0.215, amex train: 0.798, val 0.779\n",
      "12, BCE loss: 0.222, amex train: 0.785, val 0.780\n",
      "12, BCE loss: 0.210, amex train: 0.803, val 0.780\n",
      "12, BCE loss: 0.218, amex train: 0.789, val 0.778\n",
      "12, BCE loss: 0.215, amex train: 0.795, val 0.778\n",
      "12, BCE loss: 0.219, amex train: 0.783, val 0.779\n",
      "12, BCE loss: 0.215, amex train: 0.795, val 0.778\n",
      "12, BCE loss: 0.219, amex train: 0.788, val 0.778\n",
      "12, BCE loss: 0.218, amex train: 0.790, val 0.779\n",
      "13, BCE loss: 0.215, amex train: 0.799, val 0.780\n",
      "13, BCE loss: 0.217, amex train: 0.794, val 0.780\n",
      "13, BCE loss: 0.219, amex train: 0.793, val 0.780\n",
      "13, BCE loss: 0.211, amex train: 0.800, val 0.780\n",
      "13, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "13, BCE loss: 0.223, amex train: 0.782, val 0.779\n",
      "13, BCE loss: 0.214, amex train: 0.790, val 0.780\n",
      "13, BCE loss: 0.219, amex train: 0.797, val 0.779\n",
      "13, BCE loss: 0.217, amex train: 0.791, val 0.780\n",
      "13, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "13, BCE loss: 0.227, amex train: 0.780, val 0.000\n",
      "13, BCE loss: 0.222, amex train: 0.785, val 0.779\n",
      "13, BCE loss: 0.215, amex train: 0.797, val 0.779\n",
      "13, BCE loss: 0.222, amex train: 0.785, val 0.780\n",
      "13, BCE loss: 0.210, amex train: 0.803, val 0.779\n",
      "13, BCE loss: 0.218, amex train: 0.790, val 0.778\n",
      "13, BCE loss: 0.215, amex train: 0.794, val 0.778\n",
      "13, BCE loss: 0.219, amex train: 0.784, val 0.778\n",
      "13, BCE loss: 0.214, amex train: 0.795, val 0.778\n",
      "13, BCE loss: 0.219, amex train: 0.788, val 0.778\n",
      "13, BCE loss: 0.218, amex train: 0.789, val 0.779\n",
      "14, BCE loss: 0.215, amex train: 0.800, val 0.779\n",
      "14, BCE loss: 0.217, amex train: 0.795, val 0.780\n",
      "14, BCE loss: 0.219, amex train: 0.794, val 0.780\n",
      "14, BCE loss: 0.211, amex train: 0.800, val 0.780\n",
      "14, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "14, BCE loss: 0.223, amex train: 0.782, val 0.779\n",
      "14, BCE loss: 0.214, amex train: 0.790, val 0.780\n",
      "14, BCE loss: 0.219, amex train: 0.798, val 0.779\n",
      "14, BCE loss: 0.217, amex train: 0.791, val 0.780\n",
      "14, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "14, BCE loss: 0.227, amex train: 0.780, val 0.000\n",
      "14, BCE loss: 0.222, amex train: 0.784, val 0.779\n",
      "14, BCE loss: 0.215, amex train: 0.797, val 0.779\n",
      "14, BCE loss: 0.222, amex train: 0.786, val 0.779\n",
      "14, BCE loss: 0.210, amex train: 0.803, val 0.779\n",
      "14, BCE loss: 0.218, amex train: 0.790, val 0.778\n",
      "14, BCE loss: 0.215, amex train: 0.794, val 0.778\n",
      "14, BCE loss: 0.219, amex train: 0.785, val 0.778\n",
      "14, BCE loss: 0.214, amex train: 0.795, val 0.778\n",
      "14, BCE loss: 0.219, amex train: 0.789, val 0.778\n",
      "14, BCE loss: 0.218, amex train: 0.789, val 0.779\n",
      "15, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "15, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "15, BCE loss: 0.219, amex train: 0.793, val 0.780\n",
      "15, BCE loss: 0.211, amex train: 0.800, val 0.779\n",
      "15, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "15, BCE loss: 0.223, amex train: 0.782, val 0.779\n",
      "15, BCE loss: 0.214, amex train: 0.791, val 0.780\n",
      "15, BCE loss: 0.218, amex train: 0.797, val 0.779\n",
      "15, BCE loss: 0.217, amex train: 0.791, val 0.779\n",
      "15, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "15, BCE loss: 0.227, amex train: 0.780, val 0.000\n",
      "15, BCE loss: 0.222, amex train: 0.784, val 0.779\n",
      "15, BCE loss: 0.215, amex train: 0.797, val 0.779\n",
      "15, BCE loss: 0.222, amex train: 0.786, val 0.779\n",
      "15, BCE loss: 0.210, amex train: 0.803, val 0.779\n",
      "15, BCE loss: 0.218, amex train: 0.791, val 0.778\n",
      "15, BCE loss: 0.215, amex train: 0.794, val 0.778\n",
      "15, BCE loss: 0.219, amex train: 0.785, val 0.778\n",
      "15, BCE loss: 0.214, amex train: 0.795, val 0.778\n",
      "15, BCE loss: 0.219, amex train: 0.790, val 0.778\n",
      "15, BCE loss: 0.218, amex train: 0.788, val 0.779\n",
      "16, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "16, BCE loss: 0.217, amex train: 0.795, val 0.779\n",
      "16, BCE loss: 0.219, amex train: 0.793, val 0.779\n",
      "16, BCE loss: 0.211, amex train: 0.800, val 0.779\n",
      "16, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "16, BCE loss: 0.223, amex train: 0.782, val 0.779\n",
      "16, BCE loss: 0.214, amex train: 0.791, val 0.780\n",
      "16, BCE loss: 0.218, amex train: 0.797, val 0.779\n",
      "16, BCE loss: 0.217, amex train: 0.791, val 0.779\n",
      "16, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "16, BCE loss: 0.227, amex train: 0.780, val 0.000\n",
      "16, BCE loss: 0.222, amex train: 0.784, val 0.779\n",
      "16, BCE loss: 0.215, amex train: 0.798, val 0.779\n",
      "16, BCE loss: 0.222, amex train: 0.786, val 0.779\n",
      "16, BCE loss: 0.210, amex train: 0.803, val 0.779\n",
      "16, BCE loss: 0.218, amex train: 0.791, val 0.778\n",
      "16, BCE loss: 0.215, amex train: 0.794, val 0.778\n",
      "16, BCE loss: 0.219, amex train: 0.785, val 0.778\n",
      "16, BCE loss: 0.214, amex train: 0.795, val 0.779\n",
      "16, BCE loss: 0.219, amex train: 0.790, val 0.778\n",
      "16, BCE loss: 0.218, amex train: 0.788, val 0.779\n",
      "17, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "17, BCE loss: 0.218, amex train: 0.795, val 0.779\n",
      "17, BCE loss: 0.219, amex train: 0.792, val 0.779\n",
      "17, BCE loss: 0.210, amex train: 0.800, val 0.779\n",
      "17, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "17, BCE loss: 0.223, amex train: 0.783, val 0.779\n",
      "17, BCE loss: 0.214, amex train: 0.791, val 0.779\n",
      "17, BCE loss: 0.218, amex train: 0.798, val 0.779\n",
      "17, BCE loss: 0.217, amex train: 0.791, val 0.779\n",
      "17, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "17, BCE loss: 0.226, amex train: 0.779, val 0.000\n",
      "17, BCE loss: 0.222, amex train: 0.785, val 0.779\n",
      "17, BCE loss: 0.215, amex train: 0.798, val 0.779\n",
      "17, BCE loss: 0.221, amex train: 0.786, val 0.779\n",
      "17, BCE loss: 0.210, amex train: 0.803, val 0.778\n",
      "17, BCE loss: 0.218, amex train: 0.791, val 0.778\n",
      "17, BCE loss: 0.215, amex train: 0.794, val 0.778\n",
      "17, BCE loss: 0.219, amex train: 0.785, val 0.778\n",
      "17, BCE loss: 0.214, amex train: 0.795, val 0.779\n",
      "17, BCE loss: 0.219, amex train: 0.790, val 0.778\n",
      "17, BCE loss: 0.218, amex train: 0.788, val 0.779\n",
      "18, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "18, BCE loss: 0.218, amex train: 0.795, val 0.779\n",
      "18, BCE loss: 0.219, amex train: 0.792, val 0.779\n",
      "18, BCE loss: 0.210, amex train: 0.800, val 0.779\n",
      "18, BCE loss: 0.216, amex train: 0.798, val 0.778\n",
      "18, BCE loss: 0.223, amex train: 0.783, val 0.779\n",
      "18, BCE loss: 0.214, amex train: 0.790, val 0.779\n",
      "18, BCE loss: 0.218, amex train: 0.798, val 0.779\n",
      "18, BCE loss: 0.217, amex train: 0.792, val 0.779\n",
      "18, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "18, BCE loss: 0.226, amex train: 0.779, val 0.000\n",
      "18, BCE loss: 0.222, amex train: 0.785, val 0.779\n",
      "18, BCE loss: 0.215, amex train: 0.798, val 0.779\n",
      "18, BCE loss: 0.221, amex train: 0.786, val 0.779\n",
      "18, BCE loss: 0.210, amex train: 0.803, val 0.778\n",
      "18, BCE loss: 0.218, amex train: 0.791, val 0.778\n",
      "18, BCE loss: 0.215, amex train: 0.794, val 0.778\n",
      "18, BCE loss: 0.219, amex train: 0.785, val 0.778\n",
      "18, BCE loss: 0.214, amex train: 0.795, val 0.778\n",
      "18, BCE loss: 0.219, amex train: 0.790, val 0.779\n",
      "18, BCE loss: 0.218, amex train: 0.789, val 0.779\n",
      "19, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "19, BCE loss: 0.218, amex train: 0.795, val 0.779\n",
      "19, BCE loss: 0.219, amex train: 0.792, val 0.779\n",
      "19, BCE loss: 0.210, amex train: 0.800, val 0.778\n",
      "19, BCE loss: 0.216, amex train: 0.798, val 0.778\n",
      "19, BCE loss: 0.223, amex train: 0.783, val 0.779\n",
      "19, BCE loss: 0.214, amex train: 0.790, val 0.778\n",
      "19, BCE loss: 0.218, amex train: 0.798, val 0.779\n",
      "19, BCE loss: 0.217, amex train: 0.792, val 0.779\n",
      "19, BCE loss: 0.229, amex train: 0.773, val 0.000\n",
      "19, BCE loss: 0.226, amex train: 0.780, val 0.000\n",
      "19, BCE loss: 0.222, amex train: 0.785, val 0.778\n",
      "19, BCE loss: 0.215, amex train: 0.798, val 0.778\n",
      "19, BCE loss: 0.221, amex train: 0.786, val 0.779\n",
      "19, BCE loss: 0.210, amex train: 0.803, val 0.778\n",
      "19, BCE loss: 0.218, amex train: 0.791, val 0.778\n",
      "19, BCE loss: 0.215, amex train: 0.794, val 0.778\n",
      "19, BCE loss: 0.219, amex train: 0.785, val 0.778\n",
      "19, BCE loss: 0.214, amex train: 0.795, val 0.779\n",
      "19, BCE loss: 0.219, amex train: 0.790, val 0.779\n",
      "19, BCE loss: 0.217, amex train: 0.789, val 0.779\n",
      "20, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "20, BCE loss: 0.218, amex train: 0.795, val 0.779\n",
      "20, BCE loss: 0.219, amex train: 0.792, val 0.779\n",
      "20, BCE loss: 0.210, amex train: 0.800, val 0.779\n",
      "20, BCE loss: 0.216, amex train: 0.798, val 0.778\n",
      "20, BCE loss: 0.223, amex train: 0.783, val 0.779\n",
      "20, BCE loss: 0.214, amex train: 0.791, val 0.778\n",
      "20, BCE loss: 0.218, amex train: 0.798, val 0.778\n",
      "20, BCE loss: 0.217, amex train: 0.791, val 0.779\n",
      "20, BCE loss: 0.229, amex train: 0.773, val 0.000\n",
      "20, BCE loss: 0.226, amex train: 0.780, val 0.000\n",
      "20, BCE loss: 0.222, amex train: 0.785, val 0.779\n",
      "20, BCE loss: 0.215, amex train: 0.799, val 0.778\n",
      "20, BCE loss: 0.221, amex train: 0.786, val 0.779\n",
      "20, BCE loss: 0.210, amex train: 0.803, val 0.778\n",
      "20, BCE loss: 0.218, amex train: 0.792, val 0.778\n",
      "20, BCE loss: 0.215, amex train: 0.794, val 0.778\n",
      "20, BCE loss: 0.218, amex train: 0.785, val 0.778\n",
      "20, BCE loss: 0.214, amex train: 0.796, val 0.779\n",
      "20, BCE loss: 0.219, amex train: 0.790, val 0.779\n",
      "20, BCE loss: 0.217, amex train: 0.789, val 0.779\n",
      "21, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "21, BCE loss: 0.218, amex train: 0.795, val 0.778\n",
      "21, BCE loss: 0.219, amex train: 0.792, val 0.779\n",
      "21, BCE loss: 0.210, amex train: 0.800, val 0.779\n",
      "21, BCE loss: 0.216, amex train: 0.798, val 0.778\n",
      "21, BCE loss: 0.223, amex train: 0.783, val 0.779\n",
      "21, BCE loss: 0.214, amex train: 0.791, val 0.778\n",
      "21, BCE loss: 0.218, amex train: 0.798, val 0.778\n",
      "21, BCE loss: 0.217, amex train: 0.791, val 0.778\n",
      "21, BCE loss: 0.229, amex train: 0.773, val 0.000\n",
      "21, BCE loss: 0.226, amex train: 0.780, val 0.779\n",
      "21, BCE loss: 0.222, amex train: 0.785, val 0.778\n",
      "21, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "21, BCE loss: 0.221, amex train: 0.786, val 0.779\n",
      "21, BCE loss: 0.210, amex train: 0.803, val 0.778\n",
      "21, BCE loss: 0.218, amex train: 0.792, val 0.778\n",
      "21, BCE loss: 0.215, amex train: 0.794, val 0.778\n",
      "21, BCE loss: 0.218, amex train: 0.786, val 0.778\n",
      "21, BCE loss: 0.214, amex train: 0.796, val 0.779\n",
      "21, BCE loss: 0.219, amex train: 0.790, val 0.779\n",
      "21, BCE loss: 0.217, amex train: 0.789, val 0.779\n",
      "22, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "22, BCE loss: 0.218, amex train: 0.795, val 0.778\n",
      "22, BCE loss: 0.219, amex train: 0.792, val 0.779\n",
      "22, BCE loss: 0.210, amex train: 0.801, val 0.779\n",
      "22, BCE loss: 0.216, amex train: 0.798, val 0.778\n",
      "22, BCE loss: 0.223, amex train: 0.783, val 0.779\n",
      "22, BCE loss: 0.213, amex train: 0.791, val 0.778\n",
      "22, BCE loss: 0.218, amex train: 0.798, val 0.778\n",
      "22, BCE loss: 0.217, amex train: 0.792, val 0.778\n",
      "22, BCE loss: 0.229, amex train: 0.773, val 0.000\n",
      "22, BCE loss: 0.226, amex train: 0.780, val 0.779\n",
      "22, BCE loss: 0.222, amex train: 0.785, val 0.778\n",
      "22, BCE loss: 0.215, amex train: 0.799, val 0.778\n",
      "22, BCE loss: 0.221, amex train: 0.786, val 0.779\n",
      "22, BCE loss: 0.210, amex train: 0.802, val 0.778\n",
      "22, BCE loss: 0.218, amex train: 0.791, val 0.778\n",
      "22, BCE loss: 0.214, amex train: 0.794, val 0.778\n",
      "22, BCE loss: 0.218, amex train: 0.786, val 0.778\n",
      "22, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "22, BCE loss: 0.219, amex train: 0.790, val 0.779\n",
      "22, BCE loss: 0.217, amex train: 0.790, val 0.779\n",
      "23, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "23, BCE loss: 0.218, amex train: 0.795, val 0.779\n",
      "23, BCE loss: 0.219, amex train: 0.793, val 0.779\n",
      "23, BCE loss: 0.210, amex train: 0.801, val 0.779\n",
      "23, BCE loss: 0.216, amex train: 0.798, val 0.778\n",
      "23, BCE loss: 0.223, amex train: 0.783, val 0.778\n",
      "23, BCE loss: 0.213, amex train: 0.791, val 0.778\n",
      "23, BCE loss: 0.218, amex train: 0.798, val 0.778\n",
      "23, BCE loss: 0.217, amex train: 0.791, val 0.778\n",
      "23, BCE loss: 0.229, amex train: 0.773, val 0.000\n",
      "23, BCE loss: 0.226, amex train: 0.781, val 0.779\n",
      "23, BCE loss: 0.222, amex train: 0.785, val 0.778\n",
      "23, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "23, BCE loss: 0.221, amex train: 0.786, val 0.778\n",
      "23, BCE loss: 0.210, amex train: 0.802, val 0.778\n",
      "23, BCE loss: 0.218, amex train: 0.791, val 0.778\n",
      "23, BCE loss: 0.214, amex train: 0.795, val 0.778\n",
      "23, BCE loss: 0.218, amex train: 0.786, val 0.778\n",
      "23, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "23, BCE loss: 0.219, amex train: 0.790, val 0.779\n",
      "23, BCE loss: 0.217, amex train: 0.790, val 0.779\n",
      "24, BCE loss: 0.215, amex train: 0.800, val 0.779\n",
      "24, BCE loss: 0.218, amex train: 0.795, val 0.779\n",
      "24, BCE loss: 0.219, amex train: 0.793, val 0.778\n",
      "24, BCE loss: 0.210, amex train: 0.801, val 0.779\n",
      "24, BCE loss: 0.216, amex train: 0.798, val 0.778\n",
      "24, BCE loss: 0.223, amex train: 0.783, val 0.778\n",
      "24, BCE loss: 0.213, amex train: 0.791, val 0.778\n",
      "24, BCE loss: 0.218, amex train: 0.798, val 0.778\n",
      "24, BCE loss: 0.217, amex train: 0.791, val 0.778\n",
      "24, BCE loss: 0.229, amex train: 0.773, val 0.000\n",
      "24, BCE loss: 0.226, amex train: 0.781, val 0.778\n",
      "24, BCE loss: 0.222, amex train: 0.785, val 0.778\n",
      "24, BCE loss: 0.214, amex train: 0.799, val 0.778\n",
      "24, BCE loss: 0.221, amex train: 0.786, val 0.779\n",
      "24, BCE loss: 0.210, amex train: 0.802, val 0.778\n",
      "24, BCE loss: 0.218, amex train: 0.791, val 0.778\n",
      "24, BCE loss: 0.214, amex train: 0.795, val 0.778\n",
      "24, BCE loss: 0.218, amex train: 0.786, val 0.778\n",
      "24, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "24, BCE loss: 0.219, amex train: 0.790, val 0.779\n",
      "24, BCE loss: 0.217, amex train: 0.789, val 0.778\n",
      "25, BCE loss: 0.215, amex train: 0.800, val 0.779\n",
      "25, BCE loss: 0.218, amex train: 0.795, val 0.778\n",
      "25, BCE loss: 0.219, amex train: 0.793, val 0.779\n",
      "25, BCE loss: 0.210, amex train: 0.801, val 0.778\n",
      "25, BCE loss: 0.216, amex train: 0.798, val 0.778\n",
      "25, BCE loss: 0.223, amex train: 0.783, val 0.778\n",
      "25, BCE loss: 0.213, amex train: 0.791, val 0.778\n",
      "25, BCE loss: 0.218, amex train: 0.799, val 0.778\n",
      "25, BCE loss: 0.217, amex train: 0.791, val 0.778\n",
      "25, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "25, BCE loss: 0.226, amex train: 0.781, val 0.778\n",
      "25, BCE loss: 0.222, amex train: 0.785, val 0.778\n",
      "25, BCE loss: 0.214, amex train: 0.799, val 0.779\n",
      "25, BCE loss: 0.221, amex train: 0.786, val 0.778\n",
      "25, BCE loss: 0.210, amex train: 0.802, val 0.778\n",
      "25, BCE loss: 0.218, amex train: 0.791, val 0.778\n",
      "25, BCE loss: 0.214, amex train: 0.795, val 0.778\n",
      "25, BCE loss: 0.218, amex train: 0.786, val 0.779\n",
      "25, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "25, BCE loss: 0.219, amex train: 0.790, val 0.779\n",
      "25, BCE loss: 0.217, amex train: 0.790, val 0.778\n",
      "26, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "26, BCE loss: 0.218, amex train: 0.795, val 0.778\n",
      "26, BCE loss: 0.218, amex train: 0.793, val 0.779\n",
      "26, BCE loss: 0.210, amex train: 0.802, val 0.778\n",
      "26, BCE loss: 0.216, amex train: 0.797, val 0.778\n",
      "26, BCE loss: 0.223, amex train: 0.783, val 0.779\n",
      "26, BCE loss: 0.213, amex train: 0.791, val 0.778\n",
      "26, BCE loss: 0.218, amex train: 0.799, val 0.778\n",
      "26, BCE loss: 0.217, amex train: 0.792, val 0.778\n",
      "26, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "26, BCE loss: 0.226, amex train: 0.781, val 0.779\n",
      "26, BCE loss: 0.222, amex train: 0.785, val 0.778\n",
      "26, BCE loss: 0.214, amex train: 0.799, val 0.778\n",
      "26, BCE loss: 0.221, amex train: 0.786, val 0.778\n",
      "26, BCE loss: 0.210, amex train: 0.802, val 0.778\n",
      "26, BCE loss: 0.218, amex train: 0.791, val 0.778\n",
      "26, BCE loss: 0.214, amex train: 0.795, val 0.778\n",
      "26, BCE loss: 0.218, amex train: 0.787, val 0.778\n",
      "26, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "26, BCE loss: 0.219, amex train: 0.790, val 0.779\n",
      "26, BCE loss: 0.217, amex train: 0.790, val 0.778\n",
      "27, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "27, BCE loss: 0.217, amex train: 0.796, val 0.778\n",
      "27, BCE loss: 0.218, amex train: 0.793, val 0.779\n",
      "27, BCE loss: 0.210, amex train: 0.802, val 0.778\n",
      "27, BCE loss: 0.216, amex train: 0.797, val 0.778\n",
      "27, BCE loss: 0.223, amex train: 0.784, val 0.779\n",
      "27, BCE loss: 0.213, amex train: 0.791, val 0.778\n",
      "27, BCE loss: 0.218, amex train: 0.799, val 0.778\n",
      "27, BCE loss: 0.216, amex train: 0.792, val 0.778\n",
      "27, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "27, BCE loss: 0.226, amex train: 0.781, val 0.778\n",
      "27, BCE loss: 0.222, amex train: 0.785, val 0.778\n",
      "27, BCE loss: 0.214, amex train: 0.799, val 0.779\n",
      "27, BCE loss: 0.221, amex train: 0.787, val 0.778\n",
      "27, BCE loss: 0.210, amex train: 0.803, val 0.778\n",
      "27, BCE loss: 0.218, amex train: 0.792, val 0.778\n",
      "27, BCE loss: 0.214, amex train: 0.795, val 0.778\n",
      "27, BCE loss: 0.218, amex train: 0.787, val 0.779\n",
      "27, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "27, BCE loss: 0.219, amex train: 0.790, val 0.779\n",
      "27, BCE loss: 0.217, amex train: 0.790, val 0.779\n",
      "28, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "28, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "28, BCE loss: 0.218, amex train: 0.793, val 0.779\n",
      "28, BCE loss: 0.210, amex train: 0.801, val 0.778\n",
      "28, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "28, BCE loss: 0.223, amex train: 0.784, val 0.778\n",
      "28, BCE loss: 0.213, amex train: 0.791, val 0.779\n",
      "28, BCE loss: 0.218, amex train: 0.799, val 0.778\n",
      "28, BCE loss: 0.216, amex train: 0.792, val 0.778\n",
      "28, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "28, BCE loss: 0.226, amex train: 0.781, val 0.778\n",
      "28, BCE loss: 0.222, amex train: 0.785, val 0.778\n",
      "28, BCE loss: 0.214, amex train: 0.799, val 0.779\n",
      "28, BCE loss: 0.221, amex train: 0.787, val 0.778\n",
      "28, BCE loss: 0.210, amex train: 0.803, val 0.778\n",
      "28, BCE loss: 0.218, amex train: 0.792, val 0.778\n",
      "28, BCE loss: 0.214, amex train: 0.795, val 0.778\n",
      "28, BCE loss: 0.218, amex train: 0.787, val 0.779\n",
      "28, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "28, BCE loss: 0.218, amex train: 0.790, val 0.779\n",
      "28, BCE loss: 0.216, amex train: 0.789, val 0.778\n",
      "29, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "29, BCE loss: 0.217, amex train: 0.796, val 0.778\n",
      "29, BCE loss: 0.218, amex train: 0.793, val 0.779\n",
      "29, BCE loss: 0.210, amex train: 0.802, val 0.778\n",
      "29, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "29, BCE loss: 0.223, amex train: 0.784, val 0.779\n",
      "29, BCE loss: 0.213, amex train: 0.791, val 0.778\n",
      "29, BCE loss: 0.218, amex train: 0.799, val 0.778\n",
      "29, BCE loss: 0.216, amex train: 0.792, val 0.778\n",
      "29, BCE loss: 0.229, amex train: 0.774, val 0.000\n",
      "29, BCE loss: 0.226, amex train: 0.782, val 0.778\n",
      "29, BCE loss: 0.222, amex train: 0.785, val 0.779\n",
      "29, BCE loss: 0.214, amex train: 0.799, val 0.778\n",
      "29, BCE loss: 0.221, amex train: 0.787, val 0.778\n",
      "29, BCE loss: 0.210, amex train: 0.803, val 0.778\n",
      "29, BCE loss: 0.217, amex train: 0.792, val 0.778\n",
      "29, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "29, BCE loss: 0.218, amex train: 0.787, val 0.778\n",
      "29, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "29, BCE loss: 0.218, amex train: 0.790, val 0.779\n",
      "29, BCE loss: 0.216, amex train: 0.789, val 0.779\n",
      "30, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "30, BCE loss: 0.217, amex train: 0.796, val 0.778\n",
      "30, BCE loss: 0.218, amex train: 0.793, val 0.778\n",
      "30, BCE loss: 0.210, amex train: 0.802, val 0.779\n",
      "30, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "30, BCE loss: 0.223, amex train: 0.784, val 0.779\n",
      "30, BCE loss: 0.213, amex train: 0.791, val 0.778\n",
      "30, BCE loss: 0.218, amex train: 0.799, val 0.778\n",
      "30, BCE loss: 0.216, amex train: 0.793, val 0.778\n",
      "30, BCE loss: 0.228, amex train: 0.774, val 0.000\n",
      "30, BCE loss: 0.226, amex train: 0.782, val 0.778\n",
      "30, BCE loss: 0.222, amex train: 0.785, val 0.779\n",
      "30, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "30, BCE loss: 0.221, amex train: 0.787, val 0.778\n",
      "30, BCE loss: 0.210, amex train: 0.803, val 0.778\n",
      "30, BCE loss: 0.217, amex train: 0.792, val 0.778\n",
      "30, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "30, BCE loss: 0.218, amex train: 0.787, val 0.779\n",
      "30, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "30, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "30, BCE loss: 0.216, amex train: 0.789, val 0.779\n",
      "31, BCE loss: 0.215, amex train: 0.799, val 0.778\n",
      "31, BCE loss: 0.217, amex train: 0.796, val 0.778\n",
      "31, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "31, BCE loss: 0.210, amex train: 0.802, val 0.779\n",
      "31, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "31, BCE loss: 0.223, amex train: 0.785, val 0.779\n",
      "31, BCE loss: 0.213, amex train: 0.791, val 0.778\n",
      "31, BCE loss: 0.218, amex train: 0.798, val 0.779\n",
      "31, BCE loss: 0.216, amex train: 0.793, val 0.778\n",
      "31, BCE loss: 0.228, amex train: 0.774, val 0.000\n",
      "31, BCE loss: 0.226, amex train: 0.781, val 0.778\n",
      "31, BCE loss: 0.222, amex train: 0.785, val 0.779\n",
      "31, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "31, BCE loss: 0.221, amex train: 0.787, val 0.778\n",
      "31, BCE loss: 0.210, amex train: 0.803, val 0.778\n",
      "31, BCE loss: 0.217, amex train: 0.792, val 0.778\n",
      "31, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "31, BCE loss: 0.218, amex train: 0.786, val 0.779\n",
      "31, BCE loss: 0.214, amex train: 0.797, val 0.778\n",
      "31, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "31, BCE loss: 0.216, amex train: 0.789, val 0.779\n",
      "32, BCE loss: 0.215, amex train: 0.799, val 0.778\n",
      "32, BCE loss: 0.217, amex train: 0.796, val 0.778\n",
      "32, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "32, BCE loss: 0.210, amex train: 0.802, val 0.779\n",
      "32, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "32, BCE loss: 0.223, amex train: 0.784, val 0.779\n",
      "32, BCE loss: 0.213, amex train: 0.791, val 0.778\n",
      "32, BCE loss: 0.218, amex train: 0.798, val 0.779\n",
      "32, BCE loss: 0.216, amex train: 0.793, val 0.779\n",
      "32, BCE loss: 0.228, amex train: 0.774, val 0.000\n",
      "32, BCE loss: 0.226, amex train: 0.781, val 0.778\n",
      "32, BCE loss: 0.221, amex train: 0.785, val 0.779\n",
      "32, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "32, BCE loss: 0.221, amex train: 0.787, val 0.778\n",
      "32, BCE loss: 0.210, amex train: 0.803, val 0.778\n",
      "32, BCE loss: 0.217, amex train: 0.792, val 0.778\n",
      "32, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "32, BCE loss: 0.218, amex train: 0.787, val 0.778\n",
      "32, BCE loss: 0.214, amex train: 0.797, val 0.778\n",
      "32, BCE loss: 0.218, amex train: 0.790, val 0.779\n",
      "32, BCE loss: 0.216, amex train: 0.790, val 0.779\n",
      "33, BCE loss: 0.215, amex train: 0.799, val 0.778\n",
      "33, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "33, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "33, BCE loss: 0.210, amex train: 0.803, val 0.779\n",
      "33, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "33, BCE loss: 0.223, amex train: 0.785, val 0.779\n",
      "33, BCE loss: 0.213, amex train: 0.791, val 0.778\n",
      "33, BCE loss: 0.218, amex train: 0.798, val 0.778\n",
      "33, BCE loss: 0.216, amex train: 0.793, val 0.778\n",
      "33, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "33, BCE loss: 0.226, amex train: 0.781, val 0.778\n",
      "33, BCE loss: 0.221, amex train: 0.785, val 0.779\n",
      "33, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "33, BCE loss: 0.221, amex train: 0.787, val 0.778\n",
      "33, BCE loss: 0.209, amex train: 0.802, val 0.778\n",
      "33, BCE loss: 0.217, amex train: 0.792, val 0.778\n",
      "33, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "33, BCE loss: 0.218, amex train: 0.787, val 0.778\n",
      "33, BCE loss: 0.214, amex train: 0.797, val 0.779\n",
      "33, BCE loss: 0.218, amex train: 0.790, val 0.779\n",
      "33, BCE loss: 0.216, amex train: 0.790, val 0.779\n",
      "34, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "34, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "34, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "34, BCE loss: 0.210, amex train: 0.802, val 0.779\n",
      "34, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "34, BCE loss: 0.223, amex train: 0.785, val 0.778\n",
      "34, BCE loss: 0.213, amex train: 0.790, val 0.779\n",
      "34, BCE loss: 0.218, amex train: 0.798, val 0.778\n",
      "34, BCE loss: 0.216, amex train: 0.793, val 0.778\n",
      "34, BCE loss: 0.228, amex train: 0.774, val 0.000\n",
      "34, BCE loss: 0.226, amex train: 0.781, val 0.778\n",
      "34, BCE loss: 0.221, amex train: 0.785, val 0.779\n",
      "34, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "34, BCE loss: 0.221, amex train: 0.787, val 0.778\n",
      "34, BCE loss: 0.209, amex train: 0.802, val 0.778\n",
      "34, BCE loss: 0.217, amex train: 0.792, val 0.778\n",
      "34, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "34, BCE loss: 0.218, amex train: 0.787, val 0.778\n",
      "34, BCE loss: 0.214, amex train: 0.797, val 0.779\n",
      "34, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "34, BCE loss: 0.216, amex train: 0.792, val 0.779\n",
      "35, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "35, BCE loss: 0.217, amex train: 0.796, val 0.778\n",
      "35, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "35, BCE loss: 0.210, amex train: 0.802, val 0.779\n",
      "35, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "35, BCE loss: 0.222, amex train: 0.786, val 0.779\n",
      "35, BCE loss: 0.213, amex train: 0.790, val 0.779\n",
      "35, BCE loss: 0.218, amex train: 0.797, val 0.779\n",
      "35, BCE loss: 0.216, amex train: 0.793, val 0.778\n",
      "35, BCE loss: 0.228, amex train: 0.774, val 0.000\n",
      "35, BCE loss: 0.226, amex train: 0.781, val 0.778\n",
      "35, BCE loss: 0.221, amex train: 0.786, val 0.779\n",
      "35, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "35, BCE loss: 0.221, amex train: 0.787, val 0.778\n",
      "35, BCE loss: 0.209, amex train: 0.801, val 0.778\n",
      "35, BCE loss: 0.217, amex train: 0.792, val 0.778\n",
      "35, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "35, BCE loss: 0.218, amex train: 0.787, val 0.778\n",
      "35, BCE loss: 0.214, amex train: 0.797, val 0.779\n",
      "35, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "35, BCE loss: 0.216, amex train: 0.792, val 0.779\n",
      "36, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "36, BCE loss: 0.217, amex train: 0.796, val 0.778\n",
      "36, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "36, BCE loss: 0.210, amex train: 0.802, val 0.779\n",
      "36, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "36, BCE loss: 0.222, amex train: 0.785, val 0.778\n",
      "36, BCE loss: 0.213, amex train: 0.790, val 0.779\n",
      "36, BCE loss: 0.218, amex train: 0.797, val 0.779\n",
      "36, BCE loss: 0.216, amex train: 0.793, val 0.778\n",
      "36, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "36, BCE loss: 0.226, amex train: 0.781, val 0.778\n",
      "36, BCE loss: 0.221, amex train: 0.786, val 0.779\n",
      "36, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "36, BCE loss: 0.221, amex train: 0.788, val 0.779\n",
      "36, BCE loss: 0.209, amex train: 0.801, val 0.778\n",
      "36, BCE loss: 0.217, amex train: 0.792, val 0.778\n",
      "36, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "36, BCE loss: 0.218, amex train: 0.787, val 0.778\n",
      "36, BCE loss: 0.214, amex train: 0.797, val 0.779\n",
      "36, BCE loss: 0.218, amex train: 0.791, val 0.778\n",
      "36, BCE loss: 0.216, amex train: 0.792, val 0.779\n",
      "37, BCE loss: 0.215, amex train: 0.799, val 0.779\n",
      "37, BCE loss: 0.217, amex train: 0.796, val 0.778\n",
      "37, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "37, BCE loss: 0.210, amex train: 0.802, val 0.779\n",
      "37, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "37, BCE loss: 0.222, amex train: 0.785, val 0.778\n",
      "37, BCE loss: 0.213, amex train: 0.791, val 0.779\n",
      "37, BCE loss: 0.218, amex train: 0.797, val 0.779\n",
      "37, BCE loss: 0.216, amex train: 0.794, val 0.778\n",
      "37, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "37, BCE loss: 0.226, amex train: 0.781, val 0.778\n",
      "37, BCE loss: 0.221, amex train: 0.785, val 0.779\n",
      "37, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "37, BCE loss: 0.221, amex train: 0.788, val 0.779\n",
      "37, BCE loss: 0.209, amex train: 0.801, val 0.778\n",
      "37, BCE loss: 0.217, amex train: 0.792, val 0.778\n",
      "37, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "37, BCE loss: 0.218, amex train: 0.787, val 0.778\n",
      "37, BCE loss: 0.214, amex train: 0.797, val 0.779\n",
      "37, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "37, BCE loss: 0.216, amex train: 0.792, val 0.779\n",
      "38, BCE loss: 0.215, amex train: 0.800, val 0.779\n",
      "38, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "38, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "38, BCE loss: 0.210, amex train: 0.801, val 0.779\n",
      "38, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "38, BCE loss: 0.222, amex train: 0.786, val 0.778\n",
      "38, BCE loss: 0.213, amex train: 0.791, val 0.779\n",
      "38, BCE loss: 0.218, amex train: 0.797, val 0.779\n",
      "38, BCE loss: 0.216, amex train: 0.794, val 0.778\n",
      "38, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "38, BCE loss: 0.226, amex train: 0.781, val 0.778\n",
      "38, BCE loss: 0.221, amex train: 0.785, val 0.779\n",
      "38, BCE loss: 0.214, amex train: 0.801, val 0.779\n",
      "38, BCE loss: 0.221, amex train: 0.788, val 0.779\n",
      "38, BCE loss: 0.209, amex train: 0.801, val 0.779\n",
      "38, BCE loss: 0.217, amex train: 0.792, val 0.778\n",
      "38, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "38, BCE loss: 0.218, amex train: 0.788, val 0.778\n",
      "38, BCE loss: 0.214, amex train: 0.797, val 0.779\n",
      "38, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "38, BCE loss: 0.216, amex train: 0.793, val 0.779\n",
      "39, BCE loss: 0.215, amex train: 0.800, val 0.779\n",
      "39, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "39, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "39, BCE loss: 0.210, amex train: 0.800, val 0.779\n",
      "39, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "39, BCE loss: 0.223, amex train: 0.785, val 0.778\n",
      "39, BCE loss: 0.213, amex train: 0.791, val 0.779\n",
      "39, BCE loss: 0.218, amex train: 0.797, val 0.779\n",
      "39, BCE loss: 0.216, amex train: 0.794, val 0.778\n",
      "39, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "39, BCE loss: 0.226, amex train: 0.781, val 0.778\n",
      "39, BCE loss: 0.221, amex train: 0.785, val 0.779\n",
      "39, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "39, BCE loss: 0.221, amex train: 0.788, val 0.779\n",
      "39, BCE loss: 0.209, amex train: 0.802, val 0.779\n",
      "39, BCE loss: 0.217, amex train: 0.791, val 0.778\n",
      "39, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "39, BCE loss: 0.218, amex train: 0.788, val 0.778\n",
      "39, BCE loss: 0.213, amex train: 0.797, val 0.779\n",
      "39, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "39, BCE loss: 0.216, amex train: 0.792, val 0.779\n",
      "40, BCE loss: 0.215, amex train: 0.800, val 0.779\n",
      "40, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "40, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "40, BCE loss: 0.210, amex train: 0.801, val 0.779\n",
      "40, BCE loss: 0.216, amex train: 0.798, val 0.779\n",
      "40, BCE loss: 0.223, amex train: 0.785, val 0.778\n",
      "40, BCE loss: 0.213, amex train: 0.791, val 0.779\n",
      "40, BCE loss: 0.218, amex train: 0.796, val 0.779\n",
      "40, BCE loss: 0.216, amex train: 0.794, val 0.778\n",
      "40, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "40, BCE loss: 0.226, amex train: 0.782, val 0.778\n",
      "40, BCE loss: 0.221, amex train: 0.786, val 0.779\n",
      "40, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "40, BCE loss: 0.221, amex train: 0.787, val 0.779\n",
      "40, BCE loss: 0.209, amex train: 0.802, val 0.779\n",
      "40, BCE loss: 0.217, amex train: 0.791, val 0.778\n",
      "40, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "40, BCE loss: 0.218, amex train: 0.788, val 0.778\n",
      "40, BCE loss: 0.213, amex train: 0.797, val 0.779\n",
      "40, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "40, BCE loss: 0.216, amex train: 0.792, val 0.779\n",
      "41, BCE loss: 0.215, amex train: 0.800, val 0.779\n",
      "41, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "41, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "41, BCE loss: 0.210, amex train: 0.801, val 0.778\n",
      "41, BCE loss: 0.216, amex train: 0.798, val 0.778\n",
      "41, BCE loss: 0.223, amex train: 0.786, val 0.779\n",
      "41, BCE loss: 0.213, amex train: 0.792, val 0.779\n",
      "41, BCE loss: 0.218, amex train: 0.796, val 0.779\n",
      "41, BCE loss: 0.216, amex train: 0.794, val 0.778\n",
      "41, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "41, BCE loss: 0.226, amex train: 0.782, val 0.779\n",
      "41, BCE loss: 0.221, amex train: 0.786, val 0.779\n",
      "41, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "41, BCE loss: 0.221, amex train: 0.787, val 0.780\n",
      "41, BCE loss: 0.209, amex train: 0.802, val 0.779\n",
      "41, BCE loss: 0.217, amex train: 0.791, val 0.778\n",
      "41, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "41, BCE loss: 0.218, amex train: 0.788, val 0.778\n",
      "41, BCE loss: 0.213, amex train: 0.797, val 0.779\n",
      "41, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "41, BCE loss: 0.216, amex train: 0.792, val 0.779\n",
      "42, BCE loss: 0.215, amex train: 0.801, val 0.779\n",
      "42, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "42, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "42, BCE loss: 0.210, amex train: 0.801, val 0.779\n",
      "42, BCE loss: 0.216, amex train: 0.799, val 0.778\n",
      "42, BCE loss: 0.223, amex train: 0.786, val 0.778\n",
      "42, BCE loss: 0.213, amex train: 0.792, val 0.779\n",
      "42, BCE loss: 0.218, amex train: 0.797, val 0.779\n",
      "42, BCE loss: 0.216, amex train: 0.793, val 0.778\n",
      "42, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "42, BCE loss: 0.225, amex train: 0.782, val 0.778\n",
      "42, BCE loss: 0.221, amex train: 0.785, val 0.780\n",
      "42, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "42, BCE loss: 0.221, amex train: 0.787, val 0.780\n",
      "42, BCE loss: 0.209, amex train: 0.802, val 0.779\n",
      "42, BCE loss: 0.217, amex train: 0.791, val 0.778\n",
      "42, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "42, BCE loss: 0.218, amex train: 0.788, val 0.778\n",
      "42, BCE loss: 0.213, amex train: 0.797, val 0.779\n",
      "42, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "42, BCE loss: 0.216, amex train: 0.792, val 0.779\n",
      "43, BCE loss: 0.215, amex train: 0.801, val 0.779\n",
      "43, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "43, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "43, BCE loss: 0.210, amex train: 0.801, val 0.779\n",
      "43, BCE loss: 0.216, amex train: 0.798, val 0.778\n",
      "43, BCE loss: 0.223, amex train: 0.786, val 0.778\n",
      "43, BCE loss: 0.213, amex train: 0.791, val 0.779\n",
      "43, BCE loss: 0.218, amex train: 0.797, val 0.779\n",
      "43, BCE loss: 0.216, amex train: 0.793, val 0.778\n",
      "43, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "43, BCE loss: 0.225, amex train: 0.783, val 0.779\n",
      "43, BCE loss: 0.221, amex train: 0.785, val 0.780\n",
      "43, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "43, BCE loss: 0.221, amex train: 0.787, val 0.780\n",
      "43, BCE loss: 0.209, amex train: 0.802, val 0.779\n",
      "43, BCE loss: 0.217, amex train: 0.791, val 0.778\n",
      "43, BCE loss: 0.214, amex train: 0.796, val 0.778\n",
      "43, BCE loss: 0.218, amex train: 0.788, val 0.778\n",
      "43, BCE loss: 0.213, amex train: 0.797, val 0.779\n",
      "43, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "43, BCE loss: 0.216, amex train: 0.792, val 0.779\n",
      "44, BCE loss: 0.215, amex train: 0.801, val 0.779\n",
      "44, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "44, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "44, BCE loss: 0.210, amex train: 0.801, val 0.779\n",
      "44, BCE loss: 0.215, amex train: 0.798, val 0.778\n",
      "44, BCE loss: 0.223, amex train: 0.786, val 0.778\n",
      "44, BCE loss: 0.213, amex train: 0.791, val 0.779\n",
      "44, BCE loss: 0.218, amex train: 0.798, val 0.779\n",
      "44, BCE loss: 0.216, amex train: 0.794, val 0.778\n",
      "44, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "44, BCE loss: 0.225, amex train: 0.783, val 0.779\n",
      "44, BCE loss: 0.221, amex train: 0.785, val 0.780\n",
      "44, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "44, BCE loss: 0.221, amex train: 0.787, val 0.780\n",
      "44, BCE loss: 0.209, amex train: 0.803, val 0.779\n",
      "44, BCE loss: 0.217, amex train: 0.791, val 0.778\n",
      "44, BCE loss: 0.214, amex train: 0.797, val 0.778\n",
      "44, BCE loss: 0.218, amex train: 0.788, val 0.778\n",
      "44, BCE loss: 0.213, amex train: 0.798, val 0.778\n",
      "44, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "44, BCE loss: 0.216, amex train: 0.792, val 0.779\n",
      "45, BCE loss: 0.215, amex train: 0.802, val 0.779\n",
      "45, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "45, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "45, BCE loss: 0.210, amex train: 0.801, val 0.778\n",
      "45, BCE loss: 0.215, amex train: 0.798, val 0.778\n",
      "45, BCE loss: 0.223, amex train: 0.786, val 0.778\n",
      "45, BCE loss: 0.213, amex train: 0.791, val 0.779\n",
      "45, BCE loss: 0.218, amex train: 0.798, val 0.779\n",
      "45, BCE loss: 0.216, amex train: 0.794, val 0.778\n",
      "45, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "45, BCE loss: 0.225, amex train: 0.784, val 0.779\n",
      "45, BCE loss: 0.221, amex train: 0.785, val 0.779\n",
      "45, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "45, BCE loss: 0.221, amex train: 0.787, val 0.780\n",
      "45, BCE loss: 0.209, amex train: 0.803, val 0.779\n",
      "45, BCE loss: 0.217, amex train: 0.791, val 0.778\n",
      "45, BCE loss: 0.214, amex train: 0.797, val 0.778\n",
      "45, BCE loss: 0.218, amex train: 0.788, val 0.778\n",
      "45, BCE loss: 0.213, amex train: 0.798, val 0.778\n",
      "45, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "45, BCE loss: 0.216, amex train: 0.792, val 0.779\n",
      "46, BCE loss: 0.215, amex train: 0.802, val 0.779\n",
      "46, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "46, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "46, BCE loss: 0.210, amex train: 0.801, val 0.778\n",
      "46, BCE loss: 0.215, amex train: 0.798, val 0.779\n",
      "46, BCE loss: 0.223, amex train: 0.786, val 0.778\n",
      "46, BCE loss: 0.213, amex train: 0.791, val 0.780\n",
      "46, BCE loss: 0.218, amex train: 0.797, val 0.779\n",
      "46, BCE loss: 0.216, amex train: 0.794, val 0.778\n",
      "46, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "46, BCE loss: 0.225, amex train: 0.784, val 0.779\n",
      "46, BCE loss: 0.221, amex train: 0.785, val 0.780\n",
      "46, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "46, BCE loss: 0.221, amex train: 0.787, val 0.780\n",
      "46, BCE loss: 0.209, amex train: 0.803, val 0.779\n",
      "46, BCE loss: 0.217, amex train: 0.791, val 0.778\n",
      "46, BCE loss: 0.214, amex train: 0.797, val 0.778\n",
      "46, BCE loss: 0.218, amex train: 0.788, val 0.778\n",
      "46, BCE loss: 0.213, amex train: 0.798, val 0.778\n",
      "46, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "46, BCE loss: 0.215, amex train: 0.793, val 0.779\n",
      "47, BCE loss: 0.215, amex train: 0.802, val 0.780\n",
      "47, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "47, BCE loss: 0.218, amex train: 0.791, val 0.779\n",
      "47, BCE loss: 0.210, amex train: 0.801, val 0.778\n",
      "47, BCE loss: 0.215, amex train: 0.798, val 0.778\n",
      "47, BCE loss: 0.223, amex train: 0.786, val 0.778\n",
      "47, BCE loss: 0.213, amex train: 0.791, val 0.779\n",
      "47, BCE loss: 0.218, amex train: 0.797, val 0.779\n",
      "47, BCE loss: 0.216, amex train: 0.794, val 0.778\n",
      "47, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "47, BCE loss: 0.225, amex train: 0.784, val 0.779\n",
      "47, BCE loss: 0.221, amex train: 0.785, val 0.779\n",
      "47, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "47, BCE loss: 0.221, amex train: 0.787, val 0.780\n",
      "47, BCE loss: 0.209, amex train: 0.803, val 0.780\n",
      "47, BCE loss: 0.217, amex train: 0.791, val 0.778\n",
      "47, BCE loss: 0.214, amex train: 0.797, val 0.778\n",
      "47, BCE loss: 0.218, amex train: 0.788, val 0.778\n",
      "47, BCE loss: 0.213, amex train: 0.798, val 0.779\n",
      "47, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "47, BCE loss: 0.215, amex train: 0.792, val 0.779\n",
      "48, BCE loss: 0.215, amex train: 0.802, val 0.780\n",
      "48, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "48, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "48, BCE loss: 0.210, amex train: 0.801, val 0.779\n",
      "48, BCE loss: 0.215, amex train: 0.798, val 0.778\n",
      "48, BCE loss: 0.223, amex train: 0.786, val 0.778\n",
      "48, BCE loss: 0.213, amex train: 0.791, val 0.779\n",
      "48, BCE loss: 0.218, amex train: 0.796, val 0.779\n",
      "48, BCE loss: 0.216, amex train: 0.794, val 0.779\n",
      "48, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "48, BCE loss: 0.225, amex train: 0.783, val 0.779\n",
      "48, BCE loss: 0.221, amex train: 0.785, val 0.779\n",
      "48, BCE loss: 0.214, amex train: 0.800, val 0.779\n",
      "48, BCE loss: 0.221, amex train: 0.787, val 0.780\n",
      "48, BCE loss: 0.209, amex train: 0.803, val 0.780\n",
      "48, BCE loss: 0.217, amex train: 0.791, val 0.779\n",
      "48, BCE loss: 0.214, amex train: 0.797, val 0.778\n",
      "48, BCE loss: 0.218, amex train: 0.788, val 0.778\n",
      "48, BCE loss: 0.213, amex train: 0.798, val 0.779\n",
      "48, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "48, BCE loss: 0.215, amex train: 0.793, val 0.779\n",
      "49, BCE loss: 0.214, amex train: 0.802, val 0.779\n",
      "49, BCE loss: 0.217, amex train: 0.796, val 0.779\n",
      "49, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "49, BCE loss: 0.210, amex train: 0.801, val 0.779\n",
      "49, BCE loss: 0.215, amex train: 0.798, val 0.779\n",
      "49, BCE loss: 0.223, amex train: 0.786, val 0.779\n",
      "49, BCE loss: 0.213, amex train: 0.791, val 0.779\n",
      "49, BCE loss: 0.218, amex train: 0.796, val 0.779\n",
      "49, BCE loss: 0.215, amex train: 0.794, val 0.779\n",
      "49, BCE loss: 0.228, amex train: 0.773, val 0.000\n",
      "49, BCE loss: 0.225, amex train: 0.783, val 0.779\n",
      "49, BCE loss: 0.221, amex train: 0.785, val 0.779\n",
      "49, BCE loss: 0.214, amex train: 0.801, val 0.779\n",
      "49, BCE loss: 0.221, amex train: 0.787, val 0.780\n",
      "49, BCE loss: 0.209, amex train: 0.803, val 0.780\n",
      "49, BCE loss: 0.217, amex train: 0.791, val 0.779\n",
      "49, BCE loss: 0.214, amex train: 0.797, val 0.778\n",
      "49, BCE loss: 0.218, amex train: 0.789, val 0.778\n",
      "49, BCE loss: 0.213, amex train: 0.798, val 0.778\n",
      "49, BCE loss: 0.218, amex train: 0.792, val 0.779\n",
      "49, BCE loss: 0.215, amex train: 0.792, val 0.779\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_dim=X.shape[-1])\n",
    "optimizer = torch.optim.Adam(model.parameters(),)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "for epoch in range(50): \n",
    "    for idx, (feat, clabel) in enumerate(train_loader):\n",
    "        if len(feat.shape) == 4:  ## Reduce shape if its coming from a ratio version of the loader\n",
    "            feat = feat.squeeze(dim=0)\n",
    "            clabel = clabel.squeeze(dim=0)\n",
    "\n",
    "        pred = model(feat)\n",
    "        #weight = clabel.clone()\n",
    "        #weight[weight==0] = 4\n",
    "        #criterion.weight = weight\n",
    "        loss = criterion(pred, clabel)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model_metric = amex_metric(clabel.detach().numpy(), pred.detach().numpy())\n",
    "        val_metrix = 0\n",
    "        if model_metric > 0.78:\n",
    "            X_test, y_test = validation_data\n",
    "            val_features = torch.as_tensor(X_test, dtype=torch.float32)\n",
    "            val_pred = model(val_features)\n",
    "            val_metrix = amex_metric(y_test, val_pred.detach().numpy())\n",
    "\n",
    "        log_message = f\"{epoch}, BCE loss: {loss.item():.3f}, amex train: {model_metric:.3f}, val {val_metrix:.3f}\"\n",
    "        print(log_message)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(X_test)\n",
    "amex_metric(y_test, val_pred)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with Given preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import tempfile\n",
    "import click\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from torch.utils.data import Sampler, BatchSampler\n",
    "from pd.metric import amex_metric\n",
    "from pd.utils import write_log\n",
    "from pd.nn.conv import ConvPred\n",
    "from pd.utils import merge_with_pred, get_torch_agg_data, get_customers_data_indices, get_pred_data\n",
    "from pd.params import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(OUTDIR+\"train_logistic_raw_all_mean_q5_q95_q5_q95_data.npy\")\n",
    "train_labels = np.load(OUTDIR+\"train_logistic_raw_all_mean_q5_q95_q5_q95_labels.npy\")\n",
    "    \n",
    "pred_data = get_pred_data(type=\"train\", id_dir='train_logistic_raw_all_mean_q5_q95_q5_q95_id.json')\n",
    "\n",
    "model_name = f\"conv_pred{agg}\"\n",
    "indices = get_customers_data_indices(num_data_points=np.arange(14), id_dir='train_logistic_raw_all_mean_q5_q95_q5_q95_id.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import os\n",
    "import click\n",
    "import functools\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from bes.nn.es_module import ESModule\n",
    "import ray\n",
    "import tempfile\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bes.es import CMAES, BES\n",
    "from pd.nn.mlp import Linear\n",
    "from pd.params import *\n",
    "#from pd.data.loader import load_npy_data as load_data\n",
    "from pd.metric import amex_metric\n",
    "from pd.nn.recall_models import MLP, MLPAtt\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(PREDDIR+\"train_pred.csv\", index_col=0)\n",
    "train_labels = train_data[\"target\"].values\n",
    "train_data[\"STD\"] = train_data.apply(np.std, axis=1)\n",
    "train_data[\"STD\"] = train_data[\"STD\"]/train_data[\"STD\"].max()\n",
    "#train_data = train_data.drop(\"target\", axis=1)\n",
    "cols  = [col for col in train_data.columns if col != \"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=1/9, random_state=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1] - 1\n",
    "model = MLPAtt(input_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5634"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\20204069\\\\Desktop\\\\Kaggle\\\\pd\\\\data\\\\out\\\\best_iteration_2125.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-856b1fab2a04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2125\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2126\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2127\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2128\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mOUTDIR\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34mf\"best_iteration_{i}.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_model_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#val_features = torch.from_numpy(X_test[cols].values).float()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#val_pred = model(val_features)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\20204069\\Anaconda3\\envs\\p8\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\20204069\\\\Desktop\\\\Kaggle\\\\pd\\\\data\\\\out\\\\best_iteration_2125.npy'"
     ]
    }
   ],
   "source": [
    "for i in [2125, 2126, 2127, 2128]:\n",
    "    params = np.load(OUTDIR+f\"best_iteration_{i}.npy\")\n",
    "    model.set_model_params(params)\n",
    "    val_features = torch.from_numpy(X_test[cols].values).float()\n",
    "    val_pred = model(val_features)\n",
    "    val_metrix = amex_metric(y_test, val_pred.detach().numpy(), return_components=True)\n",
    "    print(\"The val \", val_metrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2431],\n",
       "        [0.2265],\n",
       "        [0.2440],\n",
       "        ...,\n",
       "        [0.2411],\n",
       "        [0.2265],\n",
       "        [0.2398]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>xgbm13_p0_agg4</th>\n",
       "      <th>catb13_agg4</th>\n",
       "      <th>xgbm_p0_agg4</th>\n",
       "      <th>catb_agg4</th>\n",
       "      <th>xgbm13_p0_agg1</th>\n",
       "      <th>catb13_agg1</th>\n",
       "      <th>xgbm_p0_agg1</th>\n",
       "      <th>catb_agg1</th>\n",
       "      <th>xgbm13_p0_agg2</th>\n",
       "      <th>...</th>\n",
       "      <th>catb_agg2</th>\n",
       "      <th>xgbm13_p0_agg0</th>\n",
       "      <th>catb13_agg0</th>\n",
       "      <th>xgbm_p0_agg0</th>\n",
       "      <th>catb_agg0</th>\n",
       "      <th>xgbmv213_p0_agg0</th>\n",
       "      <th>xgbmv2_p0_agg0</th>\n",
       "      <th>conv_pred0</th>\n",
       "      <th>STD</th>\n",
       "      <th>Att</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ebf01d675f1ffad735f18cba02fcb9e6ca89a1633f5716457246da57755b1f78</th>\n",
       "      <td>0</td>\n",
       "      <td>0.309720</td>\n",
       "      <td>0.296176</td>\n",
       "      <td>0.307184</td>\n",
       "      <td>0.355913</td>\n",
       "      <td>0.215021</td>\n",
       "      <td>0.306461</td>\n",
       "      <td>0.233854</td>\n",
       "      <td>0.314976</td>\n",
       "      <td>0.234227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.315471</td>\n",
       "      <td>0.629669</td>\n",
       "      <td>0.618208</td>\n",
       "      <td>0.611849</td>\n",
       "      <td>0.663287</td>\n",
       "      <td>0.648590</td>\n",
       "      <td>0.652470</td>\n",
       "      <td>0.062163</td>\n",
       "      <td>0.570986</td>\n",
       "      <td>0.320494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87ddf1ad5bd3e1d038f5d7e640c647a416e4944014768f94395425a94297652a</th>\n",
       "      <td>0</td>\n",
       "      <td>0.205436</td>\n",
       "      <td>0.238079</td>\n",
       "      <td>0.209533</td>\n",
       "      <td>0.245606</td>\n",
       "      <td>0.217923</td>\n",
       "      <td>0.241338</td>\n",
       "      <td>0.245820</td>\n",
       "      <td>0.191445</td>\n",
       "      <td>0.225875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257072</td>\n",
       "      <td>0.627670</td>\n",
       "      <td>0.597561</td>\n",
       "      <td>0.698448</td>\n",
       "      <td>0.640387</td>\n",
       "      <td>0.670146</td>\n",
       "      <td>0.636105</td>\n",
       "      <td>0.226823</td>\n",
       "      <td>0.593282</td>\n",
       "      <td>0.310911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57bb71d548b10bed45501a0ec86e8ad61d38335ae1f69b5b5dd64243fa67a5a0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.084283</td>\n",
       "      <td>0.070323</td>\n",
       "      <td>0.440324</td>\n",
       "      <td>0.474457</td>\n",
       "      <td>0.041680</td>\n",
       "      <td>0.029544</td>\n",
       "      <td>0.265599</td>\n",
       "      <td>0.261648</td>\n",
       "      <td>0.048897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385013</td>\n",
       "      <td>0.073103</td>\n",
       "      <td>0.089830</td>\n",
       "      <td>0.577796</td>\n",
       "      <td>0.523989</td>\n",
       "      <td>0.060392</td>\n",
       "      <td>0.569014</td>\n",
       "      <td>0.208142</td>\n",
       "      <td>0.734500</td>\n",
       "      <td>0.271766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bcecd31e6a09846f509bf1006e080db314326f6dcd749c9c15df5147df9aeafb</th>\n",
       "      <td>1</td>\n",
       "      <td>0.016243</td>\n",
       "      <td>0.020338</td>\n",
       "      <td>0.022005</td>\n",
       "      <td>0.023244</td>\n",
       "      <td>0.013838</td>\n",
       "      <td>0.014074</td>\n",
       "      <td>0.032765</td>\n",
       "      <td>0.042311</td>\n",
       "      <td>0.014107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041445</td>\n",
       "      <td>0.010715</td>\n",
       "      <td>0.015951</td>\n",
       "      <td>0.015043</td>\n",
       "      <td>0.020866</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>0.015728</td>\n",
       "      <td>0.042841</td>\n",
       "      <td>0.619363</td>\n",
       "      <td>0.265544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fd56a8c99d464dd82dbae558ea4b1d77eab030eee60e20a6e86b88f7d6a0908a</th>\n",
       "      <td>1</td>\n",
       "      <td>0.011911</td>\n",
       "      <td>0.028595</td>\n",
       "      <td>0.013458</td>\n",
       "      <td>0.034085</td>\n",
       "      <td>0.014426</td>\n",
       "      <td>0.028953</td>\n",
       "      <td>0.019422</td>\n",
       "      <td>0.030693</td>\n",
       "      <td>0.012546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031547</td>\n",
       "      <td>0.011929</td>\n",
       "      <td>0.018085</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.021376</td>\n",
       "      <td>0.013789</td>\n",
       "      <td>0.016530</td>\n",
       "      <td>0.072529</td>\n",
       "      <td>0.619253</td>\n",
       "      <td>0.265508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0489c5ce0cc83266b048f70bb8f20295b398d6487c184fe2f1aa8fc9ac0bfa04</th>\n",
       "      <td>0</td>\n",
       "      <td>0.110715</td>\n",
       "      <td>0.092340</td>\n",
       "      <td>0.139149</td>\n",
       "      <td>0.122975</td>\n",
       "      <td>0.062369</td>\n",
       "      <td>0.072801</td>\n",
       "      <td>0.097148</td>\n",
       "      <td>0.096535</td>\n",
       "      <td>0.077184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098265</td>\n",
       "      <td>0.152507</td>\n",
       "      <td>0.104120</td>\n",
       "      <td>0.163868</td>\n",
       "      <td>0.146115</td>\n",
       "      <td>0.175431</td>\n",
       "      <td>0.155093</td>\n",
       "      <td>0.156723</td>\n",
       "      <td>0.122086</td>\n",
       "      <td>0.226505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239028a5c0df09621fcec9b6ddd457aa2d18b21fbf3be7749258882a57f4b96f</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004550</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>0.005790</td>\n",
       "      <td>0.003668</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.003363</td>\n",
       "      <td>0.006581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004368</td>\n",
       "      <td>0.007277</td>\n",
       "      <td>0.003766</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.004221</td>\n",
       "      <td>0.004627</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>0.007259</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.226505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3d674abe8cb89890c103748577ed1c7d087e952658fe17411b3f0e02a3066cea</th>\n",
       "      <td>0</td>\n",
       "      <td>0.028969</td>\n",
       "      <td>0.054836</td>\n",
       "      <td>0.031787</td>\n",
       "      <td>0.043711</td>\n",
       "      <td>0.034292</td>\n",
       "      <td>0.085556</td>\n",
       "      <td>0.048540</td>\n",
       "      <td>0.052052</td>\n",
       "      <td>0.041087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061763</td>\n",
       "      <td>0.040065</td>\n",
       "      <td>0.050595</td>\n",
       "      <td>0.045576</td>\n",
       "      <td>0.048228</td>\n",
       "      <td>0.038002</td>\n",
       "      <td>0.039586</td>\n",
       "      <td>0.071938</td>\n",
       "      <td>0.049605</td>\n",
       "      <td>0.226505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94dc974f05762b3a32c7a5497554d3912a28b70d1a681e5564277b04e05e92d5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.055378</td>\n",
       "      <td>0.037842</td>\n",
       "      <td>0.042505</td>\n",
       "      <td>0.043389</td>\n",
       "      <td>0.058850</td>\n",
       "      <td>0.034519</td>\n",
       "      <td>0.048571</td>\n",
       "      <td>0.036833</td>\n",
       "      <td>0.057551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033911</td>\n",
       "      <td>0.046580</td>\n",
       "      <td>0.036218</td>\n",
       "      <td>0.044266</td>\n",
       "      <td>0.038252</td>\n",
       "      <td>0.048363</td>\n",
       "      <td>0.049128</td>\n",
       "      <td>0.038870</td>\n",
       "      <td>0.034782</td>\n",
       "      <td>0.226505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7ebb5a0c33e40d9ae753f8ec01627d815998849d9e6a47b60104e9df000efd2f</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.001931</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.005015</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.226505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50991 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    target  xgbm13_p0_agg4  \\\n",
       "customer_ID                                                                  \n",
       "ebf01d675f1ffad735f18cba02fcb9e6ca89a1633f57164...       0        0.309720   \n",
       "87ddf1ad5bd3e1d038f5d7e640c647a416e4944014768f9...       0        0.205436   \n",
       "57bb71d548b10bed45501a0ec86e8ad61d38335ae1f69b5...       1        0.084283   \n",
       "bcecd31e6a09846f509bf1006e080db314326f6dcd749c9...       1        0.016243   \n",
       "fd56a8c99d464dd82dbae558ea4b1d77eab030eee60e20a...       1        0.011911   \n",
       "...                                                    ...             ...   \n",
       "0489c5ce0cc83266b048f70bb8f20295b398d6487c184fe...       0        0.110715   \n",
       "239028a5c0df09621fcec9b6ddd457aa2d18b21fbf3be77...       0        0.004550   \n",
       "3d674abe8cb89890c103748577ed1c7d087e952658fe174...       0        0.028969   \n",
       "94dc974f05762b3a32c7a5497554d3912a28b70d1a681e5...       0        0.055378   \n",
       "7ebb5a0c33e40d9ae753f8ec01627d815998849d9e6a47b...       0        0.001817   \n",
       "\n",
       "                                                    catb13_agg4  xgbm_p0_agg4  \\\n",
       "customer_ID                                                                     \n",
       "ebf01d675f1ffad735f18cba02fcb9e6ca89a1633f57164...     0.296176      0.307184   \n",
       "87ddf1ad5bd3e1d038f5d7e640c647a416e4944014768f9...     0.238079      0.209533   \n",
       "57bb71d548b10bed45501a0ec86e8ad61d38335ae1f69b5...     0.070323      0.440324   \n",
       "bcecd31e6a09846f509bf1006e080db314326f6dcd749c9...     0.020338      0.022005   \n",
       "fd56a8c99d464dd82dbae558ea4b1d77eab030eee60e20a...     0.028595      0.013458   \n",
       "...                                                         ...           ...   \n",
       "0489c5ce0cc83266b048f70bb8f20295b398d6487c184fe...     0.092340      0.139149   \n",
       "239028a5c0df09621fcec9b6ddd457aa2d18b21fbf3be77...     0.002573      0.005790   \n",
       "3d674abe8cb89890c103748577ed1c7d087e952658fe174...     0.054836      0.031787   \n",
       "94dc974f05762b3a32c7a5497554d3912a28b70d1a681e5...     0.037842      0.042505   \n",
       "7ebb5a0c33e40d9ae753f8ec01627d815998849d9e6a47b...     0.001817      0.001672   \n",
       "\n",
       "                                                    catb_agg4  xgbm13_p0_agg1  \\\n",
       "customer_ID                                                                     \n",
       "ebf01d675f1ffad735f18cba02fcb9e6ca89a1633f57164...   0.355913        0.215021   \n",
       "87ddf1ad5bd3e1d038f5d7e640c647a416e4944014768f9...   0.245606        0.217923   \n",
       "57bb71d548b10bed45501a0ec86e8ad61d38335ae1f69b5...   0.474457        0.041680   \n",
       "bcecd31e6a09846f509bf1006e080db314326f6dcd749c9...   0.023244        0.013838   \n",
       "fd56a8c99d464dd82dbae558ea4b1d77eab030eee60e20a...   0.034085        0.014426   \n",
       "...                                                       ...             ...   \n",
       "0489c5ce0cc83266b048f70bb8f20295b398d6487c184fe...   0.122975        0.062369   \n",
       "239028a5c0df09621fcec9b6ddd457aa2d18b21fbf3be77...   0.003668        0.003580   \n",
       "3d674abe8cb89890c103748577ed1c7d087e952658fe174...   0.043711        0.034292   \n",
       "94dc974f05762b3a32c7a5497554d3912a28b70d1a681e5...   0.043389        0.058850   \n",
       "7ebb5a0c33e40d9ae753f8ec01627d815998849d9e6a47b...   0.001550        0.001168   \n",
       "\n",
       "                                                    catb13_agg1  xgbm_p0_agg1  \\\n",
       "customer_ID                                                                     \n",
       "ebf01d675f1ffad735f18cba02fcb9e6ca89a1633f57164...     0.306461      0.233854   \n",
       "87ddf1ad5bd3e1d038f5d7e640c647a416e4944014768f9...     0.241338      0.245820   \n",
       "57bb71d548b10bed45501a0ec86e8ad61d38335ae1f69b5...     0.029544      0.265599   \n",
       "bcecd31e6a09846f509bf1006e080db314326f6dcd749c9...     0.014074      0.032765   \n",
       "fd56a8c99d464dd82dbae558ea4b1d77eab030eee60e20a...     0.028953      0.019422   \n",
       "...                                                         ...           ...   \n",
       "0489c5ce0cc83266b048f70bb8f20295b398d6487c184fe...     0.072801      0.097148   \n",
       "239028a5c0df09621fcec9b6ddd457aa2d18b21fbf3be77...     0.003271      0.005776   \n",
       "3d674abe8cb89890c103748577ed1c7d087e952658fe174...     0.085556      0.048540   \n",
       "94dc974f05762b3a32c7a5497554d3912a28b70d1a681e5...     0.034519      0.048571   \n",
       "7ebb5a0c33e40d9ae753f8ec01627d815998849d9e6a47b...     0.001131      0.001574   \n",
       "\n",
       "                                                    catb_agg1  xgbm13_p0_agg2  \\\n",
       "customer_ID                                                                     \n",
       "ebf01d675f1ffad735f18cba02fcb9e6ca89a1633f57164...   0.314976        0.234227   \n",
       "87ddf1ad5bd3e1d038f5d7e640c647a416e4944014768f9...   0.191445        0.225875   \n",
       "57bb71d548b10bed45501a0ec86e8ad61d38335ae1f69b5...   0.261648        0.048897   \n",
       "bcecd31e6a09846f509bf1006e080db314326f6dcd749c9...   0.042311        0.014107   \n",
       "fd56a8c99d464dd82dbae558ea4b1d77eab030eee60e20a...   0.030693        0.012546   \n",
       "...                                                       ...             ...   \n",
       "0489c5ce0cc83266b048f70bb8f20295b398d6487c184fe...   0.096535        0.077184   \n",
       "239028a5c0df09621fcec9b6ddd457aa2d18b21fbf3be77...   0.003363        0.006581   \n",
       "3d674abe8cb89890c103748577ed1c7d087e952658fe174...   0.052052        0.041087   \n",
       "94dc974f05762b3a32c7a5497554d3912a28b70d1a681e5...   0.036833        0.057551   \n",
       "7ebb5a0c33e40d9ae753f8ec01627d815998849d9e6a47b...   0.001129        0.001219   \n",
       "\n",
       "                                                    ...  catb_agg2  \\\n",
       "customer_ID                                         ...              \n",
       "ebf01d675f1ffad735f18cba02fcb9e6ca89a1633f57164...  ...   0.315471   \n",
       "87ddf1ad5bd3e1d038f5d7e640c647a416e4944014768f9...  ...   0.257072   \n",
       "57bb71d548b10bed45501a0ec86e8ad61d38335ae1f69b5...  ...   0.385013   \n",
       "bcecd31e6a09846f509bf1006e080db314326f6dcd749c9...  ...   0.041445   \n",
       "fd56a8c99d464dd82dbae558ea4b1d77eab030eee60e20a...  ...   0.031547   \n",
       "...                                                 ...        ...   \n",
       "0489c5ce0cc83266b048f70bb8f20295b398d6487c184fe...  ...   0.098265   \n",
       "239028a5c0df09621fcec9b6ddd457aa2d18b21fbf3be77...  ...   0.004368   \n",
       "3d674abe8cb89890c103748577ed1c7d087e952658fe174...  ...   0.061763   \n",
       "94dc974f05762b3a32c7a5497554d3912a28b70d1a681e5...  ...   0.033911   \n",
       "7ebb5a0c33e40d9ae753f8ec01627d815998849d9e6a47b...  ...   0.000944   \n",
       "\n",
       "                                                    xgbm13_p0_agg0  \\\n",
       "customer_ID                                                          \n",
       "ebf01d675f1ffad735f18cba02fcb9e6ca89a1633f57164...        0.629669   \n",
       "87ddf1ad5bd3e1d038f5d7e640c647a416e4944014768f9...        0.627670   \n",
       "57bb71d548b10bed45501a0ec86e8ad61d38335ae1f69b5...        0.073103   \n",
       "bcecd31e6a09846f509bf1006e080db314326f6dcd749c9...        0.010715   \n",
       "fd56a8c99d464dd82dbae558ea4b1d77eab030eee60e20a...        0.011929   \n",
       "...                                                            ...   \n",
       "0489c5ce0cc83266b048f70bb8f20295b398d6487c184fe...        0.152507   \n",
       "239028a5c0df09621fcec9b6ddd457aa2d18b21fbf3be77...        0.007277   \n",
       "3d674abe8cb89890c103748577ed1c7d087e952658fe174...        0.040065   \n",
       "94dc974f05762b3a32c7a5497554d3912a28b70d1a681e5...        0.046580   \n",
       "7ebb5a0c33e40d9ae753f8ec01627d815998849d9e6a47b...        0.001410   \n",
       "\n",
       "                                                    catb13_agg0  xgbm_p0_agg0  \\\n",
       "customer_ID                                                                     \n",
       "ebf01d675f1ffad735f18cba02fcb9e6ca89a1633f57164...     0.618208      0.611849   \n",
       "87ddf1ad5bd3e1d038f5d7e640c647a416e4944014768f9...     0.597561      0.698448   \n",
       "57bb71d548b10bed45501a0ec86e8ad61d38335ae1f69b5...     0.089830      0.577796   \n",
       "bcecd31e6a09846f509bf1006e080db314326f6dcd749c9...     0.015951      0.015043   \n",
       "fd56a8c99d464dd82dbae558ea4b1d77eab030eee60e20a...     0.018085      0.015160   \n",
       "...                                                         ...           ...   \n",
       "0489c5ce0cc83266b048f70bb8f20295b398d6487c184fe...     0.104120      0.163868   \n",
       "239028a5c0df09621fcec9b6ddd457aa2d18b21fbf3be77...     0.003766      0.006623   \n",
       "3d674abe8cb89890c103748577ed1c7d087e952658fe174...     0.050595      0.045576   \n",
       "94dc974f05762b3a32c7a5497554d3912a28b70d1a681e5...     0.036218      0.044266   \n",
       "7ebb5a0c33e40d9ae753f8ec01627d815998849d9e6a47b...     0.001931      0.001720   \n",
       "\n",
       "                                                    catb_agg0  \\\n",
       "customer_ID                                                     \n",
       "ebf01d675f1ffad735f18cba02fcb9e6ca89a1633f57164...   0.663287   \n",
       "87ddf1ad5bd3e1d038f5d7e640c647a416e4944014768f9...   0.640387   \n",
       "57bb71d548b10bed45501a0ec86e8ad61d38335ae1f69b5...   0.523989   \n",
       "bcecd31e6a09846f509bf1006e080db314326f6dcd749c9...   0.020866   \n",
       "fd56a8c99d464dd82dbae558ea4b1d77eab030eee60e20a...   0.021376   \n",
       "...                                                       ...   \n",
       "0489c5ce0cc83266b048f70bb8f20295b398d6487c184fe...   0.146115   \n",
       "239028a5c0df09621fcec9b6ddd457aa2d18b21fbf3be77...   0.004221   \n",
       "3d674abe8cb89890c103748577ed1c7d087e952658fe174...   0.048228   \n",
       "94dc974f05762b3a32c7a5497554d3912a28b70d1a681e5...   0.038252   \n",
       "7ebb5a0c33e40d9ae753f8ec01627d815998849d9e6a47b...   0.001769   \n",
       "\n",
       "                                                    xgbmv213_p0_agg0  \\\n",
       "customer_ID                                                            \n",
       "ebf01d675f1ffad735f18cba02fcb9e6ca89a1633f57164...          0.648590   \n",
       "87ddf1ad5bd3e1d038f5d7e640c647a416e4944014768f9...          0.670146   \n",
       "57bb71d548b10bed45501a0ec86e8ad61d38335ae1f69b5...          0.060392   \n",
       "bcecd31e6a09846f509bf1006e080db314326f6dcd749c9...          0.008034   \n",
       "fd56a8c99d464dd82dbae558ea4b1d77eab030eee60e20a...          0.013789   \n",
       "...                                                              ...   \n",
       "0489c5ce0cc83266b048f70bb8f20295b398d6487c184fe...          0.175431   \n",
       "239028a5c0df09621fcec9b6ddd457aa2d18b21fbf3be77...          0.004627   \n",
       "3d674abe8cb89890c103748577ed1c7d087e952658fe174...          0.038002   \n",
       "94dc974f05762b3a32c7a5497554d3912a28b70d1a681e5...          0.048363   \n",
       "7ebb5a0c33e40d9ae753f8ec01627d815998849d9e6a47b...          0.001235   \n",
       "\n",
       "                                                    xgbmv2_p0_agg0  \\\n",
       "customer_ID                                                          \n",
       "ebf01d675f1ffad735f18cba02fcb9e6ca89a1633f57164...        0.652470   \n",
       "87ddf1ad5bd3e1d038f5d7e640c647a416e4944014768f9...        0.636105   \n",
       "57bb71d548b10bed45501a0ec86e8ad61d38335ae1f69b5...        0.569014   \n",
       "bcecd31e6a09846f509bf1006e080db314326f6dcd749c9...        0.015728   \n",
       "fd56a8c99d464dd82dbae558ea4b1d77eab030eee60e20a...        0.016530   \n",
       "...                                                            ...   \n",
       "0489c5ce0cc83266b048f70bb8f20295b398d6487c184fe...        0.155093   \n",
       "239028a5c0df09621fcec9b6ddd457aa2d18b21fbf3be77...        0.002946   \n",
       "3d674abe8cb89890c103748577ed1c7d087e952658fe174...        0.039586   \n",
       "94dc974f05762b3a32c7a5497554d3912a28b70d1a681e5...        0.049128   \n",
       "7ebb5a0c33e40d9ae753f8ec01627d815998849d9e6a47b...        0.001524   \n",
       "\n",
       "                                                    conv_pred0       STD  \\\n",
       "customer_ID                                                                \n",
       "ebf01d675f1ffad735f18cba02fcb9e6ca89a1633f57164...    0.062163  0.570986   \n",
       "87ddf1ad5bd3e1d038f5d7e640c647a416e4944014768f9...    0.226823  0.593282   \n",
       "57bb71d548b10bed45501a0ec86e8ad61d38335ae1f69b5...    0.208142  0.734500   \n",
       "bcecd31e6a09846f509bf1006e080db314326f6dcd749c9...    0.042841  0.619363   \n",
       "fd56a8c99d464dd82dbae558ea4b1d77eab030eee60e20a...    0.072529  0.619253   \n",
       "...                                                        ...       ...   \n",
       "0489c5ce0cc83266b048f70bb8f20295b398d6487c184fe...    0.156723  0.122086   \n",
       "239028a5c0df09621fcec9b6ddd457aa2d18b21fbf3be77...    0.007259  0.005164   \n",
       "3d674abe8cb89890c103748577ed1c7d087e952658fe174...    0.071938  0.049605   \n",
       "94dc974f05762b3a32c7a5497554d3912a28b70d1a681e5...    0.038870  0.034782   \n",
       "7ebb5a0c33e40d9ae753f8ec01627d815998849d9e6a47b...    0.005015  0.002627   \n",
       "\n",
       "                                                         Att  \n",
       "customer_ID                                                   \n",
       "ebf01d675f1ffad735f18cba02fcb9e6ca89a1633f57164...  0.320494  \n",
       "87ddf1ad5bd3e1d038f5d7e640c647a416e4944014768f9...  0.310911  \n",
       "57bb71d548b10bed45501a0ec86e8ad61d38335ae1f69b5...  0.271766  \n",
       "bcecd31e6a09846f509bf1006e080db314326f6dcd749c9...  0.265544  \n",
       "fd56a8c99d464dd82dbae558ea4b1d77eab030eee60e20a...  0.265508  \n",
       "...                                                      ...  \n",
       "0489c5ce0cc83266b048f70bb8f20295b398d6487c184fe...  0.226505  \n",
       "239028a5c0df09621fcec9b6ddd457aa2d18b21fbf3be77...  0.226505  \n",
       "3d674abe8cb89890c103748577ed1c7d087e952658fe174...  0.226505  \n",
       "94dc974f05762b3a32c7a5497554d3912a28b70d1a681e5...  0.226505  \n",
       "7ebb5a0c33e40d9ae753f8ec01627d815998849d9e6a47b...  0.226505  \n",
       "\n",
       "[50991 rows x 22 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sort_values(by=[\"Att\", \"target\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(PREDDIR+\"train_pred.csv\", index_col=0)\n",
    "test_data = pd.read_csv(PREDDIR+\"test_pred.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458913"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>xgbm13_p0_agg4</th>\n",
       "      <th>catb13_agg4</th>\n",
       "      <th>xgbm_p0_agg4</th>\n",
       "      <th>catb_agg4</th>\n",
       "      <th>xgbm13_p0_agg1</th>\n",
       "      <th>catb13_agg1</th>\n",
       "      <th>xgbm_p0_agg1</th>\n",
       "      <th>catb_agg1</th>\n",
       "      <th>xgbm13_p0_agg2</th>\n",
       "      <th>...</th>\n",
       "      <th>catb13_agg0</th>\n",
       "      <th>xgbm_p0_agg0</th>\n",
       "      <th>catb_agg0</th>\n",
       "      <th>xgbmv213_p0_agg0</th>\n",
       "      <th>xgbmv2_p0_agg0</th>\n",
       "      <th>conv_pred0</th>\n",
       "      <th>K7977_xgb_42</th>\n",
       "      <th>K7977_xgb_52</th>\n",
       "      <th>K7977_xgb_62</th>\n",
       "      <th>K7977_xgb_82</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.280048</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000fd6641609c6ece5454664794f0340ad84dddce9a267a310b5ae68e9d8e5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.280590</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.001328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00001b22f846c82c51f6e3958ccd81970162bae8b007e80662ef27519fcc18c1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.280590</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000041bdba6ecadd89a52d11886e8eaaec9325906c9723355abb5ca523658edc</th>\n",
       "      <td>0</td>\n",
       "      <td>0.018017</td>\n",
       "      <td>0.009783</td>\n",
       "      <td>0.016114</td>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.012393</td>\n",
       "      <td>0.005425</td>\n",
       "      <td>0.015091</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.011902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006965</td>\n",
       "      <td>0.020323</td>\n",
       "      <td>0.008674</td>\n",
       "      <td>0.014359</td>\n",
       "      <td>0.014149</td>\n",
       "      <td>0.280934</td>\n",
       "      <td>0.006640</td>\n",
       "      <td>0.008457</td>\n",
       "      <td>0.008457</td>\n",
       "      <td>0.008457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad51ca8b8c4a24cefed</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>0.003013</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004677</td>\n",
       "      <td>0.003453</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>0.003796</td>\n",
       "      <td>0.280934</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.002614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffff41c8a52833b56430603969b9ca48d208e7c192c6a4081a6acc28cf4f8af7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002730</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>0.003080</td>\n",
       "      <td>0.005093</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>0.002480</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>0.280837</td>\n",
       "      <td>0.002978</td>\n",
       "      <td>0.003718</td>\n",
       "      <td>0.003718</td>\n",
       "      <td>0.003718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd3e5b57cfcbee30286</th>\n",
       "      <td>0</td>\n",
       "      <td>0.030506</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.020954</td>\n",
       "      <td>0.026186</td>\n",
       "      <td>0.016143</td>\n",
       "      <td>0.023527</td>\n",
       "      <td>0.015302</td>\n",
       "      <td>0.021109</td>\n",
       "      <td>0.022114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025614</td>\n",
       "      <td>0.024308</td>\n",
       "      <td>0.022978</td>\n",
       "      <td>0.024616</td>\n",
       "      <td>0.028101</td>\n",
       "      <td>0.280934</td>\n",
       "      <td>0.027376</td>\n",
       "      <td>0.023917</td>\n",
       "      <td>0.023917</td>\n",
       "      <td>0.023917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffff9984b999fccb2b6127635ed0736dda94e544e67e026eee4d20f680639ff6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.002095</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.280837</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.002547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145b2c3d01967fcce461</th>\n",
       "      <td>1</td>\n",
       "      <td>0.078081</td>\n",
       "      <td>0.084864</td>\n",
       "      <td>0.079191</td>\n",
       "      <td>0.091721</td>\n",
       "      <td>0.067533</td>\n",
       "      <td>0.064484</td>\n",
       "      <td>0.097528</td>\n",
       "      <td>0.071531</td>\n",
       "      <td>0.065606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055587</td>\n",
       "      <td>0.049058</td>\n",
       "      <td>0.054159</td>\n",
       "      <td>0.046202</td>\n",
       "      <td>0.038684</td>\n",
       "      <td>0.277202</td>\n",
       "      <td>0.062325</td>\n",
       "      <td>0.053736</td>\n",
       "      <td>0.053736</td>\n",
       "      <td>0.053736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eaba8b115f71cab04681</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002845</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>0.280934</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.003112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458913 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    target  xgbm13_p0_agg4  \\\n",
       "customer_ID                                                                  \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...       0        0.001161   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...       0        0.001499   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...       0        0.001104   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...       0        0.018017   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...       0        0.002971   \n",
       "...                                                    ...             ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...       0        0.002730   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...       0        0.030506   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...       0        0.001327   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...       1        0.078081   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...       0        0.002845   \n",
       "\n",
       "                                                    catb13_agg4  xgbm_p0_agg4  \\\n",
       "customer_ID                                                                     \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...     0.000529      0.000842   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...     0.000868      0.001189   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...     0.001431      0.001254   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...     0.009783      0.016114   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...     0.003396      0.002746   \n",
       "...                                                         ...           ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...     0.004286      0.002113   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...     0.028569      0.020954   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...     0.001241      0.002094   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...     0.084864      0.079191   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...     0.002024      0.002896   \n",
       "\n",
       "                                                    catb_agg4  xgbm13_p0_agg1  \\\n",
       "customer_ID                                                                     \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...   0.000445        0.001045   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...   0.000648        0.001601   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...   0.001210        0.001626   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...   0.009630        0.012393   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...   0.003013        0.001842   \n",
       "...                                                       ...             ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...   0.003219        0.003145   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...   0.026186        0.016143   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...   0.001248        0.002010   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...   0.091721        0.067533   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...   0.002245        0.002009   \n",
       "\n",
       "                                                    catb13_agg1  xgbm_p0_agg1  \\\n",
       "customer_ID                                                                     \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...     0.000503      0.000937   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...     0.001390      0.001547   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...     0.001561      0.001255   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...     0.005425      0.015091   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...     0.003420      0.001918   \n",
       "...                                                         ...           ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...     0.003823      0.003080   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...     0.023527      0.015302   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...     0.001161      0.002095   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...     0.064484      0.097528   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...     0.001650      0.002090   \n",
       "\n",
       "                                                    catb_agg1  xgbm13_p0_agg2  \\\n",
       "customer_ID                                                                     \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...   0.000387        0.000908   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...   0.001215        0.001622   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...   0.001549        0.001448   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...   0.007225        0.011902   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...   0.002653        0.002224   \n",
       "...                                                       ...             ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...   0.005093        0.002441   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...   0.021109        0.022114   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...   0.001167        0.002004   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...   0.071531        0.065606   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...   0.001629        0.001663   \n",
       "\n",
       "                                                    ...  catb13_agg0  \\\n",
       "customer_ID                                         ...                \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...  ...     0.000383   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...  ...     0.001289   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...  ...     0.001812   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  ...     0.006965   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...  ...     0.004677   \n",
       "...                                                 ...          ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...  ...     0.005103   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...  ...     0.025614   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...  ...     0.000895   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...  ...     0.055587   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...  ...     0.001562   \n",
       "\n",
       "                                                    xgbm_p0_agg0  catb_agg0  \\\n",
       "customer_ID                                                                   \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...      0.001079   0.000288   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...      0.001231   0.001021   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...      0.002137   0.001791   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...      0.020323   0.008674   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...      0.003453   0.004487   \n",
       "...                                                          ...        ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...      0.002280   0.004329   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...      0.024308   0.022978   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...      0.001551   0.000821   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...      0.049058   0.054159   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...      0.002191   0.001718   \n",
       "\n",
       "                                                    xgbmv213_p0_agg0  \\\n",
       "customer_ID                                                            \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...          0.000329   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...          0.001514   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...          0.001509   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...          0.014359   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...          0.003531   \n",
       "...                                                              ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...          0.002480   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...          0.024616   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...          0.001009   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...          0.046202   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...          0.001709   \n",
       "\n",
       "                                                    xgbmv2_p0_agg0  \\\n",
       "customer_ID                                                          \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...        0.000353   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...        0.001200   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...        0.002012   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...        0.014149   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...        0.003796   \n",
       "...                                                            ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...        0.002420   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...        0.028101   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...        0.000888   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...        0.038684   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...        0.002109   \n",
       "\n",
       "                                                    conv_pred0  K7977_xgb_42  \\\n",
       "customer_ID                                                                    \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...    0.280048      0.000257   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...    0.280590      0.001341   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...    0.280590      0.001228   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...    0.280934      0.006640   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...    0.280934      0.002449   \n",
       "...                                                        ...           ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...    0.280837      0.002978   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...    0.280934      0.027376   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...    0.280837      0.002134   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...    0.277202      0.062325   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...    0.280934      0.003101   \n",
       "\n",
       "                                                    K7977_xgb_52  \\\n",
       "customer_ID                                                        \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...      0.000279   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...      0.001328   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...      0.001145   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...      0.008457   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...      0.002614   \n",
       "...                                                          ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...      0.003718   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...      0.023917   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...      0.002547   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...      0.053736   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...      0.003112   \n",
       "\n",
       "                                                    K7977_xgb_62  K7977_xgb_82  \n",
       "customer_ID                                                                     \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...      0.000279      0.000279  \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...      0.001328      0.001328  \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...      0.001145      0.001145  \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...      0.008457      0.008457  \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...      0.002614      0.002614  \n",
       "...                                                          ...           ...  \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...      0.003718      0.003718  \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...      0.023917      0.023917  \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...      0.002547      0.002547  \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...      0.053736      0.053736  \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...      0.003112      0.003112  \n",
       "\n",
       "[458913 rows x 24 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['target', 'xgbm13_p0_agg4', 'catb13_agg4', 'xgbm_p0_agg4', 'catb_agg4',\n",
       "       'xgbm13_p0_agg1', 'catb13_agg1', 'xgbm_p0_agg1', 'catb_agg1',\n",
       "       'xgbm13_p0_agg2', 'catb13_agg2', 'xgbm_p0_agg2', 'catb_agg2',\n",
       "       'xgbm13_p0_agg0', 'catb13_agg0', 'xgbm_p0_agg0', 'catb_agg0',\n",
       "       'xgbmv213_p0_agg0', 'xgbmv2_p0_agg0', 'conv_pred0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pd.params import *\n",
    "import pandas as pd \n",
    "train_data = pd.read_parquet(TRAINDATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = train_data.groupby(\"customer_ID\")[bestCols].agg(['mean', 'std', 'min', 'max', \"first\", 'last'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(PREDDIR, \"train_pred.csv\"), index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20204069\\Anaconda3\\envs\\p8\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:648: UserWarning: merging between different levels can give an unintended result (1 levels on the left,2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>xgbm13_p0_agg4</th>\n",
       "      <th>catb13_agg4</th>\n",
       "      <th>xgbm_p0_agg4</th>\n",
       "      <th>catb_agg4</th>\n",
       "      <th>xgbm13_p0_agg1</th>\n",
       "      <th>catb13_agg1</th>\n",
       "      <th>xgbm_p0_agg1</th>\n",
       "      <th>catb_agg1</th>\n",
       "      <th>xgbm13_p0_agg2</th>\n",
       "      <th>...</th>\n",
       "      <th>(B_2, min)</th>\n",
       "      <th>(B_2, max)</th>\n",
       "      <th>(B_2, first)</th>\n",
       "      <th>(B_2, last)</th>\n",
       "      <th>(B_9, mean)</th>\n",
       "      <th>(B_9, std)</th>\n",
       "      <th>(B_9, min)</th>\n",
       "      <th>(B_9, max)</th>\n",
       "      <th>(B_9, first)</th>\n",
       "      <th>(B_9, last)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000242</td>\n",
       "      <td>1.009672</td>\n",
       "      <td>1.006838</td>\n",
       "      <td>1.007647</td>\n",
       "      <td>0.006220</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.009535</td>\n",
       "      <td>0.008207</td>\n",
       "      <td>0.009535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00000fd6641609c6ece5454664794f0340ad84dddce9a267a310b5ae68e9d8e5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819772</td>\n",
       "      <td>1.008534</td>\n",
       "      <td>1.002647</td>\n",
       "      <td>1.004028</td>\n",
       "      <td>0.010298</td>\n",
       "      <td>0.011024</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.045093</td>\n",
       "      <td>0.007561</td>\n",
       "      <td>0.012926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00001b22f846c82c51f6e3958ccd81970162bae8b007e80662ef27519fcc18c1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810796</td>\n",
       "      <td>0.819987</td>\n",
       "      <td>0.810796</td>\n",
       "      <td>0.812649</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>0.003302</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.009521</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.009392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000041bdba6ecadd89a52d11886e8eaaec9325906c9723355abb5ca523658edc</th>\n",
       "      <td>0</td>\n",
       "      <td>0.018017</td>\n",
       "      <td>0.009783</td>\n",
       "      <td>0.016114</td>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.012393</td>\n",
       "      <td>0.005425</td>\n",
       "      <td>0.015091</td>\n",
       "      <td>0.007225</td>\n",
       "      <td>0.011902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812053</td>\n",
       "      <td>1.009999</td>\n",
       "      <td>0.812053</td>\n",
       "      <td>1.006183</td>\n",
       "      <td>0.052241</td>\n",
       "      <td>0.053342</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.176352</td>\n",
       "      <td>0.026897</td>\n",
       "      <td>0.020526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad51ca8b8c4a24cefed</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>0.003013</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810670</td>\n",
       "      <td>0.819947</td>\n",
       "      <td>0.818691</td>\n",
       "      <td>0.815746</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.009847</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>0.004027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffff41c8a52833b56430603969b9ca48d208e7c192c6a4081a6acc28cf4f8af7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002730</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>0.003823</td>\n",
       "      <td>0.003080</td>\n",
       "      <td>0.005093</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000524</td>\n",
       "      <td>1.009987</td>\n",
       "      <td>1.000690</td>\n",
       "      <td>1.009866</td>\n",
       "      <td>0.138467</td>\n",
       "      <td>0.085967</td>\n",
       "      <td>0.013571</td>\n",
       "      <td>0.299308</td>\n",
       "      <td>0.013571</td>\n",
       "      <td>0.170243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd3e5b57cfcbee30286</th>\n",
       "      <td>0</td>\n",
       "      <td>0.030506</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.020954</td>\n",
       "      <td>0.026186</td>\n",
       "      <td>0.016143</td>\n",
       "      <td>0.023527</td>\n",
       "      <td>0.015302</td>\n",
       "      <td>0.021109</td>\n",
       "      <td>0.022114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030446</td>\n",
       "      <td>0.055656</td>\n",
       "      <td>0.030639</td>\n",
       "      <td>0.055656</td>\n",
       "      <td>0.186676</td>\n",
       "      <td>0.028611</td>\n",
       "      <td>0.143207</td>\n",
       "      <td>0.230870</td>\n",
       "      <td>0.224073</td>\n",
       "      <td>0.143207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffff9984b999fccb2b6127635ed0736dda94e544e67e026eee4d20f680639ff6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.001241</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.002095</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>1.009931</td>\n",
       "      <td>1.008807</td>\n",
       "      <td>1.007023</td>\n",
       "      <td>0.080004</td>\n",
       "      <td>0.047993</td>\n",
       "      <td>0.020363</td>\n",
       "      <td>0.193388</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.046456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145b2c3d01967fcce461</th>\n",
       "      <td>1</td>\n",
       "      <td>0.078081</td>\n",
       "      <td>0.084864</td>\n",
       "      <td>0.079191</td>\n",
       "      <td>0.091721</td>\n",
       "      <td>0.067533</td>\n",
       "      <td>0.064484</td>\n",
       "      <td>0.097528</td>\n",
       "      <td>0.071531</td>\n",
       "      <td>0.065606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182818</td>\n",
       "      <td>1.005970</td>\n",
       "      <td>1.005970</td>\n",
       "      <td>0.714486</td>\n",
       "      <td>0.012824</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.020479</td>\n",
       "      <td>0.007907</td>\n",
       "      <td>0.016491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eaba8b115f71cab04681</th>\n",
       "      <td>0</td>\n",
       "      <td>0.002845</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239062</td>\n",
       "      <td>1.009708</td>\n",
       "      <td>1.009708</td>\n",
       "      <td>0.992880</td>\n",
       "      <td>0.024574</td>\n",
       "      <td>0.023343</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.078961</td>\n",
       "      <td>0.078961</td>\n",
       "      <td>0.001976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458913 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    target  xgbm13_p0_agg4  \\\n",
       "customer_ID                                                                  \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...       0        0.001161   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...       0        0.001499   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...       0        0.001104   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...       0        0.018017   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...       0        0.002971   \n",
       "...                                                    ...             ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...       0        0.002730   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...       0        0.030506   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...       0        0.001327   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...       1        0.078081   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...       0        0.002845   \n",
       "\n",
       "                                                    catb13_agg4  xgbm_p0_agg4  \\\n",
       "customer_ID                                                                     \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...     0.000529      0.000842   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...     0.000868      0.001189   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...     0.001431      0.001254   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...     0.009783      0.016114   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...     0.003396      0.002746   \n",
       "...                                                         ...           ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...     0.004286      0.002113   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...     0.028569      0.020954   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...     0.001241      0.002094   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...     0.084864      0.079191   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...     0.002024      0.002896   \n",
       "\n",
       "                                                    catb_agg4  xgbm13_p0_agg1  \\\n",
       "customer_ID                                                                     \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...   0.000445        0.001045   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...   0.000648        0.001601   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...   0.001210        0.001626   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...   0.009630        0.012393   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...   0.003013        0.001842   \n",
       "...                                                       ...             ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...   0.003219        0.003145   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...   0.026186        0.016143   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...   0.001248        0.002010   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...   0.091721        0.067533   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...   0.002245        0.002009   \n",
       "\n",
       "                                                    catb13_agg1  xgbm_p0_agg1  \\\n",
       "customer_ID                                                                     \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...     0.000503      0.000937   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...     0.001390      0.001547   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...     0.001561      0.001255   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...     0.005425      0.015091   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...     0.003420      0.001918   \n",
       "...                                                         ...           ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...     0.003823      0.003080   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...     0.023527      0.015302   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...     0.001161      0.002095   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...     0.064484      0.097528   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...     0.001650      0.002090   \n",
       "\n",
       "                                                    catb_agg1  xgbm13_p0_agg2  \\\n",
       "customer_ID                                                                     \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...   0.000387        0.000908   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...   0.001215        0.001622   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...   0.001549        0.001448   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...   0.007225        0.011902   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...   0.002653        0.002224   \n",
       "...                                                       ...             ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...   0.005093        0.002441   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...   0.021109        0.022114   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...   0.001167        0.002004   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...   0.071531        0.065606   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...   0.001629        0.001663   \n",
       "\n",
       "                                                    ...  (B_2, min)  \\\n",
       "customer_ID                                         ...               \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...  ...    1.000242   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...  ...    0.819772   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...  ...    0.810796   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...  ...    0.812053   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...  ...    0.810670   \n",
       "...                                                 ...         ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...  ...    1.000524   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...  ...    0.030446   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...  ...    0.004150   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...  ...    0.182818   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...  ...    0.239062   \n",
       "\n",
       "                                                    (B_2, max)  (B_2, first)  \\\n",
       "customer_ID                                                                    \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...    1.009672      1.006838   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...    1.008534      1.002647   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...    0.819987      0.810796   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...    1.009999      0.812053   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...    0.819947      0.818691   \n",
       "...                                                        ...           ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...    1.009987      1.000690   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...    0.055656      0.030639   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...    1.009931      1.008807   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...    1.005970      1.005970   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...    1.009708      1.009708   \n",
       "\n",
       "                                                    (B_2, last)  (B_9, mean)  \\\n",
       "customer_ID                                                                    \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...     1.007647     0.006220   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...     1.004028     0.010298   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...     0.812649     0.004730   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...     1.006183     0.052241   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...     0.815746     0.006685   \n",
       "...                                                         ...          ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...     1.009866     0.138467   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...     0.055656     0.186676   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...     1.007023     0.080004   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...     0.714486     0.012824   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...     0.992880     0.024574   \n",
       "\n",
       "                                                    (B_9, std)  (B_9, min)  \\\n",
       "customer_ID                                                                  \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...    0.003180    0.000519   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...    0.011024    0.001722   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...    0.003302    0.000422   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...    0.053342    0.001702   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...    0.002242    0.002925   \n",
       "...                                                        ...         ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...    0.085967    0.013571   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...    0.028611    0.143207   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...    0.047993    0.020363   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...    0.005475    0.001419   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...    0.023343    0.001976   \n",
       "\n",
       "                                                    (B_9, max)  (B_9, first)  \\\n",
       "customer_ID                                                                    \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...    0.009535      0.008207   \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...    0.045093      0.007561   \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...    0.009521      0.003663   \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...    0.176352      0.026897   \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...    0.009847      0.005475   \n",
       "...                                                        ...           ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...    0.299308      0.013571   \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...    0.230870      0.224073   \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...    0.193388      0.111800   \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...    0.020479      0.007907   \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...    0.078961      0.078961   \n",
       "\n",
       "                                                    (B_9, last)  \n",
       "customer_ID                                                      \n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fb...     0.009535  \n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a26...     0.012926  \n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80...     0.009392  \n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c97233...     0.020526  \n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad...     0.004027  \n",
       "...                                                         ...  \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a40...     0.170243  \n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd...     0.143207  \n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026...     0.046456  \n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145...     0.016491  \n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eab...     0.001976  \n",
       "\n",
       "[458913 rows x 75 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.merge(d, how=\"left\", on=\"customer_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['catb_agg1',\n",
       " 'catb_agg2',\n",
       " 'K7977_xgb_52',\n",
       " 'K7977_xgb_62',\n",
       " 'K7977_xgb_82',\n",
       " 'K7977_catb_52',\n",
       " 'K7977_focal_52',\n",
       " 'K7977_catb_62',\n",
       " 'K7977_focal_62',\n",
       " 'K7977_catb_82',\n",
       " 'K7977_focal_82',\n",
       " 'K7977_focal_xgb_42',\n",
       " 'K7977_focal_xgb_52',\n",
       " 'K7977_focal_xgb_62',\n",
       " ('P_2', 'mean'),\n",
       " ('P_2', 'std'),\n",
       " ('P_2', 'min'),\n",
       " ('P_2', 'max'),\n",
       " ('P_2', 'first'),\n",
       " ('P_2', 'last'),\n",
       " ('D_48', 'mean'),\n",
       " ('D_48', 'std'),\n",
       " ('D_48', 'min'),\n",
       " ('D_48', 'max'),\n",
       " ('D_48', 'first'),\n",
       " ('D_48', 'last'),\n",
       " ('R_1', 'mean'),\n",
       " ('R_1', 'std'),\n",
       " ('R_1', 'min'),\n",
       " ('R_1', 'max'),\n",
       " ('R_1', 'first'),\n",
       " ('R_1', 'last'),\n",
       " ('D_44', 'mean'),\n",
       " ('D_44', 'std'),\n",
       " ('D_44', 'min'),\n",
       " ('D_44', 'max'),\n",
       " ('D_44', 'first'),\n",
       " ('D_44', 'last'),\n",
       " ('B_2', 'mean'),\n",
       " ('B_2', 'std'),\n",
       " ('B_2', 'min'),\n",
       " ('B_2', 'max'),\n",
       " ('B_2', 'first'),\n",
       " ('B_2', 'last'),\n",
       " ('B_9', 'mean'),\n",
       " ('B_9', 'std'),\n",
       " ('B_9', 'min'),\n",
       " ('B_9', 'max'),\n",
       " ('B_9', 'first'),\n",
       " ('B_9', 'last')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"catb_agg1\", \"catb_agg2\", \"K7977_xgb_52\", \"K7977_xgb_62\", \"K7977_xgb_82\",\"K7977_catb_52\" ,\"K7977_focal_52\" ,\n",
    "        \"K7977_catb_62\", \"K7977_focal_62\", \"K7977_catb_82\", \"K7977_focal_82\", \"K7977_focal_xgb_42\", \"K7977_focal_xgb_52\", \"K7977_focal_xgb_62\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('P_2', 'mean'),\n",
       " ('P_2', 'std'),\n",
       " ('P_2', 'min'),\n",
       " ('P_2', 'max'),\n",
       " ('P_2', 'first'),\n",
       " ('P_2', 'last'),\n",
       " ('D_48', 'mean'),\n",
       " ('D_48', 'std'),\n",
       " ('D_48', 'min'),\n",
       " ('D_48', 'max'),\n",
       " ('D_48', 'first'),\n",
       " ('D_48', 'last'),\n",
       " ('R_1', 'mean'),\n",
       " ('R_1', 'std'),\n",
       " ('R_1', 'min'),\n",
       " ('R_1', 'max'),\n",
       " ('R_1', 'first'),\n",
       " ('R_1', 'last'),\n",
       " ('D_44', 'mean'),\n",
       " ('D_44', 'std'),\n",
       " ('D_44', 'min'),\n",
       " ('D_44', 'max'),\n",
       " ('D_44', 'first'),\n",
       " ('D_44', 'last'),\n",
       " ('B_2', 'mean'),\n",
       " ('B_2', 'std'),\n",
       " ('B_2', 'min'),\n",
       " ('B_2', 'max'),\n",
       " ('B_2', 'first'),\n",
       " ('B_2', 'last'),\n",
       " ('B_9', 'mean'),\n",
       " ('B_9', 'std'),\n",
       " ('B_9', 'min'),\n",
       " ('B_9', 'max'),\n",
       " ('B_9', 'first'),\n",
       " ('B_9', 'last')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customer_ID\n",
       "0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a    0.933824\n",
       "00000fd6641609c6ece5454664794f0340ad84dddce9a267a310b5ae68e9d8e5    0.899820\n",
       "00001b22f846c82c51f6e3958ccd81970162bae8b007e80662ef27519fcc18c1    0.878454\n",
       "000041bdba6ecadd89a52d11886e8eaaec9325906c9723355abb5ca523658edc    0.598969\n",
       "00007889e4fcd2614b6cbe7f8f3d2e5c728eca32d9eb8ad51ca8b8c4a24cefed    0.891679\n",
       "                                                                      ...   \n",
       "ffff41c8a52833b56430603969b9ca48d208e7c192c6a4081a6acc28cf4f8af7    0.848750\n",
       "ffff518bb2075e4816ee3fe9f3b152c57fc0e6f01bf7fdd3e5b57cfcbee30286    0.859327\n",
       "ffff9984b999fccb2b6127635ed0736dda94e544e67e026eee4d20f680639ff6    0.786838\n",
       "ffffa5c46bc8de74f5a4554e74e239c8dee6b9baf388145b2c3d01967fcce461    0.804454\n",
       "fffff1d38b785cef84adeace64f8f83db3a0c31e8d92eaba8b115f71cab04681    0.983617\n",
       "Name: (P_2, mean), Length: 458913, dtype: float32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('p8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25d4fc9e51127fc9597103904eccbdaaf4ac71170a764a4234e11ed19cb4831a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
